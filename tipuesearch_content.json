{"pages":[{"url":"pages/about.html","text":"CV (pdf) current as of December 2017","tags":"pages","title":"About"},{"url":"the-invisible-hand.html","text":"Markets and financial time series data. Basic ideas about markets Inevitably, finance and markets will be a recurring subject of discussion on this blog. They are ubiquitous, useful, fascinatingly complex at various levels and they generate a lot of readily accessible data. What is a market ? A market is a system which enables traders to exchange commodities/assets at a mutually agreeable price . Every market has at its center a market maker - an entity which matches traders who want to buy at a certain price with traders who want to sell at that price. No Arbitrage. The word arbitrage refers to a situation where the same asset has different prices in different locations. For instance, if Klingon Bat'Leths are available at 10USD a piece in Berlin and at 15USD a piece in Baghdad, and if the cost of transporting a Bat'Leth from Berlin to Baghdad is 1USD, then money can be made by buying Bat'Leths in Berlin and selling them in Baghdad. In fact, people will buy Bat'Leths in Berlin and sell them in Baghdad until the price of Bat'Leths increases in Berlin and decreases in Baghdad and there is no more profit to be made. In an ideal market, arbitrage is instantaneously washed out by traders making money off it. The no arbitrage condition implies that the market serves as a mechanism for price setting. Each commodity has a \"rational\" price decided in the market by balancing the forces of demand and supply. The efficient market. A market is said to be efficient if all the information about a particular asset is instantly assimilated by the market and is immediately reflected in the price of the asset. This assumption has significant implications for the time series of the price of an asset traded on the market. Since Bachelier in 1900, it has been argued that an efficient market should imply that prices move randomly but a formal proof was presented by Samuelson in 1965 (pdf) . The paper is readable to anyone with moderate exposure to probability theory and the principle result is that at the present time \\(t\\) , given the historical time series of prices of a particular asset \\(\\{y_t,y_{t-1}.......y_0\\}\\) , if the futures price of that asset to be delivered at time \\(T>t\\) is \\(C(T,t)\\) , then the expected price at the next time point \\(t+1\\) is given by \\(E\\left\\{C(T,t+1)\\right\\}=C(T,t)\\) . In other words, it is impossible to predict which way the price will move at the next time point based on the historical price data. Before making this idea more intuitive, we will introduce market returns . If the price of an asset today is \\(y_t\\) and tomorrow it is \\(y_{t+1}\\) , then the return an investor might have obtained by buying today and selling tomorrow is defined to be \\(\\frac{y_{t+1}-y_t}{y_t}\\) . It is more common in practice to use the logarithmic return defined by \\(ln\\left(\\frac{y_{t+1}}{y_t}\\right)\\) . It is useful to think of logarithmic returns being related to compound interest and normal returns being related to simple interest. If the price today was 100, and tomorrow is 110, then my return is 10% while my logarithmic return is 9.5%. The two methods of calculating return give approximately the same result but the logarithmic return is smaller since a lower rate of return is needed to obtain the same final capital with compound interest. We can now restate the efficient market hypothesis as a statement about returns. In an efficient market, returns must be serially uncorrelated. Stated this way, it is much easier to see the link between the efficient market hypothesis and the randomness of prices. If, for a certain asset, it were possible to predict that the price would rise (positive return) or fall (negative return) based on the past, this would present a powerful arbitrage opportunity. If prices were predicted to rise in the future, intelligent investors (intelligent enough to see correlations in returns anyway) could make a lot of money by buying today and selling when the price rose. However, this buying activity would immediately cause the price (at which the asset can be bought) to rise, washing out the gains that the investors might have made. It is worth stating that while the degree of randomness of returns on the price of a traded asset can be tested , tests for the efficient market hypothesis suffer from the so-called joint hypothesis problem . How would we know if a market is inefficient ? We might look at all the information available, and find that the market behaves \"abnormally\" or \"irrationally\", given the available information. However, to evaluate what a normal/abnormal return is, we need a model that connects available information to the price of an asset ( an asset pricing model ). And therein lies the issue : even if we observe \"abnormal returns\", is the market inefficient or is our asset pricing model wrong ? We cannot possibly know. Who does an \"informed investor\" buy from ? There is a word for them. Noise Investors. They provide the liquidity in the market, buying and selling assets regardless of price, perhaps acting on information that is really noise, or driven by other factors like an urgent need for cash resulting in a sale regardless of price. Since demand from noise investors is - by definition - random and independent of the price of the asset, the random uncorrelated fluctuations in asset prices are caused by the actions of informed investors seeking maximum profit. Anyone familiar with information theory will immediately recognize what is going on here. Since the time series of the price or returns on an asset incorporates non redundant information at each time point, it looks like a completely random, uncorrelated sequence. A sequence that has very little information can be compressed and expressed as a concise computer program (see algorithmic complexity ) or compressed in other ways using correlations. The higher the information content the more random a sequence looks. From this point of view, it is clear that a sequence incorporating a lot of information is indistinguishable from a completely random sequence. If the price can be predicted to go up based on history, it would have already gone up to a point where no more profit is expected from a price rise. Once one has assumed that the market is efficient, this conclusion seems inescapable. As the old joke goes, two economists are walking along a road and one of them spots a 100\\ \\( bill on the street. The other economist tells him not to bother, since if there really were a 100\\\\) bill lying about, it would already have been picked up ! While real markets resemble ideal, efficient markets in many ways (correlations between returns are washed out in less than 2 minutes, arbitrage is hard to find and so on) markets are only efficient in proportion to the number of intelligent investors looking to profit from their inefficiencies. There is a clear tension here. The effort investors are prepared to make to sniff out inefficiencies is proportional to the degree of inefficiency that exists. So, every profit opportunity is washed out only if one is not participating in the washing out. A peek at financial data We will use the Quandl package (see the website for details) to download recent oil prices and analyze them a little bit. This will serve as a short introduction to uni-variate time series analysis in R. See this useful resource from the NIST for a simple overview of the theory. oil_prices <- Quandl ( \"OPEC/ORB\" , type = \"raw\" , transform = \"normalize\" , collapse = \"daily\" , force_irregular = TRUE , start_date = \"2001-01-01\" ) %>% as_data_frame () %>% transmute ( date = Date , price_of_oil = Value ) %>% arrange ( date ) The transform = \"normalize\" option sets the first value in the time series to 100 and scales all the other values accordingly. Let us take a look at oil prices over the last 18 years, scaled to the price on the 1st of January 2001. ggplot ( data = oil_prices ) + geom_line ( aes ( x = date , y = price_of_oil )) + geom_vline ( xintercept = as.Date ( \"2008-07-11\" ), colour = \"#7379d6\" ) + annotate ( geom = \"text\" , label = \"Financial crash of 2008\" , x = as.Date ( \"2006-6-11\" ), y = 480 , colour = \"#7379d6\" ) + geom_vline ( xintercept = as.Date ( \"2014-10-11\" ), colour = \"#7379d6\" ) + annotate ( geom = \"text\" , label = \"Overproduction\" , x = as.Date ( \"2016-4-11\" ), y = 480 , colour = \"#7379d6\" ) + ggthemes :: theme_economist () + ggtitle ( \"Normalized oil prices since 2001-01-01\" ) In this data, there are two major crashes the first corresponding to the financial crisis of 2008 ( NYT comment on oil prices around this time ) which led to lower demand, while the oil price crash of 2014-15 seems to be linked to over production as oil producers competed for market share despite production ramp-ups in North America with fracking in the USA and oil sands in Canada . Correlations in time Is this a random walk ? The random walk hypothesis follows intuitively from the efficient market hypothesis. If today's price includes all available information then it is the best available estimate of tomorrow's price, i.e., the price could go either way tomorrow, and successive returns are un-correlated. However, We see from the oil price chart above that there are long periods of positive and negative returns. At least at some times, over short-ish time scales, returns do seem to be correlated. Another useful concept about any time series \\(\\{y_t\\}\\) is stationarity . A time series is said to be stationary if it's mean function \\(\\mu_t = E[y_t]\\) and it's autocovariance function \\(\\gamma(t,t-k) = E[(y_t-\\mu_t)(y_{t-k}-\\mu_{t-k})]\\) are both independent of time. In other words, a series is stationary if, over time, all its values are distributed around the same mean, and its relationship with its past does not evolve over time. A strongly stationary process has a joint probability distribution which does not change when shifted in time, i.e. ALL moments of the distribution are time independent. In practice, most financial time series are not stationary, however, stationary series can often be derived from non stationary series. For instance, the differences, or returns on a time series could be stationary even if the series itself is not. Or, the series could be fit to a function that approximates its mean over time, and subtracting this fitted mean from the original series yields a stationary series. As we shall see in subsequent sections, the simplest models often assume that a series is stationary. We can measure the influence of the past on the present value of a time series via its autocorrelation function. The autocorrelation of a signal is the correlation of a signal with a delayed copy of itself. The autocorrelation function (ACF) calculates the correlations with different lags, giving us some idea about how long it takes for information contained in today's price to be swamped by new information in the signal. The autocorrelation is just the normalized autocovariance function. Given observations \\(y_t\\) for \\(t\\in \\{1..N\\}\\) , the autocorrelation for lag \\(k\\) is given by, $$\\rho_y(k) = \\frac{\\gamma(t,t-k)}{\\gamma(t,t)} = \\frac{\\sum_{t=1+k}&#94;N (y_{t-k}-\\mu_{t-k})(y_t-\\mu_t)}{\\sum_{t=1}&#94;N (y_t-\\mu_t)&#94;2}$$ and stationarity would imply \\(\\mu_{t-k}=\\mu_t \\text{ }\\forall (t,k)\\) . The ACF computes this number for various values of \\(k\\) . In practice, we use the (slightly more complicated) partial autocorrelation function that computes the correlation of a time series with a lagged version of itself like the ACF, but also controls for the influence of all shorter lags. In other words, for a lag of say, 2 days, it computes how much the price day before yesterday is correlated with the price today (over the whole time series) over and above the correlation induced by the price yesterday (which is correlated to today's as well as day before yesterday's price). This gives a \"decoupled\" version of the influence of various time points in the past on the present. oil_price_returns <- data_frame ( date = oil_prices $ date [ 2 : nrow ( oil_prices )], returns = diff ( oil_prices $ price_of_oil ), log_returns = diff ( log ( oil_prices $ price_of_oil ))) ggplot ( oil_price_returns ) + geom_line ( aes ( x = date , y = log_returns , colour = \"logarithmic returns\" )) + ggtitle ( \"Time series of logarithmic returns\" ) + ggthemes :: theme_economist () + geom_vline ( xintercept = as.Date ( \"2008-07-11\" ), colour = \"#7379d6\" ) + annotate ( geom = \"text\" , label = \"Financial crash of 2008\" , x = as.Date ( \"2006-6-11\" ), y = 0.15 , colour = \"#7379d6\" ) + geom_vline ( xintercept = as.Date ( \"2014-10-11\" ), colour = \"#7379d6\" ) + annotate ( geom = \"text\" , label = \"Overproduction\" , x = as.Date ( \"2016-4-11\" ), y = 0.15 , colour = \"#7379d6\" ) temp_acf_log_returns <- pacf ( oil_price_returns $ log_returns , lag.max = 8 , plot = FALSE ) acf_df <- data_frame ( log_returns_acf = temp_acf_log_returns $ acf [, , 1 ], lag = temp_acf_log_returns $ lag [, , 1 ]) ggplot ( data = acf_df ) + geom_point ( aes ( x = lag , y = log_returns_acf )) + geom_segment ( aes ( x = lag , y = log_returns_acf , xend = lag , yend = 0 )) + geom_hline ( aes ( yintercept = 0 )) + ggtitle ( \"Partial autocorrelation function for the logarithmic returns\" ) + ggthemes :: theme_economist () In general then, the price of oil today is correlated with the price of oil yesterday, but, it would seem, has basically nothing to do with the price of oil the day before. While this is true of the whole time series, we could also compute this for windows of 365 days each (short windows lead to noisy estimates of the ACF coefficients), to see if there are periods of high long-range (multiple day) correlations. acf_noplot <- function ( vector ){ return ( pacf ( vector , lag.max = 8 , pl = FALSE )) } window_width <- 365 windowed_acf <- rollapply ( oil_price_returns $ log_returns , width = window_width , FUN = acf_noplot , align = \"left\" ) %>% unlist () windowed_acf_df <- windowed_acf %>% matrix ( 8 , length ( windowed_acf ) / 8 ) %>% t () %>% as_data_frame () %>% slice ( 1 : ( nrow ( oil_price_returns ) - window_width +1 )) %>% mutate_all ( as.numeric ) %>% mutate ( date = oil_price_returns $ date [ window_width : nrow ( oil_price_returns )]) acf_values <- windowed_acf_df %>% summarise_all ( mean ) %>% gather () acf_values %>% head () ## # A tibble: 6 x 2 ## key value ## <chr> <dbl> ## 1 V1 0.245 ## 2 V2 -0.0536 ## 3 V3 0.0213 ## 4 V4 0.00316 ## 5 V5 0.0104 ## 6 V6 -0.0128 We can see that while evaluating ACFs on smaller samples via a moving window and taking the mean is not quite the same as taking the ACF on the whole series, the pattern is not different, i.e., the correlation is washed out after the second day. Now, we can plot the 2nd, 3rd and 4th terms of the ACF function to see if there are periods of higher and lower correlations in the oil prices. ggplot ( windowed_acf_df ) + geom_line ( aes ( x = date , y = V1 , colour = \"Lag 1\" )) + geom_line ( aes ( x = date , y = V2 , colour = \"Lag 2\" )) + geom_line ( aes ( x = date , y = V3 , colour = \"Lag 3\" )) + geom_vline ( xintercept = as.Date ( \"2008-07-11\" ), colour = \"#7379d6\" ) + annotate ( geom = \"text\" , label = \"Financial crash of 2008\" , x = as.Date ( \"2006-6-11\" ), y = 0.5 , colour = \"#7379d6\" ) + geom_vline ( xintercept = as.Date ( \"2014-10-11\" ), colour = \"#7379d6\" ) + annotate ( geom = \"text\" , label = \"Overproduction\" , x = as.Date ( \"2016-4-11\" ), y = 0.5 , colour = \"#7379d6\" ) + geom_line ( aes ( x = oil_prices $ date [ window_width : ( nrow ( oil_prices ) -1 )], y = oil_prices $ price_of_oil [ window_width : ( nrow ( oil_prices ) -1 )] / 1000 , colour = \"rescaled oil price\" ), alpha = 0.55 ) + ggthemes :: theme_economist () + ggtitle ( \"Evolution of correlations with different lags\" ) + ylab ( \"correlation\" ) It is clear by inspection that both crashes correspond to increasing correlation (of log-returns) across all three lag terms plotted. That is, while the oil price was crashing, autocorrelations (of log-returns) with lags of 1, 2, 3 days were all increasing. autocorrelations peaked when the oil price reached rock bottom and relaxed again as the price recovery started. The time series of log-returns on oil prices is clearly not stationary (and nor are oil prices themselves, needless to say). So, what is a good way to forecast oil prices ? \\(AR(p)\\) , \\(ARMA(p,q)\\) , \\(ARIMA(p,d,q)\\) , \\(ARCH(q)\\) , \\(GARCH(p,q)\\) ... One possible simple model of a time series like ours is an autoregressive process of order \\(p\\) . This just means that the current value of the time series depends on the value of the time series at \\(p\\) previous time steps and a noise term. An \\(AR(p)\\) process (this is what they are called..) take the form, $$x_{t} = c + \\sum_{i = 1}&#94;p \\phi_i x_{t-i\\Delta t} + \\epsilon_t$$ where \\(\\epsilon_t\\) is the uncorrelated, unbiased noise term. For oil price returns, the coefficients \\(\\phi_i\\) will probably not be significant (overall) for \\(i>1\\) . However, we have already seen that the influence of the past changes with time, and there are periods when multiple day correlations might be vital to explaining the change in price. \\(AR(p)\\) processes need not always be stationary. Moving average models \\(MA(q)\\) on the other hand are always stationary and posit that the present value \\(y_t\\) is the sum of some mean value, a white noise term, and a sum over \\(q\\) past values of noise terms (the moving average referred to in the name). $$y_t = \\mu + \\epsilon_t + \\sum_{i=1}&#94;q \\epsilon_{t-i\\Delta t}.$$ It does not take a genius to infer that autoregressive moving average \\(ARMA(p,q)\\) models consist of \\(p\\) auto regressive and \\(q\\) moving average terms. They are weakly stationary (the first two moments are time invariant). To be able to forecast non-stationary processes, \\(ARMA(p,q)\\) models have been generalized to autoregressive integrated moving average \\(ARIMA(p,d,q)\\) models. Apart from the \\(p\\) lagged values of itself and the sum over \\(q\\) noise terms from the past the \\(ARIMA(p,d,q)\\) also have the time series values differenced \\(d\\) times. This differencing is the discrete version of a derivative, so 1st order differencing is \\(y_t' = y_t - y_{t-1}\\) while second order differencing is \\(y_t'' = y'_t - y'_{t-1} = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2})\\) and so on. So far, we have seen schemes add successive levels of complexity to model the value of a time series, but none of these attempt to model the changes over time of the noise terms. So far, these schemes have assumed the parameters of the noise term to be constants. The autoregressive conditional heteroskedasticity \\(ARCH(q)\\) and it's cousin the generalized autoregressive conditional heteroskedasticity \\(GARCH(p,q)\\) do model the evolution of the noise term over time. In particular, \\(ARCH(q)\\) models assume an autoregression of order \\(q\\) , an \\(AR(q)\\) model for the variance of the noise term, while \\(GARCH(p,q)\\) models assume an \\(ARIMA(p,q)\\) model for the variance of the noise term. Thus, one might use a \\(ARIMA(p,d,q)\\) process to model the price of oil, and a \\(GARCH(r,s)\\) process to model its volatility (the variance of the noise term !). Now, we will attempt to forecast oil prices using a \\(ARIMA(2,2,2)\\) process. We will fit the process to data until 2018-01-01, and calculate the RMS error on log-returns data post 2015-01-01. test_date <- \"2017-05-01\" train <- oil_prices $ price_of_oil [ oil_prices $ date <= as.Date ( test_date )] test <- oil_prices $ price_of_oil [ oil_prices $ date > as.Date ( test_date )] arima_fit <- arima ( train , order = c ( 3 , 1 , 3 ), transform.pars = TRUE , seasonal = list ( order = c ( 1 , 0 , 1 ), period = 20 )) simulated_prices <- predict ( arima_fit , length ( test )) test_df <- data_frame ( date = oil_prices $ date , price_of_oil = oil_prices $ price_of_oil , arima_prediction = c ( train , simulated_prices $ pred ), arima_error = c ( seq ( 0 , 0 , length.out = length ( train )), simulated_prices $ se )) ggplot ( test_df ) + geom_line ( aes ( x = date , y = arima_prediction , colour = \"arima pred\" )) + geom_errorbar ( aes ( x = date , ymin = arima_prediction - arima_error , ymax = arima_prediction + arima_error , colour = \"arima pred\" ), alpha = 0.07 ) + geom_line ( aes ( x = date , y = price_of_oil , colour = \"price of oil\" )) + ggthemes :: theme_economist () + xlim ( as.Date ( \"2015-08-01\" ), as.Date ( \"2018-03-25\" )) + ylim ( 20 , 350 ) + ggtitle ( \"Arima predictions\" ) Clearly, not a great forecast even during a period without extreme price movements. We will round off our little discussion of univariate financial time series with a small section on how returns are distributed. Distribution of returns With all the talk around random walks on wall street, and with Gaussian distributions being analytically tractable, people - including experts - have come to rely on too many distributions in finance being Gaussian, and they are not. There is a rather good reason for random walks leading to Gaussian distributions : the central limit theorem . The basic idea is, for a large class of probability distributions (i.e. those whose variances are finite) if one adds a large number of independent random variables (eg. steps in a random walk... the position after a large number of steps is the sum of each step) one gets a number that has a Gaussian distribution. However, these conditions are not always fulfilled. We have already seen that each step (the returns) in the random walk (of the price) is not always independent of the others (see the autocorrelations in the returns discussed above), and even worse, the returns may or may not have a distribution that is nice and has a finite variance. Let us take a look at the distribution of scaled logarithmic returns of oil prices as compared to the normal (Gaussian) distribution via quantile-quantile plots . ggplot ( oil_price_returns ) + geom_qq ( aes ( sample = scale ( log_returns ), colour = \"log-returns\" ), distribution = stats :: qnorm ) + ggtitle ( \"Quantile-quantile plot of log-returns against normal distribution\" ) + ggthemes :: theme_economist () Clearly, both returns and logarithmic returns take large positive and negative values far more frequently than they would if they indeed followed a Gaussian distribution. This tells us that the distributions of returns (and log-returns) of oil prices have fatter tails . We can try to fit these to a distribution with a fatter tail, like the Cauchy distribution, and plot the densities on a semi-log plot so that we see the tails better. cauchy_fit <- fitdistr ( scale ( oil_price_returns $ returns ), densfun = \"cauchy\" ) glue ( \"Location parameter is {cauchy_fit$estimate[1]} and the scale parameter is {cauchy_fit$estimate[2]}\" ) ## Location parameter is 0.0347862087434961 and the scale parameter is 0.497758531292875 ggplot ( oil_price_returns ) + geom_point ( aes ( x = scale ( oil_price_returns $ log_returns ), colour = \"log returns\" , y = .. density.. ), stat = \"bin\" ) + stat_function ( fun = dcauchy , n = 1e2 , args = list ( location = cauchy_fit $ estimate [ 1 ], scale = cauchy_fit $ estimate [ 2 ]), size = 1 , alpha = 0.8 , aes ( colour = \"cauchy\" )) + stat_function ( fun = dnorm , n = 1e2 , args = list ( mean = mean ( scale ( oil_price_returns $ log_returns )), sd = sd ( scale ( oil_price_returns $ log_returns ))), size = 1 , alpha = 0.8 , aes ( colour = \"gaussian\" ), xlim = c ( -4 , 4 )) + scale_y_log10 () + ggtitle ( \"Distribution of logarithmic returns\" ) + ggthemes :: theme_economist () + xlab ( \"Scaled logarithmic returns\" ) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We see that the distribution of logarithmic returns has fatter tails than the Gaussian, but is not quite as fat tailed as the Cauchy distribution. The central limit theorem is only one of a class of limit theorems, and the Gaussian is only one attractor of an infinite set of attractors in the space of probability distributions. When assumptions about independence and existence of second moments that lead to the CLT fail, we should examine other limit distributions that may lead to behavior that is qualitatively different from that of a pleasant Gaussian random walk. But, that is a story for a later blog post :) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"math_sci_tech","title":"\"The Invisible Hand\""},{"url":"greta-playground.html","text":"A first foray into probabilistic programming with Greta Models and modelling Much of science - physical and social - is devoted to positing mechanisms that explain how the data we observe are generated. In a classic example, after Tycho Brahe made detailed observations of planetary motion ( here is data on mars), Johannes Kepler posited laws of planetary motion that explained how this data were generated. Effectively, modelling is the art of constructing data generators that help us understand and predict. Statistical models are one class of models that aim to construct - given some observed data - the probability distribution from which the data were drawn. That is, given a sample of data, a statistical model is a hypothesis about how this data were generated. In practice, this happens in two steps : - constructing a hypothesis, or a model \\(H\\) parametrized by some parameters \\(\\theta\\) , - finding ( inferring ) the distribution of parameters \\(\\theta\\) or, the most suitable parameters \\(\\theta\\) given the observed data What parameters are \"most suitable\" is indicated (in a particular sense of the word \"suitable\" will become clear in the following discussion) by the likelihood function that quantifies how probable the observed data set is, for a given hypothesis parametrized by some particular parameters \\(H_{\\theta}\\) . Understandably, we want to find parameters such that the observed data is the most likely, this is called maximum likelihood estimation . Since all but the simplest models are analytically intractable (i.e., the maximum of the likelihood function needs to be evaluated numerically and parameter distributions are even harder to compute) it makes sense to construct general rules and syntax to easily define statistical models and quickly infer their parameters. This is the field of probabilistic programming. Probabilistic programming The probabilistic programming language (PPL) has two tasks : be able to construct a useful class of statistical models be able to infer the parameters (and their distributions) of this class of models given some observed data. As has been explained in this excellent paper introducing the PPL Edward that is based on Python and Tensorflow , some PPLs restrict the class of models they allow in order to optimize the inference algorithm, while other emphasize expressiveness and sacrifice performance of the inference algorithms. Modern PPLs like Edward , Pyro , and the R based Greta use the robust infrastructure (hardware and software) that was first developed in the context of deep learning and thus ensure scalability and performance while being expressive. The tensor and the computational graph The fundamental data structure of this group of languages is the tensor which is just a multidimensional array. Data, model parameters, samples from distributions are all stored in tensors. All the manipulations that go into the construction of the output tensor constitute the computational graph (see this for an exceptionally clear exposition of the concept) associated with that tensor. Data and parameter tensors are inputs to the computational graph. In the context of deep learning, \"training\" consists of the following steps : Randomly initializing the parameter tensors Computing the output Measuring the error compared to the real/desired output Tweaking the parameter tensors to reduce the error. The algorithm that does this is called back propagation . Thus, the objective in deep learning or machine learning is to obtain the best values (in the sense of that they minimize error on the training set) of the parameters given some data. The objective of probabilistic modelling is subtly different. The aim here is to obtain the distribution (called posterior distribution ) of parameters, given the data. If we denote the data by \\(D\\) , Bayes theorem relates (for a particular hypothesis about how the data were generated \\(H\\) ), the likelihood of the data given some parameters \\(P(D|\\theta,H)\\) , our prior expectations about how the parameters are distributed \\(P(\\theta)\\) and the posterior distribution of the parameters themselves \\(P(\\theta|D,H)\\) : $$P(\\theta|D,H) = \\frac{P(D|\\theta,H)P(\\theta)}{P(D)}.$$ The priors \\(P(\\theta)\\) do not depend on the data and encode \"domain knowledge\" while the probability of the data set \\(P(D)\\) over the whole parameter space is (typically) a high dimensional integral given by $$P(D|H) = \\int P(D,\\theta|H)d\\theta.$$ Intuitively, we can see that the most likely parameters given the data, i.e. the parameters \\(\\theta\\) which maximize \\(P(\\theta|D,H)\\) ought to correspond to the sense of \"best\" or \"most suitable\" described above. From Bayes theorem, it is clear that the posterior distribution is directly proportional to the likelihood \\(P(\\theta|D,H) \\propto P(D|\\theta,H)\\) . Thus, maximizing likelihood is one way to get estimates of the \"most likely parameters\" (in the limit of infinite data), but computing the full distribution \\(P(\\theta|D,H)\\) involves dealing with the difficult integral for \\(P(D|H)\\) . Bayesian prediction and MCMC Prediction in this framework is also fundamentally different from typical machine learning model. The probability of a new data point \\(d\\) , $$P(d|D,H) = \\int P(d|\\theta,H)P(\\theta|D,H)d\\theta,$$ which consists of the expectation value of the new data point over the whole distribution of parameters given the observed data (the posterior distribution calculated obtained from the solution to the inference problem), instead of a value calculated by plugging in the \"learned parameter values\" into the machine learning model. The integrals needed for inference ( \\(P(D|H) = \\int P(D,\\theta|H)d\\theta\\) as well as prediction \\(P(d|D,H) = \\int P(d|\\theta,H)P(\\theta|D,H)d\\theta\\) need to be evaluated over the entire parameter space of the model which can be very high dimensional. Markov Chain Monte Carlo methods are used to approximate these integrals. This is an excellent overview of modern Hamiltonian Monte Carlo methods while this provides wonderful perspective from the dawn of the field. Both papers are long but eminently readable and highly recommended. Clearly then, along with the computational graph to define models, a PPL needs a good MCMC algorithm (or another inference algorithm) to compute the high dimensional integrals needed to infer as well as perform a prediction on a general probabilistic model. A broad overview of Bayesian machine learning is available here (PDF) and here Now, we illustrate some of these points using the simplest possible example, linear regression. Basic linear regression. We will generate artificial data with known parameters, so that we can check if Greta (the PPL we are using for this article) gets it right later. Generating fake data to fit a model to length_of_data <- 100 sd_eps <- pi &#94; exp ( 1 ) intercept <- -5.0 slope <- pi x <- seq ( -10 * pi , 10 * pi , length.out = length_of_data ) y <- intercept + slope * x + rnorm ( n = length_of_data , mean = 0 , sd = sd_eps ) data <- data_frame ( y = y , x = x ) ggplot ( data , aes ( x = x , y = y , colour = \"simulated dependent variable\" )) + geom_point () + geom_smooth ( method = 'lm' ) + ggtitle ( \"Fake experimental data\" ) + ggthemes :: theme_economist () Given this data, we want to write Greta code to infer the posterior distributions of the model parameters. Defining clueless priors for model parameters In this case, the parameters of our model are simple, but in principle, they can be arbitrary tensors. Since we really don't know anything about the prior distributions of our parameters, we look at the experimental data and take rough, uniform priors. intercept_p <- uniform ( -10 , 10 ) sd_eps_p <- uniform ( 0 , 50 ) slope_p <- uniform ( 0 , 10 ) Defining the model mean_y <- intercept_p + slope_p * x distribution ( y ) <- normal ( mean_y , sd_eps_p ) Here, we hypothesize that the target variable \\(y\\) is linearly dependent on some independent variable \\(x\\) with a noise term drawn from a Gaussian distribution whose standard deviation is also a parameter of the model. Under the hood, Greta has constructed a computational graph that encapsulates all these operations, and defines the process of computing the target \\(y\\) starting from the prior distributions of our input variables. We plot this computational graph below : our_model <- model ( intercept_p , slope_p , sd_eps_p ) our_model %>% plot () Inference There are two distinct types of inference possible, Sampling from the full posterior distribution for the parameters given the data and the model. Maximizing likelihood to compute \"most probable\" parameters given the data and the model. Sampling from the posterior distribution of parameters with MCMC num_samples <- 1000 param_draws <- mcmc ( our_model , n_samples = num_samples , warmup = num_samples / 10 ) and plot the densities of samples drawn from the parameter posterior distributions, and the parameter fits. mcmc_dens ( param_draws ) mcmc_intervals ( param_draws ) By inspection, it looks like the HMC has found reasonable values for our model parameters and their posterior distributions. Most probable parameters Explicitly, the mean estimates can be computed from the param_draws data structure, or via the greta::opt function. param_draws_df <- as_data_frame ( param_draws [[ 1 ]]) param_estimates <- param_draws_df %>% summarise_all ( mean ) param_estimates %>% print () ## # A tibble: 1 x 3 ## intercept_p slope_p sd_eps_p ## <dbl> <dbl> <dbl> ## 1 -7.24 3.22 21.2 opt_params <- opt ( our_model ) opt_params $ par %>% print () ## intercept_p slope_p sd_eps_p ## -8.367414 3.098064 19.139992 Bayesian prediction Bayesian prediction is implemented via the calculate() function available in the latest release of greta on github. This generates a prediction on \\(y\\) for each draw from the posterior distribution of the parameters (see previous section). Taking the expectation over this distribution of predictions gives us the mean value of the target variable \\(y\\) but we have the whole distribution of \\(y\\) available to us if we need to analyse it. mean_y_plot <- intercept_p + slope_p * x mean_y_plot_draws <- calculate ( mean_y_plot , param_draws ) mean_y_est <- colMeans ( mean_y_plot_draws [[ 1 ]]) data_pred <- data %>% mutate ( y_fit = mean_y_est ) ggplot ( data_pred ) + geom_point ( aes ( x , y , colour = \"simulated dependent variable\" )) + geom_line ( aes ( x , y_fit , colour = \"estimated expectation value\" )) + ggtitle ( \"Fitted model\" ) + ggthemes :: theme_economist () Further exploration The most mature PPL out there (with good R bindings) is Stan. There is a lot of material available, and it might be a good place to start to pick up some intuition. See this page . This is a good intro to the role of MCMC in inference. These video lectures on statistical rethinking emphasizing Bayesian statistics also seem interesting. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"math_sci_tech","title":"\"greta playground\""},{"url":"tiger-hill.html","text":"1. \"I've checked\", the Principal said one morning at assembly \"The hill behind the school does not seem to have a name. In the memory of those who died in Kashmir, we will call it Tiger Hill. So that we never forget.\" It was the monsoon of 1999. I stood there in my first, never-shaved moustache and navy blue uniform, cold despite the blazer and the multitudes who stood silently around me. Princi - as we affectionately called him - spoke on for a while about duty, country, bravery, gratitude and other noble things, and my eyes were moist but I was not really listening. I had long since drifted off into one of my all too frequent day dreams, off in some foreign land unknown to science, exploring, fighting, upholding some nebulous idea of civilisation. Dancing behind my patriotic tears were visions of erudite men in khakis who could shoot straight and fight for the country (which country ?) when they were not writing poetry, observing birds or charming ladies. Our history textbooks were (of course) filled with accounts of India's heroic fight for independence and the atrocities of the British. While I was suitably moved by the sacrifices of the freedom fighters and shocked by the brutality and unfairness of the colonial regime, (and I admit this to myself only now) my loyalties lay firmly with the empire builders. In fact, I positively loved the Empire. How do I know this ? Because in my daydreams I was never a revolutionary or even an Indian. I was always a scientist/explorer, implicitly European, privileged, male. I dreamt of being the strapping officer in Africa facing the maneating lions of Tsavo to get a railway built (never mind that Indian slave labourers were used and killed in large numbers ) or one of those promising young archaeologists Doyle liked to write about, bringing treasures from Egypt back \"home\" to London or even one of Jim Corbett's sportsmen friends trudging through the foothills tracking some maneater or the other, never mind that for all his love of India and Indians, Corbett moved to Kenya the moment India became independent. No doubt he thought the place would go to the dogs directly after the British left. 2. In the years before Kargil, before our Principal baptised the hill, before my moustache and my long pants, the hill was my first haunt outside the city. One could cycle from my house past the dargah and the unguarded railway crossing and into the wide open countryside. Golden-brown in summer - the dry grass shimmering in the hot wind - emerald green come the monsoon. We would ride out on cycles too big for us, in the rain, past the rushing streams in their little canyons and the tiny crabs clambering over bare rock, riding out to the mist covered hills that dominated the horizon. We rarely got very far though, the mud that clogged up our wheels combined with the hunger that was ever gnawing at our young, rapidly growing bodies always ensured we were home well in time for the next meal. Occasionally, we would make it all the way to school, and the hill. It is a peculiar type of hill commonly seen on the Deccan, with a one dimensional summit - long, thin and flat. This particular hill also tapered toward the back giving it the appearance of a Sphinx with its head lopped off. And if you stood where the head might have been, you could see the city in the far distance and the hills on the other side of it. Some with a masjid on top, some with temples, sometimes even a tree to liven up the flat, shaven countryside. And directly below you - at your feet - the school, with its fields and buildings arranged in a wide arc. The wind carried soft voices all the way to the top of the hill so that you heard them but could not understand what they were saying. Sheep and cows grazed on the hillside, a bell tinkled occasionally. It was beautiful, and I love it with a love I find impossible to articulate. 3. In my early years Haggard and Doyle and Stevenson and Defoe invited me to find blanks on the map, rough seas and fierce natives, mysterious artefacts and legends of treasure, forgotten kingdoms and ruined cities. Then in 7th standard, a mathematically minded Vice Principal gifted me Arthur C. Clarke's 2001 : A Space Odyssey. I devoured it, and then devoured every other Clarke book I could find (an obsession that continues to this day) and the nature of my affliction changed forever. Clarke invited me to a world altogether more civilised than the one my daydreams had conjured, but no less marvelous. I now dreamt of exploration and adventure via international scientific bureaucracy - minor astronomers and common engineers on lonely space stations and planetary bases, a gentle, optimistic post-national future threatened only by the vastness and indifference of the universe. What could be more meaningful ? What could possibly compete ? Long before I had heard of Star Trek in my little town on the Deccan, I yearned for Starfleet. I remember the exact moment - sometime in middle school - when I decided what kind of person I wanted to be. In 2010 : Odyssey Two, the narrator describes Dr. Chandra (the man behind the infamous sentient computer HAL who joined the Russian ship Leonov on its voyage to rescue the American ship Discovery) as having \"an educated Indian accent\". And as I read those words, I knew, I knew I wanted to be a dislocated scientist, far away, surrounded by foreigners, but (hopefully) with an \"educated Indian accent\". The idea had an unbearable attraction. Despite the golden grass shimmering in the summer sun and the emerald monsoon hills hiding in the mist, despite family and country and patriotism, I knew I had to leave. The yearning to be part of something greater was too strong, the need to explore, push mankind forward somehow, to belong elsewhere. What happened then ? The last man landed on the moon over a decade before I was born, and it seems unlikely that humanity will go anywhere in the next 50 years. But, even our complacent age is not without its attractions. The efforts to understand biology and disease using math and physics might lay to rest some more of our oldest enemies, private companies are rushing into space and billions of people from formerly deprived nations now find a voice in the global cacophony. Despite this - and despite having participated in some of it - I remain discomfited. I do not belong, there is no greater Cause, there is no Starfleet. 4. I remember passing through Kargil a few years after leaving my school and my hometown. Our car stopped, a short walk away from the base of the real, original Tiger Hill. Massive, menacing, unbelievably close to the national highway. I wondered how many Indian army men died there, storming the steep, steep slopes in the darkness in a hail of bullets to drive the insurgents from their entrenched positions. The Light Brigade could hardly have been more valiant. And Tiger Hill was only one of hundreds of such peaks that the Indian army stormed, young officers leading from the front, dying shortly after TV interviews. As one of them famously said, \"Yeh dil maange more !\". I felt it keenly then, my Indian-ness. Here, a range of lofty and blood stained mountains that separates my country from another, very different one. There, the Siachen glacier and the Indira Col watershed. A drop of water on this side will flow into the Indus, onto the bustling Indian sub-continent. A drop of water on the other side flows into Central Asia to cities like Kashgar, Khokand, Bokhara, broad, empty plains cut by high mountains, verdant valleys and old old caravan routes and those ancient rivers, the Amu and Syr Darya. A very different country. To settle down anywhere but India will be to become of that place and my love for my town and my country will not allow that. But my love remains uncomfortable with returning. It is essentially a love of absence and nostalgia, of tragedy and loneliness, of being far away, a love that wants to yearn to be home, but does not want to be at home. And so I must keep moving, travelling, relocating, relearning. There is nothing else I have ever dreamt of doing, little else I wish to do. Until something fills the hole in my heart that my longing for India occupies, and finally allows me to go home. /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */ var disqus_shortname = 'theclarkeorbit'; // required: replace example with your forum shortname /* * * DON'T EDIT BELOW THIS LINE * * */ (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); Please enable JavaScript to view the comments powered by Disqus.","tags":"blog","title":"Tiger Hill"}]}