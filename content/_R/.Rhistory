setwd("~//Dropbox/website/")
# if Mac
# system("bash publish_on_mac.sh")
# if ubuntu
#system("git clone --recursive https://github.com/getpelican/pelican-themes ~/pelican-themes")
# system("pip install markdown typogrify beautifulsoup4 pelican")
system("bash publishsite.sh")
library(torch)
library(luz)
library(tidyverse) #Because what can one do without the tidyverse
library(ggthemes) #for Tufte
sentence <- "Life is short, eat dessert first"
# Remove the comma and split the sentence into words
words <- strsplit(gsub(",", "", sentence), " ")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
sentence <- "Life is short, eat dessert first, okay."
# Remove the comma and split the sentence into words
words <- strsplit(gsub(",", "", sentence), " ")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
sentence <- "Life is short, eat dessert first, okay."
# Replace all non-word characters with spaces and split the sentence into words
words <- strsplit(gsub("[^\\w\\s]", " ", sentence), "\\s+")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
sentence <- "Life is short, eat dessert first"
# Remove the comma and split the sentence into words
words <- strsplit(gsub(",", "", sentence), " ")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
sentence <- "Life is short, eat dessert first"
# Remove the comma and split the sentence into words
words <- strsplit(gsub("[^\\w\\s]", "", sentence), " ")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
sentence <- "Life is short, eat dessert first"
# Remove the comma and split the sentence into words
words <- strsplit(gsub("[^\w\s]", "", sentence), " ")[[1]]
sentence <- "Life is short, eat dessert first"
# Remove the comma and split the sentence into words
words <- strsplit(gsub("[^ws]", "", sentence), " ")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
sentence <- "Life is short, eat dessert first - okay?"
# Remove the comma and split the sentence into words
words <- strsplit(str_replace_all(sentence, "[[:punct:]]", " "), " ")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
sentence <- "Life is short, eat dessert first - okay?"
# Remove the comma and split the sentence into words
words <- strsplit(str_replace_all(sentence, "[[:punct:]]", " +"), " ")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
sentence <- "Life is short, eat dessert first - okay?"
# Remove the comma and split the sentence into words
words <- strsplit(str_replace_all(sentence, "[[:punct:]]", "+"), " ")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
sentence <- "Life is short, eat dessert first - okay?"
# Remove the comma and split the sentence into words
words <- strsplit(str_replace_all(sentence, "[[:punct:]]", " "), " +")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
sentence <- "Life is short, eat dessert first - okay?"
# Remove the comma and split the sentence into words
words <- strsplit(str_replace_all(sentence, "[[:punct:]]", " "), " +")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
# Create a torch tensor from the indices
sentence_tensor <- torch_tensor(sentence_int, dtype = torch_long())
sentence <- "Life is short, eat dessert first - okay?"
# Remove the comma and split the sentence into words
words <- strsplit(str_replace_all(sentence, "[[:punct:]]", " "), " +")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
# Convert sentence words to indices using the dictionary
sentence_int <- sapply(strsplit(str_replace_all("first is short", "[[:punct:]]", " "), " +")[[1]], function(x) dc[x])
# Create a torch tensor from the indices
sentence_tensor <- torch_tensor(sentence_int, dtype = torch_long())
# Print the tensor
print(sentence_tensor)
sentence <- "Life is short, eat dessert first - okay?"
# Remove the comma and split the sentence into words
words <- strsplit(str_replace_all(sentence, "[[:punct:]]", " "), " +")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
# Convert sentence words to indices using the dictionary
sentence_int <- sapply(strsplit(str_replace_all("first eat short dessert", "[[:punct:]]", " "), " +")[[1]], function(x) dc[x])
# Create a torch tensor from the indices
sentence_tensor <- torch_tensor(sentence_int, dtype = torch_long())
# Print the tensor
print(sentence_tensor)
sentence <- "Life is short, eat dessert first - okay? okay."
# Remove the comma and split the sentence into words
words <- strsplit(str_replace_all(sentence, "[[:punct:]]", " "), " +")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices - the dictionary
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
# Convert sentence words to indices using the dictionary
sentence_int <- sapply(strsplit(str_replace_all("first eat short dessert", "[[:punct:]]", " "), " +")[[1]], function(x) dc[x])
# Create a torch tensor from the indices
sentence_tensor <- torch_tensor(sentence_int, dtype = torch_long())
# Print the tensor
print(sentence_tensor)
sentence <- "Life is short, eat dessert first - okay?"
# Remove the comma and split the sentence into words
words <- strsplit(str_replace_all(sentence, "[[:punct:]]", " "), " +")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices - the dictionary
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
# Convert sentence words to indices using the dictionary
sentence_int <- sapply(strsplit(str_replace_all("first eat short dessert", "[[:punct:]]", " "), " +")[[1]], function(x) dc[x])
# Create a torch tensor from the indices
sentence_tensor <- torch_tensor(sentence_int, dtype = torch_long())
# Print the tensor
print(sentence_tensor)
sentence <- "Life is short, eat dessert first - okay?"
# Remove the comma and split the sentence into words
words <- strsplit(str_replace_all(sentence, "[[:punct:]]", " "), " +")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices - the dictionary
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
# Convert sentence words to indices using the dictionary
sentence_int <- sapply(strsplit(str_replace_all("first eat short dessert", "[[:punct:]]", " "), " +")[[1]], function(x) dc[x])
# Create a torch tensor from the indices
sentence_tensor <- torch_tensor(sentence_int, dtype = torch_long())
# Print the tensor
print(sentence_tensor)
# Apply embedding to the sentence tensor
embedded_sentence <- embed(sentence_tensor)$detach()
?embed
?torch::nnf_embedding
sentence <- "Life is short, eat dessert first - okay?"
# Remove the comma and split the sentence into words
words <- strsplit(str_replace_all(sentence, "[[:punct:]]", " "), " +")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices - the dictionary
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
# Convert sentence words to indices using the dictionary
sentence_int <- sapply(strsplit(str_replace_all("first eat short dessert", "[[:punct:]]", " "), " +")[[1]], function(x) dc[x])
# Create a torch tensor from the indices
sentence_tensor <- torch_tensor(sentence_int, dtype = torch_long())
# Print the tensor
print(sentence_tensor)
# Define an embedding layer
embed <- nn_embedding(num_embeddings = vocab_size, embedding_dim = embedding_dim)
sentence <- "Life is short, eat dessert first - okay?"
# Remove the comma and split the sentence into words
words <- strsplit(str_replace_all(sentence, "[[:punct:]]", " "), " +")[[1]]
# Sort the words
sorted_words <- sort(words)
# Create a named vector with indices - the dictionary
dc <- setNames(seq_along(sorted_words), sorted_words)
# Print the result
print(dc)
# Convert sentence words to indices using the dictionary
sentence_int <- sapply(strsplit(str_replace_all("first eat short dessert", "[[:punct:]]", " "), " +")[[1]], function(x) dc[x])
# Create a torch tensor from the indices
sentence_tensor <- torch_tensor(sentence_int, dtype = torch_long())
# Print the tensor
print(sentence_tensor)
vocab_size <- 50000
embedding_dim <- 3
# Define an embedding layer
embed <- nn_embedding(num_embeddings = vocab_size, embedding_dim = embedding_dim)
# Apply embedding to the sentence tensor
embedded_sentence <- embed(sentence_tensor)$detach()
# Print the embedded sentence and its shape
print(embedded_sentence)
print(dim(embedded_sentence))
?nn_multihead_attention
attention_l <- nn_multihead_attention(embed_dim = 10, num_heads = 1)
attention_l(embedded_sentence)
attention_l(embedded_sentence)
attention_l(embedded_sentence)$detach()
attention_1
attention_l <- nn_multihead_attention(embed_dim = 3, num_heads = 1)
attention_1
attention_l
attention_l(embedded_sentence)
attention_l
?nnf_multi_head_attention_forward
attention_l(embedded_sentence, embedded_sentence, embedded_sentence)
?nn_multihead_attention
query <- embedded_sentence |> unsqueeze()
query <- embedded_sentence$unsqueeze()
query <- embedded_sentence$unsqueeze(1)
query <- embedded_sentence$unsqueeze(1)$unsqueeze(1)
dim(query)
query <- embedded_sentence$unsqueeze(3)
dim(query)
query <- embedded_sentence$unsqueeze(1)
attention_l(query,query,query)
query <- embedded_sentence$unsqueeze(2)
attention_l(query,query,query)
embedded_sentence |> dim()
query |> dim()
dir <- "~/.torch-datasets"
img_ds <- tiny_imagenet_dataset(
dir,
download = TRUE,
transform = function(x) {
x %>%
transform_to_tensor()
}
)
dir <- "~/.torch-datasets"
library(torchvision)
img_ds <- tiny_imagenet_dataset(
dir,
download = TRUE,
transform = function(x) {
x %>%
transform_to_tensor()
}
)
getOption('timeout')
options(timeout=100)
options(timeout=100000)
library(torch)
library(luz)
library(torchvision)
library(tidyverse) #Because what can one do without the tidyverse
library(ggthemes) #for Tufte
dir <- "~/.torch-datasets"
img_ds <- tiny_imagenet_dataset(
dir,
download = TRUE,
transform = function(x) {
x %>%
transform_to_tensor()
}
)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, dpi = 300)
library(tidyverse)
library(lubridate)
library(dagitty)
library(dosearch)
library(broom)
library(sandwich)
library(lmtest)
library(ggthemes)
library(knitr)
library(kableExtra)
# Simple visualization for HTML output
library(ggdag)
dag <- dagify(
Y ~ X + Z + C,
X ~ Z + C,
C ~ Z,
exposure = "X",
outcome = "Y",
labels = c(Y = "Sales (Y)", X = "Focal Coupon (X)",
Z = "Propensity (Z)", C = "Other Coupons (C)")
)
ggdag(dag, text = FALSE, use_labels = "label") +
theme_dag_blank() +
labs(title = "Coupon Campaign DAG")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, dpi = 300)
library(tidyverse)
library(lubridate)
library(dagitty)
library(dosearch)
library(broom)
library(sandwich)
library(lmtest)
library(ggthemes)
library(knitr)
library(kableExtra)
# Simple visualization for HTML output
library(ggdag)
dag <- dagify(
Y ~ X + Z + C,
X ~ Z + C,
C ~ Z,
exposure = "X",
outcome = "Y",
labels = c(Y = "Sales (Y)", X = "Focal Coupon (X)",
Z = "Propensity (Z)", C = "Other Coupons (C)")
)
ggdag(dag, text = FALSE, use_labels = "label") +
theme_dag_blank() +
labs(title = "Coupon Campaign DAG")
# =============================================================================
# LOAD DATA
# =============================================================================
train                <- readr::read_csv("~/Documents/causal_talk_5thel/mlcausal_data/train.csv")
campaign_data        <- readr::read_csv("~/Documents/causal_talk_5thel/mlcausal_data/campaign_data.csv")
coupon_item_mapping  <- readr::read_csv("~/Documents/causal_talk_5thel/mlcausal_data/coupon_item_mapping.csv")
customer_demo        <- readr::read_csv("~/Documents/causal_talk_5thel/mlcausal_data/customer_demographics.csv")
customer_txn         <- readr::read_csv("~/Documents/causal_talk_5thel/mlcausal_data/customer_transaction_data.csv")
item_data            <- readr::read_csv("~/Documents/causal_talk_5thel/mlcausal_data/item_data.csv")
# =============================================================================
# DEFINE COUPON CATEGORIES (following Langen & Huber)
# =============================================================================
ready2eat_cats     <- c("Bakery", "Restaurant", "Prepared Food", "Dairy, Juices & Snacks")
drugstore_cats     <- c("Pharmaceutical", "Skin & Hair Care")
other_food_cats    <- c("Grocery", "Salads", "Vegetables (cut)", "Natural Products")
other_nonfood_cats <- c("Flowers & Plants", "Garden", "Travel", "Miscellaneous")
# =============================================================================
# CLEAN TRANSACTIONS (remove B2B/wholesale outliers)
# =============================================================================
customer_txn <- customer_txn |>
dplyr::mutate(total_value = quantity * selling_price)
threshold <- min(quantile(customer_txn$total_value, 0.95, na.rm = TRUE), 2000)
customer_txn_clean <- customer_txn |>
dplyr::filter(total_value <= threshold)
# =============================================================================
# PARSE CAMPAIGN DATES
# =============================================================================
campaign_data <- campaign_data |>
dplyr::mutate(
start_date = lubridate::dmy(start_date),
end_date   = lubridate::dmy(end_date),
camp_days  = as.numeric(end_date - start_date) + 1
)
# =============================================================================
# BUILD CUSTOMER-CAMPAIGN PANEL
# =============================================================================
# Treated: customers who received coupons
cust_with_coupons <- train |>
dplyr::distinct(customer_id, campaign_id)
# Control candidates: customers who transacted during campaigns but got no coupons
cust_transacted <- customer_txn_clean |>
dplyr::select(customer_id, date) |>
tidyr::crossing(campaign_data |> dplyr::select(campaign_id, start_date, end_date)) |>
dplyr::filter(date >= start_date, date <= end_date) |>
dplyr::distinct(customer_id, campaign_id)
# Union of both sources
cust_camp_full <- dplyr::bind_rows(cust_with_coupons, cust_transacted) |>
dplyr::distinct(customer_id, campaign_id) |>
dplyr::left_join(campaign_data, by = "campaign_id") |>
dplyr::mutate(
received_any_coupon = paste(customer_id, campaign_id, sep = "_") %in%
paste(cust_with_coupons$customer_id, cust_with_coupons$campaign_id, sep = "_")
)
# =============================================================================
# CONSTRUCT Z: Pre-campaign propensity index
# =============================================================================
pre_window <- 60
pre_spend <- customer_txn_clean |>
dplyr::inner_join(
cust_camp_full |> dplyr::select(customer_id, campaign_id, start_date),
by = "customer_id"
) |>
dplyr::filter(date >= start_date - pre_window, date < start_date) |>
dplyr::group_by(customer_id, campaign_id) |>
dplyr::summarise(
pre_daily = sum(total_value, na.rm = TRUE) / pmax(dplyr::n_distinct(date), 1),
.groups = "drop"
)
cust_camp <- cust_camp_full |>
dplyr::left_join(pre_spend, by = c("customer_id", "campaign_id")) |>
dplyr::mutate(Z = tidyr::replace_na(pre_daily, 0))
# =============================================================================
# CONSTRUCT X: Treatment indicators by coupon category
# =============================================================================
coupon_with_cat <- coupon_item_mapping |>
dplyr::left_join(item_data, by = "item_id") |>
dplyr::group_by(coupon_id) |>
dplyr::summarise(main_category = names(sort(table(category), decreasing = TRUE))[1]) |>
dplyr::mutate(
is_ready2eat    = main_category %in% ready2eat_cats,
is_drugstore    = main_category %in% drugstore_cats,
is_other_food   = main_category %in% other_food_cats,
is_other_nonfood = main_category %in% other_nonfood_cats
)
treatment_flags <- train |>
dplyr::select(customer_id, campaign_id, coupon_id) |>
dplyr::left_join(coupon_with_cat, by = "coupon_id") |>
dplyr::group_by(customer_id, campaign_id) |>
dplyr::summarise(
X_ready2eat     = as.integer(any(is_ready2eat, na.rm = TRUE)),
X_drugstore     = as.integer(any(is_drugstore, na.rm = TRUE)),
X_other_food    = as.integer(any(is_other_food, na.rm = TRUE)),
X_other_nonfood = as.integer(any(is_other_nonfood, na.rm = TRUE)),
n_coupons       = dplyr::n(),
.groups = "drop"
)
cust_camp <- cust_camp |>
dplyr::left_join(treatment_flags, by = c("customer_id", "campaign_id")) |>
dplyr::mutate(
across(starts_with("X_"), ~tidyr::replace_na(.x, 0L)),
n_coupons  = tidyr::replace_na(n_coupons, 0L),
any_coupon = as.integer(n_coupons > 0)
)
# =============================================================================
# CONSTRUCT Y: Campaign-period average daily spending
# =============================================================================
txn_daily <- customer_txn_clean |>
dplyr::group_by(customer_id, date) |>
dplyr::summarise(daily_spend = sum(total_value, na.rm = TRUE), .groups = "drop")
campaign_spend <- cust_camp |>
dplyr::distinct(customer_id, campaign_id, start_date, end_date, camp_days) |>
dplyr::left_join(txn_daily, by = "customer_id", relationship = "many-to-many") |>
dplyr::filter(date >= start_date, date <= end_date) |>
dplyr::group_by(customer_id, campaign_id, camp_days) |>
dplyr::summarise(Y = sum(daily_spend, na.rm = TRUE) / first(camp_days), .groups = "drop")
analysis_df <- cust_camp |>
dplyr::left_join(campaign_spend, by = c("customer_id", "campaign_id", "camp_days")) |>
dplyr::mutate(Y = tidyr::replace_na(Y, 0))
# Trim Y outliers at 99th percentile
y_99 <- quantile(analysis_df$Y, 0.99, na.rm = TRUE)
analysis_df <- analysis_df |> dplyr::filter(Y <= y_99)
# =============================================================================
# BUILD ANALYSIS SAMPLES (Langen & Huber methodology)
# =============================================================================
# For each coupon type: compare (has this coupon) vs (no coupons at all)
# C_other = 1 if customer received OTHER coupon types
df_drugstore <- analysis_df |>
dplyr::filter(X_drugstore == 1 | any_coupon == 0) |>
dplyr::mutate(
X = X_drugstore,
C_other = as.integer(X_ready2eat == 1 | X_other_food == 1 | X_other_nonfood == 1)
) |>
dplyr::select(customer_id, campaign_id, Z, X, Y, C_other)
df_ready2eat <- analysis_df |>
dplyr::filter(X_ready2eat == 1 | any_coupon == 0) |>
dplyr::mutate(
X = X_ready2eat,
C_other = as.integer(X_drugstore == 1 | X_other_food == 1 | X_other_nonfood == 1)
) |>
dplyr::select(customer_id, campaign_id, Z, X, Y, C_other)
sample_sizes <- tibble::tibble(
coupon_type = c("Drugstore", "Ready-to-eat"),
n           = c(nrow(df_drugstore), nrow(df_ready2eat)),
treated     = c(sum(df_drugstore$X == 1), sum(df_ready2eat$X == 1)),
control     = c(sum(df_drugstore$X == 0), sum(df_ready2eat$X == 0))
)
sample_sizes |>
knitr::kable(
col.names = c("Coupon type", "N", "Treated (X = 1)", "Control (X = 0)"),
caption   = "Sample sizes by coupon type and treatment status"
) |>
kableExtra::kable_styling(full_width = FALSE)
# Check adjustment sets
dagitty::adjustmentSets(dag, exposure = "X", outcome = "Y")
# Verify with dosearch
data_spec  <- "P(X, Y, Z, C)"
query_spec <- "P(Y | do(X))"
dosearch::dosearch(data_spec, query_spec, dag)
