---
title: "A little r-torch"
Date: 29 Apr 2024
output:
  html_document:
    df_print: paged
---

These notes are meant to impliment little examples from [Francois Fleuret's](https://fleuret.org/francois/index.html) Little Book of Deep Learning ([pdf link](https://fleuret.org/public/lbdl.pdf)) in r-torch. I'm writing these as a fun way to dive into torch in R while surveying DL quickly. 

You'll need to have the book with you to understand these notes, but since it is available freely, that ought not to be an issue.


```{r setup, include=FALSE}
library(torch)
library(luz)
library(tidyverse) #Because what can one do without the tidyverse
library(ggthemes) #for Tufte
```

### Basis function regression

We will generate synthetic data that is similar to the example shown in the book:

```{r}
# Number of samples
n <- 50

# Randomly distributed x values from -1 to 1
set.seed(123) # for reproducibility
x <- runif(n, min = -1, max = 1)
x <- sort(x) # Sorting for plotting purposes

# y values as the semi-circle
y <- sqrt(1 - x^2)

# Plotting the original semi-circle with randomly distributed x
data <- data.frame(x = x, y = y)
ggplot(data, aes(x, y)) + 
  geom_point() + 
  ggtitle("Semi-Circle") +
  theme_tufte()
```

Following the book, we use gaussian kernels as the basis functions to fit $y \sim f(x;w)$, where $w$ are the weights of the basis functions.

```{r}
# Define Gaussian basis functions
basis_functions <- function(x, centers, scale = 0.1) {
  exp(-((x - centers)^2) / (2 * scale^2))
}

# Centers of the Gaussian kernels, these do cover the region of space we are interested in
centers <- seq(-1, 1, length.out = 10)
```

Now, we define our model for $y$, which, for basis function regression, is a linear combination of the basis functions initialized with random weights $w$. 

```{r}
# Initial random weights
weightss <- torch_randn(length(centers), requires_grad = TRUE)

# Calculate the model output
model_y <- function(x) {
  # Convert x to a torch tensor if it isn't already one
  x_tensor <- torch_tensor(x)
  
  # Create a tensor for the basis functions evaluated at each x
  # Resulting tensor will have size [length(x), length(centers)]
  basis_matrix <- torch_stack(lapply(centers, function(c) basis_functions(x_tensor, c)), dim = 2)
  
  # Calculate the output using matrix multiplication
  # basis_matrix is [n, 10] and weights is [10, 1]
  y_pred <- torch_matmul(basis_matrix, weightss)
  
  # Flatten the output to match the dimension of y
  return(y_pred)
}
```

Now,we will use gradient descent to minimise the MSE between the model and the real values of $y$ to obtain the optimal weights $w^*$. 

```{r}
# Learning rate
lr <- 0.01

# Gradient descent loop
for (epoch in 1:5000) {
  y_pred <- model_y(x)
  
  op <- nnf_mse_loss(y_pred, torch_tensor(y))
  
  # Backpropagation
  op$backward()
  
  # Update weights
  with_no_grad({
    weightss$sub_(lr * weightss$grad)
    weightss$grad$zero_()
  })
}
```

Now, we can see how good our predictions are:
```{r}
# Get model predictions
fin_x <- seq(-1,1,length.out=100)
final_y <- model_y(fin_x)

# Plotting
predicted_data <- data.frame(x = fin_x, y = as.numeric(final_y))
ggplot() +
  geom_point(data = data, aes(x, y)) +
  geom_line(data = predicted_data, aes(x, y)) +
  ggtitle("Original and Fitted Semi-Circle, 50 points and 10 weights, basis functions") +
  theme_tufte()

```

**Underfitting** 
When the model is too small to capture the features of the data (in our case, too few weights)

```{r, echo=FALSE}
# Number of samples
n <- 50

# Randomly distributed x values from -1 to 1
set.seed(123) # for reproducibility
x <- runif(n, min = -1, max = 1)
x <- sort(x) # Sorting for plotting purposes

# y values as the semi-circle
y <- sqrt(1 - x^2)

# Plotting the original semi-circle with randomly distributed x
data <- data.frame(x = x, y = y)

centers <- seq(-1, 1, length.out = 5)

# Initial random weights
weightss <- torch_randn(length(centers), requires_grad = TRUE)

# Gradient descent loop
for (epoch in 1:5000) {
  y_pred <- model_y(x)
  
  op <- nnf_mse_loss(y_pred, torch_tensor(y))
  
  # Backpropagation
  op$backward()
  
  # Update weights
  with_no_grad({
    weightss$sub_(lr * weightss$grad)
    weightss$grad$zero_()
  })
}

# Get model predictions
fin_x <- seq(-1,1,length.out=100)
final_y <- model_y(fin_x)

# Plotting
predicted_data <- data.frame(x = fin_x, y = as.numeric(final_y))
ggplot() +
  geom_point(data = data, aes(x, y)) +
  geom_line(data = predicted_data, aes(x, y)) +
  ggtitle("Underfitting, 50 points and 5 weights, basis functions") +
  theme_tufte()
```

**Overfitting** 
When there is too little data to properly constrain the parameters of a larger model.

```{r, echo=FALSE}
# Number of samples
n <- 5

# Randomly distributed x values from -1 to 1
set.seed(123) # for reproducibility
x <- runif(n, min = -1, max = 1)
x <- sort(x) # Sorting for plotting purposes

# y values as the semi-circle
y <- sqrt(1 - x^2)

# Plotting the original semi-circle with randomly distributed x
data <- data.frame(x = x, y = y)

centers <- seq(-1, 1, length.out = 10)

# Initial random weights
weightss <- torch_randn(length(centers), requires_grad = TRUE)

# Gradient descent loop
for (epoch in 1:5000) {
  y_pred <- model_y(x)
  
  op <- nnf_mse_loss(y_pred, torch_tensor(y))
  
  # Backpropagation
  op$backward()
  
  # Update weights
  with_no_grad({
    weightss$sub_(lr * weightss$grad)
    weightss$grad$zero_()
  })
}

# Get model predictions
fin_x <- seq(-1,1,length.out=100)
final_y <- model_y(fin_x)

# Plotting
predicted_data <- data.frame(x = fin_x, y = as.numeric(final_y))
ggplot() +
  geom_point(data = data, aes(x, y)) +
  geom_line(data = predicted_data, aes(x, y)) +
  ggtitle("Overfitting, 5 points and 10 weights, basis functions") +
  theme_tufte()
```

### Autoregressive models

Let us generate data that looks similar to that shown in section 3.5 of the book. 

```{r}

# Function to generate C-shaped data
generate_c_data <- function(n, x_offset, y_offset, y_scale, label, xflipaxis) {
  theta <- runif(n, pi/2, 3*pi/2)  # Angle for C shape
  x <- cos(theta) + rnorm(n, mean = 0, sd = 0.1) + x_offset
  if(xflipaxis==T){
    x <- 1-x
  }
  y <- sin(theta) * y_scale + rnorm(n, mean = 0, sd = 0.1) + y_offset
  data.frame(x = x, y = y, label = label)
}

# Number of points per class
n_points <- 1000

# Generate data for both classes
data_class_1 <- generate_c_data(n_points, x_offset = 0, 
                                y_offset = 0, y_scale = 1, label = 1,
                                xflipaxis = F)
data_class_0 <- generate_c_data(n_points, x_offset = 1.25, 
                                y_offset = -1.0, y_scale = -1, label = 2,
                                xflipaxis = T)  # Mirrored and adjusted

# Combine data
data <- rbind(data_class_0, data_class_1)

# Plotting the data
ggplot(data, aes(x, y, color = as.factor(label))) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "Adjusted C-shaped Data for Classification", x = "X axis", y = "Y axis") +
  theme_tufte()
```

Now, we can build a very simple neural net to classify these points and try to visualize what the trained net is doing at each layer.

```{r, include=FALSE}
net <- nn_module(
  "ClassifierNet",
  initialize = function() {
    self$layer1 <- nn_linear(2, 2)
    self$layer2 <- nn_linear(2, 2)
    self$layer3 <- nn_linear(2, 2)
    self$layer4 <- nn_linear(2, 2)
    self$layer5 <- nn_linear(2, 2)
    self$layer6 <- nn_linear(2, 2)
    self$layer7 <- nn_linear(2, 2)
    self$output <- nn_softmax(2)
    # Initialize an environment to store activations
    self$activations <- list()
  },
  
  forward = function(x) {
    x <- self$layer1(x) |> tanh()
    self$activations$layer1 <- x
    x <- self$layer2(x) |> tanh()
    self$activations$layer2 <- x
    x <- self$layer3(x) |> tanh()
    self$activations$layer3 <- x
    x <- self$layer4(x) |> tanh()
    self$activations$layer4 <- x
    x <- self$layer5(x) |> tanh()
    self$activations$layer5 <- x
    x <- self$layer6(x) |> tanh()
    self$activations$layer6 <- x
    x <- self$layer7(x) |> tanh()
    self$activations$layer7 <- x
    x <- self$output(x)
    x
  }
)


# Convert the features and labels into tensors
features <- torch_tensor(as.matrix(data[c("x", "y")]))
labels <- torch_tensor(as.integer(data$label))

# Create a dataset using lists of features and labels
data_classif <- tensor_dataset(features, labels)

# Create a dataloader from the dataset
dataloaders <- dataloader(data_classif, batch_size = 100, shuffle = TRUE)

# defining the model
model <- net %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam
  ) %>%
  fit(dataloaders, epochs = 200)
```

Now, let us see (visually) how well the model predicts some new synthetic data generated similarly. 

```{r, echo=FALSE}
# Number of points per class
n_points <- 50

# Generate data for both classes
test_data_class_1 <- generate_c_data(n_points, x_offset = 0, 
                                y_offset = 0, y_scale = 1, label = 1,
                                xflipaxis = F)
test_data_class_0 <- generate_c_data(n_points, x_offset = 1.25, 
                                y_offset = -1.0, y_scale = -1, label = 2,
                                xflipaxis = T)  # Mirrored and adjusted
# Combine data
test_data <- rbind(test_data_class_0, test_data_class_1)

# Convert the features and labels into tensors
test_features <- torch_tensor(as.matrix(test_data[c("x", "y")]))
test_labels <- torch_tensor(as.integer(test_data$label))

# Create a dataset using lists of features and labels
test_data_classif <- tensor_dataset(test_features, test_labels)

preds <- predict(model, test_data_classif) |> 
  as.matrix()
colnames(preds) <- c("1","2")

predicted_labels <- apply(preds, 1, which.max)
test_data$predictions <- predicted_labels

# Plotting the data
ggplot(test_data, aes(x, y, color = as.factor(label), shape = as.factor(predictions))) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "Test data", x = "X axis", y = "Y axis") +
  theme_tufte()
```

**TODO:** I have not yet figured out a way to access model activations when the model is trained with luz.


### Architectures

#### Multi Layer Perceptrons

This is a neural net that has a series of fully connected layers seperated by activations. We will illustrate an MLP using the Penguins dataset, where we try to predict the species of a penguin from some features. Example adapted from the excellent Deep Learning with R Torch [book](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/). 

```{r}
library(palmerpenguins)

penguins <- na.omit(penguins)
ds <- tensor_dataset(
  torch_tensor(as.matrix(penguins[, 3:6])),
  torch_tensor(
    as.integer(penguins$species)
  )$to(torch_long())
)

n_class <- penguins$species |> unique() |> length() |> as.numeric()
```

Now, we train a simple MLP on 75% of this dataset. 

```{r}
mlpnet <- nn_module(
  "MLPnet",
  initialize = function(din, dhidden1, dhidden2, dhidden3, n_class) {
    self$net <- nn_sequential(
      nn_linear(din, dhidden1),
      nn_relu(),
      nn_linear(dhidden1, dhidden2),
      nn_relu(),
      nn_linear(dhidden2, dhidden3),
      nn_relu(),
      nn_linear(dhidden3, n_class)
    )
  },
  forward = function(x) {
    self$net(x)
  }
)
```

Now, we train. 

```{r, include=FALSE}
total_size <- length(ds)
train_size <- floor(0.8 * total_size)
valid_size <- total_size - train_size

# Generate indices and shuffle them
set.seed(123)  # For reproducibility
indices <- sample(total_size)

train_indices <- indices[1:train_size]
valid_indices <- indices[(train_size + 1):total_size]

train_dataset <- ds[train_indices]
valid_dataset <- ds[valid_indices]

fitted_mlp <- mlpnet |> 
  setup(loss = nn_cross_entropy_loss(), optimizer = optim_adam) |> 
  set_hparams(din = 4,
              dhidden1 = 2,
              dhidden2 = 2,
              dhidden3 = 2,
              n_class = n_class) |> 
  fit(train_dataset, epochs = 15, valid_data = valid_dataset)
```

Now, let us visualize the validation loss. 

```{r, echo=FALSE}
fitted_mlp$records$metrics$train |> 
  unlist() |> 
  as_tibble()-> train_loss
colnames(train_loss) <- c("training_loss")
fitted_mlp$records$metrics$valid |> 
  unlist() |> 
  as_tibble()-> valid_loss
colnames(valid_loss) <- c("validation_loss")

loss_df <- cbind(train_loss, valid_loss) |> 
  mutate(epoch = seq(1,nrow(train_loss))) |> 
  pivot_longer(cols = c(training_loss, validation_loss),
               names_to = "loss_type",
               values_to = "loss")
ggplot(loss_df, aes(x = epoch, y = loss, colour = loss_type)) +
  geom_line() +
  theme_tufte()
```

