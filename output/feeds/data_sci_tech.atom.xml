<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>p. bhogale - data_sci_tech</title><link href="https://theclarkeorbit.github.io/" rel="alternate"></link><link href="https://theclarkeorbit.github.io/feeds/data_sci_tech.atom.xml" rel="self"></link><id>https://theclarkeorbit.github.io/</id><updated>2024-04-29T12:06:15+02:00</updated><subtitle>Data Sci, Quant Fin, Quant Bio.</subtitle><entry><title>"Becoming a Data Scientist : an opinionated take in 2020"</title><link href="https://theclarkeorbit.github.io/becoming-a-data-scientist-an-opinionated-take-in-2020.html" rel="alternate"></link><published>2020-08-25T00:00:00+02:00</published><updated>2024-04-29T12:06:15+02:00</updated><author><name>pras</name></author><id>tag:theclarkeorbit.github.io,2020-08-25:/becoming-a-data-scientist-an-opinionated-take-in-2020.html</id><summary type="html">&lt;p&gt;There are many articles on this subject, and many of them are excellent. For some examples see &lt;a href="https://towardsdatascience.com/how-to-become-a-data-scientist-2a02ed565336"&gt;here&lt;/a&gt;, &lt;a href="https://towardsdatascience.com/how-to-become-a-data-scientist-3f8d6e75482f"&gt;here&lt;/a&gt;, &lt;a href="https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html"&gt;here&lt;/a&gt; and &lt;a href="https://www.discoverdatascience.org/career-information/data-scientist/"&gt;here&lt;/a&gt;. All such articles are opinionated takes, and articles like these did help when I was starting out in data science some years ago. Contrary to my expectations, the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;There are many articles on this subject, and many of them are excellent. For some examples see &lt;a href="https://towardsdatascience.com/how-to-become-a-data-scientist-2a02ed565336"&gt;here&lt;/a&gt;, &lt;a href="https://towardsdatascience.com/how-to-become-a-data-scientist-3f8d6e75482f"&gt;here&lt;/a&gt;, &lt;a href="https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html"&gt;here&lt;/a&gt; and &lt;a href="https://www.discoverdatascience.org/career-information/data-scientist/"&gt;here&lt;/a&gt;. All such articles are opinionated takes, and articles like these did help when I was starting out in data science some years ago. Contrary to my expectations, the term &lt;strong&gt;Data Scientist&lt;/strong&gt; has become more confusing over the last 3-4 years.&lt;/p&gt;
&lt;p&gt;This is my poor attempt to clarify (to myself) what I  mean when I use this term. This also helps contextualize other terms like "ML engineer" and "Data engineer". &lt;/p&gt;
&lt;h3&gt;A definition, and the basic requirements&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A data scientist uses quantitative techniques to understand, define and solve business problems.&lt;/strong&gt; That's it. In this view, a data scientist is a scientist who brings an analytical mind and mathematical techniques to business problems. &lt;/p&gt;
&lt;p&gt;While that works as a definition, it is rather inadequate because it tells us very little about the skills needed and the type of work a data scientist has to do. The answers to those questions depend partly on the context and culture of the business in which a data scientist works, but some skills are likely to be universally required for any data scientist working in the wilderness of corporate life. IMHO, the following skills are the minimum for any data scientist :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A bouquet of quantitative skills&lt;/strong&gt; : depending on your background, you'll have some quant skills. Monte carlo simulations, probability theory, fourier transforms, etc. The more the better. Apart from those, its good to brush up on statistics, &lt;em&gt;really&lt;/em&gt; understand linear regression and PCA and get a solid grounding in Bayesian thinking. You could do a lot worse than investing 20-24 hrs in &lt;a href="https://www.youtube.com/watch?v=4WVelCswXo4&amp;amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI"&gt;Richard McElreath's course&lt;/a&gt; on statistics, bayesian thinking etc. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQL&lt;/strong&gt; : it is hard to imagine a data scientist who does not - very regularly - need to access databases, and for good or ill, SQL is the lingua franca, along with some dialects. Learning SQL well is also a way to learn the paradigms of data wrangling that come in handy later. There are plenty of resources out there to learn SQL, but I quite liked this &lt;a href="https://www.udacity.com/course/sql-for-data-analysis--ud198#"&gt;free course from Udacity&lt;/a&gt;.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Excel&lt;/strong&gt; : every single business uses Excel, and certainly a lot of your stakeholders will use it, and sooner rather than later, you will have to deal with data in excel sheets. Its ubiquity (despite its multifarious flaws) makes it worth learning. I haven't made myself learn it yet, so I can't point to any good resources.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Programming&lt;/strong&gt; : It does not really matter what you learn to code in, as long as you can code. The aim should be to feel "I don't care what language or package code I need to deal with, I'll pick it up". But if you must pick one language, I'd recommend Python. Even though I personally prefer R. Learn to write clean code, document it, write tests and version control. &lt;a href="https://docs.python-guide.org/intro/learning/"&gt;This&lt;/a&gt; is a great list of free resources.   &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data wrangling and visualization&lt;/strong&gt; : This is another skill no data scientist can do without. There is no better resource to learn how to approach data once you get hold of it than the &lt;a href="https://r4ds.had.co.nz/"&gt;free R4DS book&lt;/a&gt;. Yes it is in R, but that is a language worth picking up anyway.   &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A sprinkling of ML&lt;/strong&gt; : Its good to know the classical ML techniques and where you might use them, an excellent, self contained and concise resource is &lt;a href="http://faculty.marshall.usc.edu/gareth-james/ISL/"&gt;ISLR&lt;/a&gt; also available freely.   &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pursuation and the ability to say NO&lt;/strong&gt; : Every data scientist works with multiple stakeholders (often non technical ones) from whome they need to learn, and whome they need to teach. This is unavoidable, and the better you are at listening carefully and communicating well, the happier you will be.   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With these things in your kitty, you are well set to be the problem solver of last resort (and first preference) for your employer. A Sherlock Holmes for business, if you will. However, depending on your context you will probably need some other skills as well.. &lt;/p&gt;
&lt;h3&gt;Context : the dev teams&lt;/h3&gt;
&lt;p&gt;Increasingly, data scientists are embedded in the engineering departments of their companies, and so (unfortunately) have to jump through such utterly pointless hoops as 5 technical interviews and live coding tests. After you have performed for the engineering manager and convinced 10 developers that you are worth hiring, you will need to be able to interact with their code (this is the easy part) and their organizational and social mores (this is the hard part). Here is what you will need :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The phoenix project&lt;/strong&gt; : A &lt;a href="https://www.amazon.com/Phoenix-Project-DevOps-Helping-Business/dp/0988262592"&gt;book like no other&lt;/a&gt; to understand why things are the way they are in modern software organizations. Added bonus, most devs and product people havent read it so you can quote scripture at them from Day 1.   &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DevOps&lt;/strong&gt; : An understanding of CI/CD pipelines that take your code (the DNA of the thing you are building) and turn it into a running product that does things (the working cell with proteins and interfaces to other cells). LinkedIn has a course called &lt;a href="https://www.linkedin.com/learning/devops-for-data-scientists"&gt;DevOps for Data Scientists&lt;/a&gt;. Havent watched it, but it'll probably be useful.   &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Docker + APIs&lt;/strong&gt; : Continuing the DNA/Cell analogy, think of a Docker container as the cell wall that encloses the environment in which your code runs. That makes no sense ? It will, eventually. This seems like a &lt;a href="https://www.analyticsvidhya.com/blog/2017/11/reproducible-data-science-docker-for-data-science/"&gt;fairly useful introduction&lt;/a&gt;. An API is just the interface to the outside world (the stuff the other devs are building) through the cell wall. Take a look at &lt;a href="https://www.restapitutorial.com/"&gt;this&lt;/a&gt;.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enough deep learning to tell people why you aren't using deep learning&lt;/strong&gt; : this is a cultural issue. Every week, 2-3 devs will ask you why you aren't doing what ever you are doing with deep learning.   &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Context : the marketing/finance departments&lt;/h3&gt;
&lt;p&gt;Many data scientists are concerned with answering business questions and helping design strategy or informing management decisions. In such situations, the data scientist will probably work closely with a data base team and communicate results in meetings quite a bit. What you will need to succeed :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PowerPoint Ninja&lt;/strong&gt; : If you can make impactful slides quickly, you are golden. It is worth investing in learning and mastering this artform, for that is what it is. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DWH skills&lt;/strong&gt; : Its probably worth being able to lend a hand in maintaining and managing the business data warehouse (and learning what that is, in the first place). You will also want to be familiar with such things as cubes, business KPIs, accounting standards and pivot tables.   &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Context : replacing human cognition&lt;/h3&gt;
&lt;p&gt;A lot of companies have realized that one way of utilize data is to make things easier for humans and/or replacing humans for some tasks altogether. These topics have traditionally been the domain of electrical engineering (image and audio processing) and computational linguistics (NLP). If you are expected to work on these fascinating problems, you will need a bouquet of skills that we have not really mentioned before : &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Software engineering&lt;/strong&gt; : Delivering solutions to these problems - for now - require the data scientist to be more aquainted with robust software engineering techniques than some of the other contexts we have mentioned before. It is probably wise to become very good at writing idiomatic python code, just for starters.   &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep learning&lt;/strong&gt; : While these were disparate fields once upon a time, now they are all three (computer vision, NLP, audio processing) sub-fields of "Deep learning" and you will have to master a fairly large amount of material that is common to them all, and quite a lot of (much more interesting) material sepcific to each domain. I unhesitatingly reccommend the &lt;a href="https://www.fast.ai/"&gt;material provided for free by Fast AI&lt;/a&gt;. If you put in the time and effort to work through it, you will be in an excellent position to contribute. &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;What about those other things ?&lt;/h3&gt;
&lt;p&gt;An &lt;strong&gt;ML engineer&lt;/strong&gt; is a software engineer who trains, deploys and maintains a machine learning model in production. This is something a data scientist might do as well, but not necessarily.&lt;br&gt;
A &lt;strong&gt;Data engineer&lt;/strong&gt; is a software and infrastructure engineer who builds and maintains the cloud infrastructure for the company databases &lt;strong&gt;and&lt;/strong&gt; ensures data quality and availability for those in the company who need it, like people who make operational decisions, dashboards, data scientists. This is a hard hard job, and in high demand these days. A data scientist should know some data engineering, if only to appreciate and help the data engineering teams they work with. &lt;/p&gt;
&lt;h3&gt;Take home message&lt;/h3&gt;
&lt;p&gt;While there there is an infinite variety of skills one could learn (I have not even touched upon such useful things as Spark and Airflow) I will return to what (I think) really characterizes a data scientist. &lt;strong&gt;Using quantitative techniques to understand, define and solve business problems.&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;A data scientist must have an analytical mind to bring to the table, and must take the effort to develop a deep understanding of the domain and the business context they operate in. Then, they must do whatever it takes (software engineering, organizational lobbying, operational changes) to create impact from the solutions they have devised. &lt;/p&gt;</content><category term="data_sci_tech"></category></entry><entry><title>"Linear and mixed integer programming"</title><link href="https://theclarkeorbit.github.io/linear-and-mixed-integer-programming.html" rel="alternate"></link><published>2018-09-08T00:00:00+02:00</published><updated>2024-04-27T15:17:44+02:00</updated><author><name>pras</name></author><id>tag:theclarkeorbit.github.io,2018-09-08:/linear-and-mixed-integer-programming.html</id><summary type="html">&lt;p&gt;post inspired by the OptiPy meetup. &lt;a href="https://www.meetup.com/OptiPy-Python-Quants-of-BeNeDeLux/events/253090625/"&gt;link to meetup&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Linear programming&lt;/h2&gt;
&lt;p&gt;Optimisation problems where the constraints and cost function are linear, and the decision variables are continuous, are the simple, canonical domain of linear programming. However, such problems can be solved in polynomial time, which means that to tackle a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;post inspired by the OptiPy meetup. &lt;a href="https://www.meetup.com/OptiPy-Python-Quants-of-BeNeDeLux/events/253090625/"&gt;link to meetup&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Linear programming&lt;/h2&gt;
&lt;p&gt;Optimisation problems where the constraints and cost function are linear, and the decision variables are continuous, are the simple, canonical domain of linear programming. However, such problems can be solved in polynomial time, which means that to tackle a hard (NP hard) problem with this framework, the problem definition needs to be exponentially large (&lt;a href="https://www.cwi.nl/system/files/scimeeting13.pdf"&gt;see this (pdf)&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;For basic details, see the &lt;a href="https://en.wikipedia.org/wiki/Linear_programming"&gt;wiki article on linear programming&lt;/a&gt;. Originally, such problems were made tractable by the &lt;a href="https://en.wikipedia.org/wiki/Simplex_algorithm"&gt;simplex algorithm&lt;/a&gt;. The most common framework in python is &lt;a href="https://pythonhosted.org/PuLP/"&gt;PuLP&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, if we allow for (some) decision variables to be integers instead of reals, a much richer range of problems can be expressed and solved in a similar framework. This is called mixed integer programming.&lt;/p&gt;
&lt;p&gt;First, a toy linear programming problem in PuLP.&lt;/p&gt;
&lt;h3&gt;Simple linear programming&lt;/h3&gt;
&lt;p&gt;Eat the optimal amount of schnitzel and pommes to survive while minimising cost, given nutrition value. Assume that schnitzel and pommes come in continuous quantities, and humans need at least 150 units of carbs and 50 units of proteins to survive (schnitzel has 23 units of carbs and 18 units of proteins, while pommes has 33 units of carbs and 4 units of proteins) while consuming less than 75 units of fat (schnitzel has 15 units of fat and pommes 13). Schnitzel costs 8 per unit, and pommes costs 3.&lt;/p&gt;
&lt;p&gt;This problem can be formulated as follows :&lt;/p&gt;
&lt;p&gt;Decision vars :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how much schnitzel : x1&lt;/li&gt;
&lt;li&gt;how much pommes : x2&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;cost = 8*x1 + 3*x2 # cost function
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;constraints :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; 23*x1 + 33*x2 &amp;gt;= 150 # carbs
 18*x1 + 4*x2 &amp;gt;= 50 # protein
 15*x1 + 13*x2 &amp;lt;= 75 # fats
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;we now code this up in the popular python LP framework, PuLP :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pulp&lt;/span&gt;

&lt;span class="n"&gt;diet_program&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pulp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LpProblem&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Diet Program&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pulp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LpMinimize&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pulp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LpVariable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;schnitzel&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lowBound&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Continuous&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pulp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LpVariable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pommes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lowBound&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Continuous&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;diet_program&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;cost&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;diet_program&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;33&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;150&lt;/span&gt;
&lt;span class="n"&gt;diet_program&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;
&lt;span class="n"&gt;diet_program&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;75&lt;/span&gt;

&lt;span class="n"&gt;diet_program&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;solve&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;diet_program&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;solve&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;pulp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LpStatus&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;diet_program&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;variable&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;diet_program&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variables&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt; = &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;variable&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;variable&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;varValue&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;giving the result&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pommes = 3.0876494
schnitzel = 2.0916335
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Often, real problems have a large number of decision variables and constraints, but LP problems remain tractable (by and large) even in high dimensions.&lt;/p&gt;
&lt;h2&gt;Mixed integer programming&lt;/h2&gt;
&lt;p&gt;Mixed integer models, however, are a different story. Large models are very hard to solve with open source solvers and expensive commercial solvers are needed to solve real world problems in a reasonable amount of time.&lt;/p&gt;
&lt;h3&gt;Material&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;the Sagemath documentation page has a rather &lt;a href="http://doc.sagemath.org/html/en/thematic_tutorials/linear_programming.html"&gt;good chapter&lt;/a&gt; on linear and mixed integer programming. On the face of it, the syntax seems more elegant and extensible than &lt;a href="https://pythonhosted.org/PuLP/"&gt;PuLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;For R, most optimisation problems need matrix definitions and this makes constructing large models in R basically impossible. The &lt;a href="https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/ompr-an-alternative-way-to-model-mixed-integer-linear-programs"&gt;ompr package&lt;/a&gt; seems to solve this issue, and enables construction of models in a step by step fashion, like PuLP and sagemath.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Below, we will work through a couple of relatively simple problems in sagemath, R and PuLP.&lt;/p&gt;
&lt;h3&gt;Tutorial - knapsack problem.&lt;/h3&gt;
&lt;h4&gt;Sagemath version&lt;/h4&gt;
&lt;p&gt;see &lt;a href="http://doc.sagemath.org/html/en/thematic_tutorials/linear_programming.html"&gt;this page&lt;/a&gt; for an intro and sage code.&lt;/p&gt;
&lt;p&gt;We have some objects L each with some weights and some usefulness. We can carry a maximum weight of C, while optimising the total usefulness of the objects we pack.&lt;/p&gt;
&lt;p&gt;Below, we assign random weights and usefulness to our objects-&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;L&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;book&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;knife&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gourd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;flashlight&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extend&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;random_stuff_&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;usefulness&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;set_random_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;685474&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;usefulness&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We now define the mixed integer model.
The objective to be maximised is usefulness of taken objects, the constraint is the maximum weight C and the only decision variables are an array of binary variables corresponding to each objects, determining if they are taken or not.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MixedIntegerLinearProgram&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;taken&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;binary&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_constraint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;taken&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_objective&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;usefulness&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;taken&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Having set up the model, we solve it using the in-built optimizer.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;solve&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# abs tol 1e-6&lt;/span&gt;
&lt;span class="n"&gt;taken&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;taken&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;the total weight of taken objects is&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;taken&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;which gives the expected result.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;the total weight of taken objects is
0.6964959796619171
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4&gt;R version&lt;/h4&gt;
&lt;p&gt;Most optimisers in R (including the popular &lt;a href="https://cran.r-project.org/web/packages/ROI/"&gt;ROI&lt;/a&gt; package that provides a unified interface to multiple solvers) need problem definitions in matrix form. However, since any final problem definition to be optimised is the end result of a long process of experimentation and development, hard to interpret matrices do not provide a convenient language in which to tackle a new problem. Step by step definitions of the optimisation problem (like the one above) are much better in this respect.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://dirkschumacher.github.io/ompr/"&gt;ompr&lt;/a&gt; package provides such an interface for problem definition and solution in R, using the pipe operator from the &lt;a href="https://www.tidyverse.org/"&gt;Tidyverse&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Setting up the basics&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tidyverse&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ROI&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ROI.plugin.glpk&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ompr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ompr.roi&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# max weight&lt;/span&gt;
&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;torch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;food&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;tent&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;knife&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;books&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# objects&lt;/span&gt;
&lt;span class="nf"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;paste&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;random_stuff&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nf"&gt;set.seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;685474&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;usefulness&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;runif&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;runif&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Defining the model&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model_mip&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;MIPModel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nf"&gt;add_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;binary&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nf"&gt;set_objective&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;sum_expr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;usefulness&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;max&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nf"&gt;add_constraint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;sum_expr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Solving the model with the &lt;code&gt;glpk&lt;/code&gt;  solver.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;solution&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;model_mip&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nf"&gt;solve_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;with_ROI&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;solver&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;glpk&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nf"&gt;get_solution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nf"&gt;arrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And then checking what the weight of our knapsack is !&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;solution_data&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;data_frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;objects&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;unlist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;usefulness&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;usefulness&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;taken&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;solution&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;objects_taken&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;solution_data&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;taken&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;objects_taken&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and it is&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[1] 0.9203911

objects usefulness  weights       taken
&amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
torch   0.7605667   0.44670019     0
food    0.9440156   0.53843038     0
tent    0.3925896   0.02805875     1
knife   0.7986606   0.88793490     0
books   0.8166644   0.40233223     0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;an overview of optimization in R is provided &lt;a href="https://www.is.uni-freiburg.de/resources/computational-economics/5_OptimizationR.pdf"&gt;here (pdf)&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;PuLP version&lt;/h4&gt;
&lt;p&gt;The pulp problem is setup in a very similar fashion to the sage problem.  Below is the entire code for the problem solution in PuLP.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pulp&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="c1"&gt;# the max weight&lt;/span&gt;
&lt;span class="n"&gt;L&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;torch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;food&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tent&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;knife&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;books&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extend&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;random_stuff&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;usefulness&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;685474&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;usefulness&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#the decision variables&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pulp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LpVariable&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dicts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lowBound&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;upBound&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pulp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LpInteger&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# declaring the PuLP model&lt;/span&gt;
&lt;span class="n"&gt;knapsack_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pulp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LpProblem&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;knapsack&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pulp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LpMaximize&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# cost function :&lt;/span&gt;
&lt;span class="n"&gt;knapsack_model&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;usefulness&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;thing&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;thing&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;thing&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;usefulness&amp;quot;&lt;/span&gt;
&lt;span class="c1"&gt;# constraints&lt;/span&gt;
&lt;span class="n"&gt;knapsack_model&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;thing&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;thing&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;thing&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;

&lt;span class="c1"&gt;# solving the model&lt;/span&gt;
&lt;span class="n"&gt;knapsack_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;solve&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;pulp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LpStatus&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;knapsack_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# displaying the solution in a useful form&lt;/span&gt;
&lt;span class="n"&gt;total_weight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
&lt;span class="n"&gt;things&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;variable&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;knapsack_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variables&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;variable&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="n"&gt;things&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;variable&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;varValue&lt;/span&gt;
    &lt;span class="n"&gt;total_weight&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;things&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="c1"&gt;# print(&amp;quot;{} = {}&amp;quot;.format(var, variable.varValue))&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;the total weight taken is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_weight&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;solution_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;usefulness&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;things&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;
&lt;span class="n"&gt;solution_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;weight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;usefulness&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;taken&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;solution_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;with the output&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;the total weight taken is 0.911966521327
                 weight  usefulness  taken
books          0.977869    0.290585    0.0
food           0.112391    0.445158    1.0
knife          0.735062    0.919826    0.0
random_stuff0  0.881662    0.800397    0.0
random_stuff1  0.888736    0.636453    0.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With these basics out of the way, we can tackle a non-trivial model inspired by real data in the next post.&lt;/p&gt;</content><category term="data_sci_tech"></category></entry><entry><title>"The Invisible Hand"</title><link href="https://theclarkeorbit.github.io/the-invisible-hand.html" rel="alternate"></link><published>2018-03-18T00:00:00+01:00</published><updated>2024-04-27T15:17:44+02:00</updated><author><name>pras</name></author><id>tag:theclarkeorbit.github.io,2018-03-18:/the-invisible-hand.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Markets and financial time series data.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Basic ideas about markets&lt;/h2&gt;
&lt;p&gt;Inevitably, finance and markets will be a recurring subject of discussion on this blog. They are ubiquitous, useful, fascinatingly complex at various levels and they generate a lot of readily accessible data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is a market ?&lt;/strong&gt; A market is a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Markets and financial time series data.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Basic ideas about markets&lt;/h2&gt;
&lt;p&gt;Inevitably, finance and markets will be a recurring subject of discussion on this blog. They are ubiquitous, useful, fascinatingly complex at various levels and they generate a lot of readily accessible data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is a market ?&lt;/strong&gt; A market is a system which enables &lt;em&gt;traders&lt;/em&gt; to exchange &lt;em&gt;commodities/assets&lt;/em&gt; at a mutually agreeable &lt;em&gt;price&lt;/em&gt;. Every market has at its center a &lt;em&gt;market maker&lt;/em&gt; - an entity which matches traders who want to buy at a certain price with traders who want to sell at that price.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;No Arbitrage.&lt;/strong&gt; The word arbitrage refers to a situation where the same asset has different prices in different locations. For instance, if Klingon Bat'Leths are available at 10USD a piece in Berlin and at 15USD a piece in Baghdad, and if the cost of transporting a Bat'Leth from Berlin to Baghdad is 1USD, then money can be made by buying Bat'Leths in Berlin and selling them in Baghdad. In fact, people will buy Bat'Leths in Berlin and sell them in Baghdad until the price of Bat'Leths increases in Berlin and decreases in Baghdad and there is no more profit to be made. In an ideal market, arbitrage is instantaneously washed out by traders making money off it. The &lt;em&gt;no arbitrage condition&lt;/em&gt; implies that the market serves as a mechanism for price setting. Each commodity has a "rational" price decided in the market by balancing the forces of demand and supply.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The efficient market.&lt;/strong&gt; A market is said to be efficient if all the information about a particular asset is instantly assimilated by the market and is immediately reflected in the price of the asset. This assumption has significant implications for the time series of the price of an asset traded on the market. Since Bachelier in 1900, it has been argued that an efficient market should imply that prices move randomly but a formal proof was presented by &lt;a href="https://www.ifa.com/media/images/pdf%20files/samuelson-proof.pdf"&gt;Samuelson in 1965 (pdf)&lt;/a&gt;. The paper is readable to anyone with moderate exposure to probability theory and the principle result is that at the present time $t$, given the historical time series of prices of a particular asset ${y_t,y_{t-1}.......y_0}$, if the futures price of that asset to be delivered at time $T&amp;gt;t$ is $C(T,t)$, then the expected price at the next time point $t+1$ is given by $E\left{C(T,t+1)\right}=C(T,t)$. In other words, it is impossible to predict which way the price will move at the next time point based on the historical price data.&lt;/p&gt;
&lt;p&gt;Before making this idea more intuitive, we will introduce market &lt;em&gt;returns&lt;/em&gt;. If the price of an asset today is $y_t$ and tomorrow it is $y_{t+1}$, then the return an investor might have obtained by buying today and selling tomorrow is defined to be $\frac{y_{t+1}-y_t}{y_t}$. It is more common in practice to use the logarithmic return defined by $ln\left(\frac{y_{t+1}}{y_t}\right)$. It is useful to think of logarithmic returns being related to compound interest and normal returns being related to simple interest. If the price today was 100, and tomorrow is 110, then my return is 10% while my logarithmic return is 9.5%. The two methods of calculating return give approximately the same result but the logarithmic return is smaller since a lower rate of return is needed to obtain the same final capital with compound interest.&lt;/p&gt;
&lt;p&gt;We can now restate the efficient market hypothesis as a statement about returns. In an efficient market, returns must be serially uncorrelated. Stated this way, it is much easier to see the link between the efficient market hypothesis and the randomness of prices. If, for a certain asset, it were possible to predict that the price would rise (positive return) or fall (negative return) based on the past, this would present a powerful arbitrage opportunity. If prices were predicted to rise in the future, intelligent investors (intelligent enough to see correlations in returns anyway) could make a lot of money by buying today and selling when the price rose. However, this buying activity would immediately cause the price (at which the asset can be bought) to rise, washing out the gains that the investors might have made.&lt;/p&gt;
&lt;p&gt;It is worth stating that while the degree of randomness of returns on the price of a traded asset &lt;a href="https://www.sciencedirect.com/science/article/pii/037842669390087T"&gt;can be tested&lt;/a&gt;, tests for the efficient market hypothesis suffer from the so-called &lt;a href="http://finance.wharton.upenn.edu/~jwachter/fnce100/h11.pdf"&gt;joint hypothesis problem&lt;/a&gt;. How would we know if a market is inefficient ? We might look at all the information available, and find that the market behaves "abnormally" or "irrationally", given the available information. However, to evaluate what a normal/abnormal return is, we need a model that connects available information to the price of an asset (&lt;a href="https://hbr.org/1982/01/does-the-capital-asset-pricing-model-work"&gt;an asset pricing model&lt;/a&gt;). And therein lies the issue : even if we observe "abnormal returns", is the market inefficient or is our asset pricing model wrong ? We cannot possibly know. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Who does an "informed investor" buy from ?&lt;/strong&gt; There is a word for them. Noise Investors. They provide the liquidity in the market, buying and selling assets regardless of price, perhaps acting on information that is really noise, or driven by other factors like an urgent need for cash resulting in a sale regardless of price. Since demand from noise investors is - by definition - random and independent of the price of the asset, the random uncorrelated fluctuations in asset prices are caused by the actions of informed investors seeking maximum profit. Anyone familiar with information theory will immediately recognize what is going on here. Since the time series of the price or returns on an asset incorporates non redundant information at each time point, it looks like a completely random, uncorrelated sequence. A sequence that has very little information can be compressed and expressed as a concise computer program (see &lt;a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity"&gt;algorithmic complexity&lt;/a&gt;) or compressed in other ways using correlations. The higher the information content the more random a sequence looks. From this point of view, it is clear that a sequence incorporating a lot of information is indistinguishable from a completely random sequence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If the price can be predicted to go up based on history, it would have already gone up to a point where no more profit is expected from a price rise.&lt;/strong&gt; Once one has assumed that the market is efficient, this conclusion seems inescapable. As the old joke goes, two economists are walking along a road and one of them spots a 100\$ bill on the street. The other economist tells him not to bother, since if there really were a 100\$ bill lying about, it would already have been picked up !&lt;/p&gt;
&lt;p&gt;While real markets resemble ideal, efficient markets in many ways (correlations between returns are washed out in less than 2 minutes, arbitrage is hard to find and so on) markets are only efficient in proportion to the number of intelligent investors looking to profit from their inefficiencies. There is a clear tension here. The effort investors are prepared to make to sniff out inefficiencies is proportional to the degree of inefficiency that exists. So, every profit opportunity is washed out only if one is not participating in the washing out.&lt;/p&gt;
&lt;h2&gt;A peek at financial data&lt;/h2&gt;
&lt;p&gt;We will use the &lt;code&gt;Quandl&lt;/code&gt; package (see the &lt;a href="https://www.quandl.com/tools/full-list"&gt;website&lt;/a&gt; for details) to download recent oil prices and analyze them a little bit. This will serve as a short introduction to uni-variate time series analysis in R. See &lt;a href="https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm"&gt;this&lt;/a&gt; useful resource from the NIST for a simple overview of the theory. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Quandl&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;OPEC/ORB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;raw&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                     &lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;normalize&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                     &lt;/span&gt;&lt;span class="n"&gt;collapse&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;daily&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                     &lt;/span&gt;&lt;span class="n"&gt;force_irregular&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                     &lt;/span&gt;&lt;span class="n"&gt;start_date&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2001-01-01&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;as_data_frame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;transmute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;price_of_oil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;arrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;transform = "normalize"&lt;/code&gt; option sets the first value in the time series to 100 and scales all the other values accordingly. Let us take a look at oil prices over the last 18 years, scaled to the price on the 1st of January 2001.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_line&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;price_of_oil&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_vline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xintercept&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2008-07-11&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#7379d6&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;geom&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;text&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;           &lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Financial crash of 2008&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;           &lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2006-6-11&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;           &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#7379d6&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_vline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xintercept&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2014-10-11&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#7379d6&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;geom&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;text&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Overproduction&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;           &lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2016-4-11&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;480&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;           &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#7379d6&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;ggthemes&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;theme_economist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;ggtitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Normalized oil prices since 2001-01-01&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="center" src="/figures/The_Invisible_Hand/unnamed-chunk-3-1.png"&gt;&lt;/p&gt;
&lt;p&gt;In this data, there are two major crashes the first corresponding to the financial crisis of 2008 (&lt;a href="http://www.nytimes.com/2008/11/12/business/worldbusiness/12oil.html"&gt;NYT comment on oil prices around this time&lt;/a&gt;) which led to lower demand, while the oil price crash of 2014-15 seems to be linked to over production as oil producers competed for market share despite production ramp-ups in North America with &lt;a href="https://www.forbes.com/sites/uhenergy/2017/09/05/how-american-fracking-ran-opecs-oil-recovery-off-the-rails/#11ee9db1ec26"&gt;fracking in the USA&lt;/a&gt; and &lt;a href="https://oilprice.com/Energy/Energy-General/What-Does-The-Future-Hold-For-Canadas-Oil-Sands.html"&gt;oil sands in Canada&lt;/a&gt;. &lt;/p&gt;
&lt;h3&gt;Correlations in time&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Is this a random walk ?&lt;/strong&gt; &lt;a href="https://www.chicagobooth.edu/~/media/34F68FFD9CC04EF1A76901F6C61C0A76.PDF"&gt;The random walk hypothesis&lt;/a&gt; follows intuitively from the efficient market hypothesis. If today's price includes &lt;em&gt;all available information&lt;/em&gt; then it is the best available estimate of tomorrow's price, i.e., the price could go either way tomorrow, and successive returns are un-correlated. However, We see from the oil price chart above that there are long periods of positive and negative returns. At least at some times, over short-ish time scales, returns do seem to be correlated. &lt;/p&gt;
&lt;p&gt;Another useful concept about any time series ${y_t}$ is &lt;strong&gt;stationarity&lt;/strong&gt;. A time series is said to be stationary if it's mean function $\mu_t = E[y_t]$ and it's autocovariance function $\gamma(t,t-k) = E[(y_t-\mu_t)(y_{t-k}-\mu_{t-k})]$ are both independent of time. In other words, a series is stationary if, over time, all its values are distributed around the same mean, and its relationship with its past does not evolve over time. A strongly stationary process has a joint probability distribution which does not change when shifted in time, i.e. ALL moments of the distribution are time independent. &lt;/p&gt;
&lt;p&gt;In practice, most financial time series are not stationary, however, stationary series can often be derived from non stationary series. For instance, the differences, or returns on a time series could be stationary even if the series itself is not. Or, the series could be fit to a function that approximates its mean over time, and subtracting this fitted mean from the original series yields a stationary series. As we shall see in subsequent sections, the simplest models often assume that a series is stationary.&lt;/p&gt;
&lt;p&gt;We can measure the influence of the past on the present value of a time series via its autocorrelation function. The &lt;a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda35c.htm"&gt;autocorrelation&lt;/a&gt; of a signal is the correlation of a signal with a delayed copy of itself. The autocorrelation function (ACF) calculates the correlations with different lags, giving us some idea about how long it takes for information contained in today's price to be swamped by new information in the signal. &lt;/p&gt;
&lt;p&gt;The autocorrelation is just the normalized autocovariance function. Given observations $y_t$ for $t\in {1..N}$, the autocorrelation for lag $k$ is given by,
$$\rho_y(k) = \frac{\gamma(t,t-k)}{\gamma(t,t)} = \frac{\sum_{t=1+k}^N (y_{t-k}-\mu_{t-k})(y_t-\mu_t)}{\sum_{t=1}^N (y_t-\mu_t)^2}$$
and stationarity would imply $\mu_{t-k}=\mu_t \text{  }\forall (t,k)$. The ACF computes this number for various values of $k$. In practice, we use the (slightly more complicated) &lt;a href="https://en.wikipedia.org/wiki/Partial_autocorrelation_function"&gt;partial autocorrelation function&lt;/a&gt; that computes the correlation of a time series with a lagged version of itself like the ACF, but also controls for the influence of all shorter lags. In other words, for a lag of say, 2 days, it computes how much the price day before yesterday is correlated with the price today (over the whole time series) over and above the correlation induced by the price yesterday (which is correlated to today's as well as day before yesterday's price). This gives a "decoupled" version of the influence of various time points in the past on the present.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;oil_price_returns&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;data_frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nf"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                &lt;/span&gt;&lt;span class="n"&gt;returns&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;diff&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;price_of_oil&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                &lt;/span&gt;&lt;span class="n"&gt;log_returns&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;diff&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;price_of_oil&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_price_returns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_line&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;log_returns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;logarithmic returns&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;ggtitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Time series of logarithmic returns&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;ggthemes&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;theme_economist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_vline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xintercept&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2008-07-11&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;             &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#7379d6&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;geom&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;text&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Financial crash of 2008&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;           &lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2006-6-11&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0.15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;           &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#7379d6&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_vline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xintercept&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2014-10-11&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#7379d6&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;geom&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;text&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Overproduction&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;           &lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2016-4-11&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0.15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;           &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#7379d6&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="center" src="/figures/The_Invisible_Hand/unnamed-chunk-4-1.png"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;temp_acf_log_returns&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;pacf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_price_returns&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;log_returns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                             &lt;/span&gt;&lt;span class="n"&gt;lag.max&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;FALSE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;acf_df&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;data_frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log_returns_acf&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;temp_acf_log_returns&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;acf&lt;/span&gt;&lt;span class="p"&gt;[,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                     &lt;/span&gt;&lt;span class="n"&gt;lag&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;temp_acf_log_returns&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;lag&lt;/span&gt;&lt;span class="p"&gt;[,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;acf_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_point&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lag&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;log_returns_acf&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_segment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lag&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;log_returns_acf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                   &lt;/span&gt;&lt;span class="n"&gt;xend&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lag&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;yend&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_hline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yintercept&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;ggtitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Partial autocorrelation function for the logarithmic returns&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;ggthemes&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;theme_economist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="center" src="/figures/The_Invisible_Hand/unnamed-chunk-4-2.png"&gt;
In general then, the price of oil today is correlated with the price of oil yesterday, but, it would seem, has basically nothing to do with the price of oil the day before. &lt;/p&gt;
&lt;p&gt;While this is true of the whole time series, we could also compute this for windows of 365 days each (short windows lead to noisy estimates of the ACF coefficients), to see if there are periods of high long-range (multiple day) correlations. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;acf_noplot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nf"&gt;return&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;pacf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lag.max&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;FALSE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;window_width&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;365&lt;/span&gt;
&lt;span class="n"&gt;windowed_acf&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;rollapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_price_returns&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;log_returns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                          &lt;/span&gt;&lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;window_width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                          &lt;/span&gt;&lt;span class="n"&gt;FUN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;acf_noplot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;                          &lt;/span&gt;&lt;span class="n"&gt;align&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;left&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;unlist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;windowed_acf_df&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;windowed_acf&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;windowed_acf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;t&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;as_data_frame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;slice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_price_returns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;window_width&lt;/span&gt;&lt;span class="m"&gt;+1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;mutate_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;as.numeric&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;oil_price_returns&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;window_width&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nf"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_price_returns&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;

&lt;span class="n"&gt;acf_values&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;windowed_acf_df&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;summarise_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;gather&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;acf_values&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;## # A tibble: 6 x 2
##   key      value
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 V1     0.245  
## 2 V2    -0.0536 
## 3 V3     0.0213 
## 4 V4     0.00316
## 5 V5     0.0104 
## 6 V6    -0.0128
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can see that while evaluating ACFs on smaller samples via a moving window and taking the mean is not &lt;em&gt;quite&lt;/em&gt; the same as taking the ACF on the whole series, the pattern is not different, i.e., the correlation is washed out after the second day.&lt;/p&gt;
&lt;p&gt;Now, we can plot the 2nd, 3rd and 4th terms of the ACF function to see if there are periods of higher and lower correlations in the oil prices.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;windowed_acf_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_line&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;V1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Lag 1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_line&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;V2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Lag 2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_line&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;V3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Lag 3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_vline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xintercept&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2008-07-11&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;             &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#7379d6&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;geom&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;text&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Financial crash of 2008&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;           &lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2006-6-11&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;           &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#7379d6&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_vline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xintercept&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2014-10-11&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;             &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#7379d6&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;geom&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;text&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Overproduction&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;           &lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2016-4-11&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;           &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#7379d6&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_line&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;window_width&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="m"&gt;-1&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;price_of_oil&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;window_width&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="m"&gt;-1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;rescaled oil price&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0.55&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;ggthemes&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;theme_economist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;ggtitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Evolution of correlations with different lags&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;ylab&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;correlation&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="center" src="/figures/The_Invisible_Hand/unnamed-chunk-6-1.png"&gt;
It is clear by inspection that both crashes correspond to increasing correlation (of log-returns) across all three lag terms plotted. That is, while the oil price was crashing, autocorrelations (of log-returns) with lags of 1, 2, 3 days were all increasing. autocorrelations peaked when the oil price reached rock bottom and relaxed again as the price recovery started. &lt;/p&gt;
&lt;p&gt;The time series of log-returns on oil prices is clearly not stationary (and nor are oil prices themselves, needless to say). So, what is a good way to forecast oil prices ?&lt;/p&gt;
&lt;h3&gt;$AR(p)$, $ARMA(p,q)$, $ARIMA(p,d,q)$, $ARCH(q)$, $GARCH(p,q)$ ...&lt;/h3&gt;
&lt;p&gt;One possible simple model of a time series like ours is an &lt;a href="https://en.wikipedia.org/wiki/Autoregressive_model"&gt;autoregressive process&lt;/a&gt; of order $p$. This just means that the current value of the time series depends on the value of the time series at $p$ previous time steps and a noise term. An $AR(p)$ process (this is what they are called..) take the form, 
$$x_{t} = c + \sum_{i = 1}^p \phi_i x_{t-i\Delta t} + \epsilon_t$$
where $\epsilon_t$ is the uncorrelated, unbiased noise term. For oil price returns, the coefficients $\phi_i$ will probably not be significant (overall) for $i&amp;gt;1$. However, we have already seen that the influence of the past changes with time, and there are periods when multiple day correlations might be vital to explaining the change in price. $AR(p)$ processes need not always be stationary. &lt;a href="https://en.wikipedia.org/wiki/Moving-average_model"&gt;Moving average models&lt;/a&gt; $MA(q)$ on the other hand are always stationary and posit that the present value $y_t$ is the sum of some mean value, a white noise term, and a sum over $q$ past values of noise terms (the moving average referred to in the name).
$$y_t = \mu + \epsilon_t + \sum_{i=1}^q \epsilon_{t-i\Delta t}.$$
It does not take a genius to infer that &lt;a href="https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model"&gt;autoregressive moving average&lt;/a&gt; $ARMA(p,q)$ models consist of $p$ auto regressive and $q$ moving average terms. They are weakly stationary (the first two moments are time invariant). &lt;/p&gt;
&lt;p&gt;To be able to forecast non-stationary processes, $ARMA(p,q)$ models have been generalized to &lt;a href="https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average"&gt;autoregressive integrated moving average&lt;/a&gt; $ARIMA(p,d,q)$ models. Apart from the $p$ lagged values of itself and the sum over $q$ noise terms from the past the $ARIMA(p,d,q)$ also have the time series values differenced $d$ times. This differencing is the discrete version of a derivative, so 1st order differencing is $y_t' = y_t - y_{t-1}$ while second order differencing is $y_t'' = y'&lt;em t-1&gt;t - y'&lt;/em&gt;)$ and so on. } = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2&lt;/p&gt;
&lt;p&gt;So far, we have seen schemes add successive levels of complexity to model the value of a time series, but none of these attempt to model the changes over time of the noise terms. So far, these schemes have assumed the parameters of the noise term to be constants. The &lt;a href="https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity"&gt;autoregressive conditional heteroskedasticity&lt;/a&gt; $ARCH(q)$ and it's cousin the generalized autoregressive conditional heteroskedasticity $GARCH(p,q)$ do model the evolution of the noise term over time. In particular, $ARCH(q)$ models assume an autoregression of order $q$, an $AR(q)$ model for the variance of the noise term, while $GARCH(p,q)$ models assume an $ARIMA(p,q)$ model for the variance of the noise term. Thus, one might use a $ARIMA(p,d,q)$ process to model the price of oil, and a $GARCH(r,s)$ process to model its &lt;a href="https://www.reuters.com/article/us-usa-stocks-weekahead/stock-volatility-back-with-a-bang-and-here-to-stay-idUSKCN1G02AP"&gt;volatility&lt;/a&gt; (the variance of the noise term !). 
Now, we will attempt to forecast oil prices using a $ARIMA(2,2,2)$ process. We will fit the process to data until 2018-01-01, and calculate the RMS error on log-returns data post 2015-01-01. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;test_date&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2017-05-01&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;price_of_oil&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_date&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;price_of_oil&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_date&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="n"&gt;arima_fit&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;arima&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;order&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;transform.pars&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;                   &lt;/span&gt;&lt;span class="n"&gt;seasonal&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nf"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;order&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;period&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;simulated_prices&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arima_fit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nf"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;test_df&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;data_frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                      &lt;/span&gt;&lt;span class="n"&gt;price_of_oil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;oil_prices&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;price_of_oil&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                      &lt;/span&gt;&lt;span class="n"&gt;arima_prediction&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;simulated_prices&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                      &lt;/span&gt;&lt;span class="n"&gt;arima_error&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;length.out&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
&lt;span class="w"&gt;                                      &lt;/span&gt;&lt;span class="n"&gt;simulated_prices&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;se&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_line&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;arima_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;arima pred&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_errorbar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                    &lt;/span&gt;&lt;span class="n"&gt;ymin&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;arima_prediction&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arima_error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                    &lt;/span&gt;&lt;span class="n"&gt;ymax&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;arima_prediction&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;arima_error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                    &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;arima pred&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0.07&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_line&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;price_of_oil&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;price of oil&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;ggthemes&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;theme_economist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2015-08-01&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2018-03-25&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;350&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;ggtitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Arima predictions&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="center" src="/figures/The_Invisible_Hand/unnamed-chunk-7-1.png"&gt;&lt;/p&gt;
&lt;p&gt;Clearly, not a great forecast even during a period without extreme price movements. We will round off our little discussion of univariate financial time series with a small section on how returns are distributed.&lt;/p&gt;
&lt;h3&gt;Distribution of returns&lt;/h3&gt;
&lt;p&gt;With all the talk around random walks on wall street, and with Gaussian distributions being analytically tractable, people - including experts - have come to rely on too many distributions in finance being Gaussian, and they are not. There is a rather good reason for random walks leading to Gaussian distributions : the &lt;a href="https://www.khanacademy.org/math/ap-statistics/sampling-distribution-ap/sampling-distribution-mean/v/central-limit-theorem"&gt;central limit theorem&lt;/a&gt;. The basic idea is, for a large class of probability distributions (i.e. those whose variances are finite) if one adds a large number of independent random variables (eg. steps in a random walk... the position after a large number of steps is the sum of each step) one gets a number that has a Gaussian distribution.&lt;/p&gt;
&lt;p&gt;However, these conditions are not always fulfilled. We have already seen that each step (the returns) in the random walk (of the price) is not always independent of the others (see the autocorrelations in the returns discussed above), and even worse, the returns may or may not have a distribution that is nice and has a finite variance. &lt;/p&gt;
&lt;p&gt;Let us take a look at the distribution of scaled logarithmic returns of oil prices as compared to the normal (Gaussian) distribution via &lt;a href="http://data.library.virginia.edu/understanding-q-q-plots/"&gt;quantile-quantile plots&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_price_returns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nf"&gt;geom_qq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log_returns&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;log-returns&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;stats&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;qnorm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nf"&gt;ggtitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Quantile-quantile plot of log-returns against normal distribution&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;ggthemes&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;theme_economist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="center" src="/figures/The_Invisible_Hand/unnamed-chunk-8-1.png"&gt;
Clearly, both returns and logarithmic returns take large positive and negative values far more frequently than they would if they indeed followed a Gaussian distribution. This tells us that the distributions of returns (and log-returns) of oil prices have &lt;a href="http://nassimtaleb.org/tag/fat-tails/"&gt;fatter tails&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;We can try to fit these to a distribution with a fatter tail, like the Cauchy distribution, and plot the densities on a semi-log plot so that we see the tails better.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;cauchy_fit&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;fitdistr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_price_returns&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;returns&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;densfun&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;cauchy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;glue&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Location parameter is {cauchy_fit$estimate[1]} and the scale parameter is {cauchy_fit$estimate[2]}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;## Location parameter is 0.0347862087434961 and the scale parameter is 0.497758531292875
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_price_returns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_point&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_price_returns&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;log_returns&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                 &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;log returns&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;..density..&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;stat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;bin&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;stat_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fun&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;dcauchy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1e2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cauchy_fit&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;estimate&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="w"&gt;                                                    &lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cauchy_fit&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;estimate&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;cauchy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;stat_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fun&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;dnorm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1e2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_price_returns&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;log_returns&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
&lt;span class="w"&gt;                            &lt;/span&gt;&lt;span class="n"&gt;sd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;sd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oil_price_returns&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;log_returns&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;gaussian&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;-4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;scale_y_log10&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;ggtitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Distribution of logarithmic returns&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;ggthemes&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;theme_economist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;xlab&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Scaled logarithmic returns&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="center" src="/figures/The_Invisible_Hand/unnamed-chunk-9-1.png"&gt;
We see that the distribution of logarithmic returns has fatter tails than the Gaussian, but is not quite as fat tailed as the Cauchy distribution.&lt;/p&gt;
&lt;p&gt;The central limit theorem is only one of a class of limit theorems, and the Gaussian is only one attractor of an infinite set of attractors in the space of probability distributions. When assumptions about independence and existence of second moments that lead to the CLT fail, we should examine other limit distributions that may lead to behavior that is qualitatively different from that of a pleasant Gaussian random walk. But, that is a story for a later blog post :)&lt;/p&gt;</content><category term="data_sci_tech"></category></entry><entry><title>"greta playground"</title><link href="https://theclarkeorbit.github.io/greta-playground.html" rel="alternate"></link><published>2018-03-11T00:00:00+01:00</published><updated>2024-04-27T15:17:44+02:00</updated><author><name>pras</name></author><id>tag:theclarkeorbit.github.io,2018-03-11:/greta-playground.html</id><summary type="html">&lt;p&gt;A first foray into probabilistic programming with Greta&lt;/p&gt;
&lt;h2&gt;Models and modelling&lt;/h2&gt;
&lt;p&gt;Much of science - physical and social - is devoted to positing mechanisms that explain how the data we observe are generated. In a classic example, after &lt;a href="https://en.wikipedia.org/wiki/Tycho_Brahe"&gt;Tycho Brahe&lt;/a&gt; made detailed observations of planetary motion (&lt;a href="http://www.pafko.com/tycho/observe.html"&gt;here&lt;/a&gt; is data on mars), &lt;a href="https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion"&gt;Johannes …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;A first foray into probabilistic programming with Greta&lt;/p&gt;
&lt;h2&gt;Models and modelling&lt;/h2&gt;
&lt;p&gt;Much of science - physical and social - is devoted to positing mechanisms that explain how the data we observe are generated. In a classic example, after &lt;a href="https://en.wikipedia.org/wiki/Tycho_Brahe"&gt;Tycho Brahe&lt;/a&gt; made detailed observations of planetary motion (&lt;a href="http://www.pafko.com/tycho/observe.html"&gt;here&lt;/a&gt; is data on mars), &lt;a href="https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion"&gt;Johannes Kepler posited laws&lt;/a&gt; of planetary motion that &lt;em&gt;explained&lt;/em&gt; how this data were generated. Effectively, &lt;strong&gt;modelling&lt;/strong&gt; is the art of constructing data generators that help us understand and predict. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistical models&lt;/strong&gt; are one class of models that aim to construct - given some observed data - the probability distribution from which the data were drawn. That is, given a sample of data, a statistical model is a hypothesis about how this data were generated. In practice, this happens in two steps :&lt;br&gt;
- constructing a hypothesis, or a model $H$ parametrized by some parameters $\theta$,&lt;br&gt;
- finding (&lt;em&gt;inferring&lt;/em&gt;) the distribution of parameters $\theta$ or, the most suitable parameters $\theta$ given the observed data&lt;/p&gt;
&lt;p&gt;What parameters are "most suitable" is indicated (in a particular sense of the word "suitable" will become clear in the following discussion) by the &lt;a href="https://en.wikipedia.org/wiki/Likelihood_function"&gt;likelihood function&lt;/a&gt; that quantifies how probable the observed data set is, for a given hypothesis parametrized by some particular parameters $H_{\theta}$. Understandably, we want to find parameters such that the observed data is the most likely, this is called &lt;a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation"&gt;maximum likelihood estimation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since all but the simplest models are analytically intractable (i.e., the maximum of the likelihood function needs to be evaluated numerically and parameter distributions are even harder to compute) it makes sense to construct general rules and syntax to easily define statistical models and quickly infer their parameters. This is the field of probabilistic programming. &lt;/p&gt;
&lt;h2&gt;Probabilistic programming&lt;/h2&gt;
&lt;p&gt;The probabilistic programming language (PPL) has two tasks :  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;be able to construct a useful class of statistical models  &lt;/li&gt;
&lt;li&gt;be able to infer the parameters (and their distributions) of this class of models given some observed data.  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As has been explained in this &lt;a href="https://www.reddit.com/r/deeplearning/comments/846wb6/the_paper_that_introduces_the_edward_ppl_by/"&gt;excellent paper introducing the PPL Edward&lt;/a&gt; that is based on Python and &lt;a href="https://www.tensorflow.org/"&gt;Tensorflow&lt;/a&gt;, some PPLs restrict the class of models they allow in order to optimize the inference algorithm, while other emphasize expressiveness and sacrifice performance of the inference algorithms. Modern PPLs like &lt;a href="http://edwardlib.org/"&gt;Edward&lt;/a&gt;, &lt;a href="https://eng.uber.com/pyro/"&gt;Pyro&lt;/a&gt;, and the R based &lt;a href="https://greta-dev.github.io/greta/index.html"&gt;Greta&lt;/a&gt; use the robust infrastructure (hardware and software) that was first developed in the context of deep learning and thus ensure scalability and performance while being expressive. &lt;/p&gt;
&lt;h3&gt;The tensor and the computational graph&lt;/h3&gt;
&lt;p&gt;The fundamental data structure of this group of languages is the &lt;a href="https://en.wikipedia.org/wiki/Tensor"&gt;tensor&lt;/a&gt; which is just a multidimensional array. Data, model parameters, samples from distributions are all stored in tensors. All the manipulations that go into the construction of the output tensor constitute the computational graph (see &lt;a href="http://colah.github.io/posts/2015-08-Backprop/"&gt;this&lt;/a&gt; for an exceptionally clear exposition of the concept) associated with that tensor.  &lt;/p&gt;
&lt;p&gt;Data and parameter tensors are inputs to the computational graph. In the context of deep learning, "training" consists of the following steps :  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Randomly initializing the parameter tensors  &lt;/li&gt;
&lt;li&gt;Computing the output  &lt;/li&gt;
&lt;li&gt;Measuring the error compared to the real/desired output  &lt;/li&gt;
&lt;li&gt;Tweaking the parameter tensors to reduce the error.  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The algorithm that does this is called &lt;a href="https://en.wikipedia.org/wiki/Backpropagation"&gt;back propagation&lt;/a&gt;.
Thus, the objective in deep learning or machine learning is to obtain the &lt;strong&gt;best values&lt;/strong&gt; (in the sense of that they minimize error on the training set) of the parameters given some data.&lt;/p&gt;
&lt;p&gt;The objective of probabilistic modelling is subtly different. The aim here is to obtain the &lt;strong&gt;distribution&lt;/strong&gt; (called &lt;strong&gt;posterior distribution&lt;/strong&gt;) of parameters, given the data. If we denote the data by $D$, &lt;a href="https://en.wikipedia.org/wiki/Bayes%27_theorem"&gt;Bayes theorem&lt;/a&gt; relates (for a particular hypothesis about how the data were generated $H$), the likelihood of the data given some parameters $P(D|\theta,H)$, our prior expectations about how the parameters are distributed $P(\theta)$ and the posterior distribution of the parameters themselves $P(\theta|D,H)$ :&lt;/p&gt;
&lt;p&gt;$$P(\theta|D,H) = \frac{P(D|\theta,H)P(\theta)}{P(D)}.$$&lt;/p&gt;
&lt;p&gt;The priors $P(\theta)$ do not depend on the data and encode "domain knowledge" while the probability of the data set $P(D)$ over the whole parameter space is (typically) a high dimensional integral given by
$$P(D|H) = \int P(D,\theta|H)d\theta.$$&lt;/p&gt;
&lt;p&gt;Intuitively, we can see that the most likely parameters given the data, i.e. the parameters $\theta$ which maximize $P(\theta|D,H)$ ought to correspond to the sense of "best" or "most suitable" described above. From Bayes theorem, it is clear that the posterior distribution is directly proportional to the likelihood $P(\theta|D,H) \propto P(D|\theta,H)$. Thus, maximizing likelihood is one way to get estimates of the "most likely parameters" (in the limit of infinite data), but computing the full distribution $P(\theta|D,H)$ involves dealing with the difficult integral for $P(D|H)$.&lt;/p&gt;
&lt;h3&gt;Bayesian prediction and MCMC&lt;/h3&gt;
&lt;p&gt;Prediction in this framework is also fundamentally different from typical machine learning model. The probability of a new data point $d$,
$$P(d|D,H) = \int P(d|\theta,H)P(\theta|D,H)d\theta,$$
which consists of the expectation value of the new data point over the whole distribution of parameters given the observed data (the posterior distribution calculated obtained from the solution to the inference problem), instead of a value calculated by plugging in the "learned parameter values" into the machine learning model. &lt;/p&gt;
&lt;p&gt;The integrals needed for inference ($P(D|H) = \int P(D,\theta|H)d\theta$ as well as prediction $P(d|D,H) = \int P(d|\theta,H)P(\theta|D,H)d\theta$ need to be evaluated over the entire parameter space of the model which can be very high dimensional. Markov Chain Monte Carlo methods are used to approximate these integrals. &lt;a href="https://www.reddit.com/r/deeplearning/comments/8487xg/very_good_introduction_to_hamiltonian_monte_carlo/"&gt;This&lt;/a&gt; is an excellent overview of modern Hamiltonian Monte Carlo methods while &lt;a href="https://www.reddit.com/r/MachineLearning/comments/84fobk/superb_overview_and_motivation_for_monte_carlo/?ref=share&amp;amp;ref_source=link"&gt;this&lt;/a&gt; provides wonderful perspective from the dawn of the field. Both papers are long but eminently readable and highly recommended. &lt;/p&gt;
&lt;p&gt;Clearly then, along with the computational graph to define models, a PPL needs a good MCMC algorithm (or another inference algorithm) to compute the high dimensional integrals needed to infer as well as perform a prediction on a general probabilistic model. &lt;/p&gt;
&lt;p&gt;A broad overview of Bayesian machine learning is available &lt;a href="http://mlg.eng.cam.ac.uk/zoubin/talks/mit12csail.pdf"&gt;here (PDF)&lt;/a&gt; and &lt;a href="http://fastml.com/bayesian-machine-learning/"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now, we illustrate some of these points using the simplest possible example, linear regression.&lt;/p&gt;
&lt;h2&gt;Basic linear regression.&lt;/h2&gt;
&lt;p&gt;We will generate artificial data with known parameters, so that we can check if Greta (the PPL we are using for this article) gets it right later. &lt;/p&gt;
&lt;h3&gt;Generating fake data to fit a model to&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;length_of_data&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;
&lt;span class="n"&gt;sd_eps&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;intercept&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;-5.0&lt;/span&gt;
&lt;span class="n"&gt;slope&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;pi&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;-10&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="kc"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="kc"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;length.out&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;length_of_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;intercept&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;slope&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;rnorm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;length_of_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sd_eps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;data_frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;simulated dependent variable&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_point&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_smooth&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lm&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;ggtitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Fake experimental data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;ggthemes&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;theme_economist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="center" src="/figures/greta_playground/unnamed-chunk-2-1.png"&gt;&lt;/p&gt;
&lt;p&gt;Given this data, we want to write Greta code to infer the posterior distributions of the model parameters. &lt;/p&gt;
&lt;h3&gt;Defining clueless priors for model parameters&lt;/h3&gt;
&lt;p&gt;In this case, the parameters of our model are simple, but in principle, they can be arbitrary tensors. Since we really don't know anything about the prior distributions of our parameters, we look at the experimental data and take rough, uniform priors. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;intercept_p&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;-10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sd_eps_p&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;slope_p&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Defining the model&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mean_y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;intercept_p&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;slope_p&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;
&lt;span class="nf"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sd_eps_p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here, we hypothesize that the target variable $y$ is linearly dependent on some independent variable $x$ with a noise term drawn from a Gaussian distribution whose standard deviation is also a parameter of the model. &lt;/p&gt;
&lt;p&gt;Under the hood, Greta has constructed a computational graph that encapsulates all these operations, and defines the process of computing the target $y$ starting from the prior distributions of our input variables. We plot this computational graph below :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;our_model&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;intercept_p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;slope_p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sd_eps_p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;our_model&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="center" src="/figures/greta_playground/model.png"&gt;&lt;/p&gt;
&lt;h3&gt;Inference&lt;/h3&gt;
&lt;p&gt;There are two distinct types of inference possible, &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Sampling from the full posterior distribution&lt;/strong&gt; for the parameters given the data and the model. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maximizing likelihood to compute "most probable" parameters&lt;/strong&gt; given the data and the model.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Sampling from the posterior distribution of parameters with MCMC&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1000&lt;/span&gt;
&lt;span class="n"&gt;param_draws&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;mcmc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;our_model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;warmup&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and plot the densities of samples drawn from the parameter posterior distributions, and the parameter fits.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;mcmc_dens&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param_draws&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="center" src="/figures/greta_playground/unnamed-chunk-7-1.png"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;mcmc_intervals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param_draws&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="center" src="/figures/greta_playground/unnamed-chunk-7-2.png"&gt;&lt;/p&gt;
&lt;p&gt;By inspection, it looks like the &lt;a href="https://arxiv.org/abs/1701.02434"&gt;HMC&lt;/a&gt; has found reasonable values for our model parameters and their posterior distributions. &lt;/p&gt;
&lt;h4&gt;Most probable parameters&lt;/h4&gt;
&lt;p&gt;Explicitly, the mean estimates can be computed from the &lt;code&gt;param_draws&lt;/code&gt; data structure, or via the &lt;code&gt;greta::opt&lt;/code&gt; function.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;param_draws_df&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;as_data_frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param_draws&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;param_estimates&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;param_draws_df&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;summarise_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;param_estimates&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;print&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gu"&gt;##&lt;/span&gt; # A tibble: 1 x 3
&lt;span class="gu"&gt;##&lt;/span&gt;   intercept_p slope_p sd_eps_p
&lt;span class="gu"&gt;##&lt;/span&gt;         &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
&lt;span class="gu"&gt;##&lt;/span&gt; 1       -6.12    3.12     22.8
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;opt_params&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;opt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;our_model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;opt_params&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;par&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;print&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gu"&gt;##&lt;/span&gt; intercept_p     slope_p    sd_eps_p 
&lt;span class="gu"&gt;##&lt;/span&gt;   -6.686146    3.187089   23.300232
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Bayesian prediction&lt;/h3&gt;
&lt;p&gt;Bayesian prediction is implemented via the &lt;code&gt;calculate()&lt;/code&gt; function available in the latest release of &lt;code&gt;greta&lt;/code&gt; on github. This generates a prediction on $y$ for each draw from the posterior distribution of the parameters (see previous section). Taking the expectation over this distribution of predictions gives us the mean value of the target variable $y$ but we have the whole distribution of $y$ available to us if we need to analyse it. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mean_y_plot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;intercept_p&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;slope_p&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;
&lt;span class="n"&gt;mean_y_plot_draws&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;calculate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean_y_plot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;param_draws&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mean_y_est&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;colMeans&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean_y_plot_draws&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;data_pred&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_fit&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;mean_y_est&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_point&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;simulated dependent variable&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;geom_line&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_fit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;colour&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;estimated expectation value&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;ggtitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Fitted model&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;ggthemes&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;theme_economist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="center" src="/figures/greta_playground/unnamed-chunk-9-1.png"&gt;&lt;/p&gt;
&lt;h2&gt;Further exploration&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The most mature PPL out there (with good R bindings) is Stan. There is a lot of material available, and it might be a good place to start to pick up some intuition. See &lt;a href="http://mc-stan.org/users/documentation/"&gt;this page&lt;/a&gt;.  &lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.mit.edu/~9.520/spring10/Classes/class21_mcmc_2010.pdf"&gt;This&lt;/a&gt; is a good intro to the role of MCMC in inference.  &lt;/li&gt;
&lt;li&gt;These video lectures on &lt;a href="https://www.youtube.com/watch?v=oy7Ks3YfbDg"&gt;statistical rethinking&lt;/a&gt; emphasizing Bayesian statistics also seem interesting.&lt;/li&gt;
&lt;/ol&gt;</content><category term="data_sci_tech"></category></entry><entry><title>"Notes from the original ML course by Andrew Ng"</title><link href="https://theclarkeorbit.github.io/notes-from-the-original-ml-course-by-andrew-ng.html" rel="alternate"></link><published>2017-02-24T00:00:00+01:00</published><updated>2024-04-27T15:17:44+02:00</updated><author><name>pras</name></author><id>tag:theclarkeorbit.github.io,2017-02-24:/notes-from-the-original-ml-course-by-andrew-ng.html</id><summary type="html">&lt;p&gt;These are notes I took while watching the lectures from &lt;a href="https://www.coursera.org/learn/machine-learning"&gt;Andrew Ng's ML course&lt;/a&gt;. There is no code, just some math and my take aways from the course. It should still serve as a useful first document to skim for someone just starting out with machine learning.&lt;/p&gt;
&lt;h2&gt;What can ML …&lt;/h2&gt;</summary><content type="html">&lt;p&gt;These are notes I took while watching the lectures from &lt;a href="https://www.coursera.org/learn/machine-learning"&gt;Andrew Ng's ML course&lt;/a&gt;. There is no code, just some math and my take aways from the course. It should still serve as a useful first document to skim for someone just starting out with machine learning.&lt;/p&gt;
&lt;h2&gt;What can ML do ?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Mining large data sets&lt;/strong&gt;&lt;br&gt;
Automation and digitization has led to huge data sets which can be mined, and used using machine learning techniques. Predictions become possible because sophisticated algorithms can learn from large data sets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problems that cannot be programmed by hand&lt;/strong&gt;&lt;br&gt;
Flying a helicopter. It is very hard to program a computer to do this complex task, but a good neural network can learn to do it via a reasonable training program.&lt;/p&gt;
&lt;h2&gt;Definitions of ML&lt;/h2&gt;
&lt;p&gt;Arthur Samuel : "The field of study that gives computers the ability to learn without being explicitly programmed."&lt;/p&gt;
&lt;p&gt;Tom Mitchell : "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."&lt;/p&gt;
&lt;p&gt;Broad types of machine learning :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Supervised learning   &lt;/li&gt;
&lt;li&gt;Unsupervised learning   &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Supervised learning&lt;/h2&gt;
&lt;p&gt;The learning algorithm is provided with a dataset that includes the correct values for the target variable. So, in case we want to predict house prices as a function of size, then a data set that has a list of house sizes and the corresponding prices will be provided to an algorithm. Once the algorithm "leans" from this &lt;strong&gt;training set&lt;/strong&gt; and constructs a &lt;strong&gt;model&lt;/strong&gt; for house prices, this model can be used to &lt;strong&gt;predict&lt;/strong&gt; the prices of houses whose sizes are known listed in some &lt;strong&gt;test set&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;When the target variable (here price) is &lt;em&gt;continuous&lt;/em&gt;, the problem is known as a &lt;strong&gt;regression&lt;/strong&gt; problem. If the target variable is &lt;em&gt;discrete&lt;/em&gt; (e.g.. if we wanted to predict which zip code a house was in based on its size and price) the problem is called a &lt;strong&gt;classification&lt;/strong&gt; problem.&lt;/p&gt;
&lt;h2&gt;Unsupervised learning&lt;/h2&gt;
&lt;p&gt;Here, the problem is not to predict the value of a particular variable, but to detect some structure or pattern in the data set. A common example is &lt;strong&gt;clustering&lt;/strong&gt; where the points in the data set are grouped into clusters that are somehow similar to each other. Examples :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Google news clusters similar news stories together.  &lt;/li&gt;
&lt;li&gt;Genomics. Expression levels of thousands of genes are measured in various situations and genes which seem to be related to each other are identified using clustering.  &lt;/li&gt;
&lt;li&gt;Market segmentation for marketing.  &lt;/li&gt;
&lt;li&gt;Cocktail party algorithm. Separate the voices of different people at a party by identifying sounds with the same characteristics.   &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;and other such applications. &lt;/p&gt;
&lt;h2&gt;Models and cost functions&lt;/h2&gt;
&lt;p&gt;Recall that a supervised learning algorithm takes a data set (the &lt;strong&gt;training set&lt;/strong&gt; with $m$ examples (rows)) and learns from it to construct a &lt;strong&gt;model&lt;/strong&gt;. We denote the &lt;em&gt;features&lt;/em&gt; or &lt;em&gt;predictors&lt;/em&gt; by the letter $x$ while the target variable is $y$. $(x^i, y^i)$ is one row of the training set. $x^i$ is a vector with as many elements as there are features. For convenience, we will assume the number of features (predictors, columns) to be $n$. &lt;/p&gt;
&lt;p&gt;We denote the model by $h$ (for hypothesis) this is a function that maps from $x$ to $y$. If $h$ is parametrized by some set of parameters $\theta$, we can write $y=h_{\theta}(x)$. When the model is linear, we call this &lt;strong&gt;linear regression&lt;/strong&gt;.   &lt;/p&gt;
&lt;p&gt;$$
y=h_{\theta}(x)=\sum_{j}\theta_j x_j=x\theta
$$&lt;/p&gt;
&lt;p&gt;where $x$ is a $1&lt;em&gt;n$ vector and $\theta$ is a $n&lt;/em&gt;1$ vector of parameters. Note that the first row of data is always 1, so that the first parameter is always a bias value. &lt;/p&gt;
&lt;p&gt;We determine the values of the model parameters $\theta$ that will result in the best possible prediction of $y$ given $x$. We do this by defining a cost function (something that measures the error in predictions from our model) and minimise this cost function to obtain the final form of our model. This is an &lt;em&gt;optimization problem&lt;/em&gt;. So, one might want to find $\theta$ such that $E\left[\sum_i(h_{\theta}(x^i)-y^i)^2\right]$ is minimized. So, we minimise the expectation value of the squared error.  &lt;/p&gt;
&lt;p&gt;$$
J(\theta) = E\left[\sum_i(h_{\theta}(x^i)-y^i)^2\right]
$$&lt;/p&gt;
&lt;p&gt;This is called the squared error cost function and is commonly used for linear regression problems. Other cost functions are possible, but generally the form of the cost function that is used is determined by how efficiently it can be minimized. One common way of writing the above cost function is &lt;/p&gt;
&lt;p&gt;$$J(\theta) = \frac{1}{2m}\left[\sum_{i=1}^m (h_{\theta}(x^i)-y^i)^2\right] $$&lt;/p&gt;
&lt;p&gt;where the factor of $\frac{1}{2}$ is added by convention. Representing by $X$ the matrix of the data (excluding the target variable) where predictors are columns and each example is in a different row ($X$ is an $m&lt;em&gt;n$ matrix), and letting $y$ be the $n&lt;/em&gt;1$ vector with the target variables, we can write down the matrix version of the cost function for linear regression ($h_{\theta}(x^i)=x^i\theta$, the product of the vector of parameters $\theta$ and the $i^{th}$ row of the data) as follows,&lt;/p&gt;
&lt;p&gt;$$J(\theta) = \frac{1}{2m}(X \theta-y)^T (X \theta-y).$$&lt;/p&gt;
&lt;p&gt;Now, the problem of learning this linear model is reduced to searching for $\theta^{&lt;em&gt;}$ in the multi dimensional space ${\theta}$ for which the cost function $J$ is minimised for the given training set. This is achieved (in general) using something called a &lt;/em&gt;&lt;em&gt;gradient descent algorithm&lt;/em&gt;*. &lt;/p&gt;
&lt;p&gt;$$
\theta^{*} = \arg \min_{\theta}\left(E\left[\sum_i(h_{\theta}(x^i)-y^i)^2\right]\right)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notation alert :&lt;/strong&gt; $x^i_j$ denotes the $j^{th}$ feature in the $i^{th}$ row of the training set. &lt;/p&gt;
&lt;h2&gt;Gradient Descent&lt;/h2&gt;
&lt;p&gt;The basic prescription is the following :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start with a random initial guess $\theta_0$.  &lt;/li&gt;
&lt;li&gt;Find the direction in the space ${\theta}$ in which the cost function $J$ decreases the most.  &lt;/li&gt;
&lt;li&gt;Take a baby step in this direction and repeat step 2, until a minimum is reached.   &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We determine the "direction of maximum decrease" for $J(\theta)$ using a derivative. &lt;/p&gt;
&lt;p&gt;$$
\text{repeat until convergence} \left[\theta_j := \theta_j-\alpha\frac{dJ(\theta)}{d\theta_j} \text{  }\forall j\right]
$$&lt;/p&gt;
&lt;p&gt;where $\alpha$ determines the size of our baby step (the &lt;em&gt;learning rate&lt;/em&gt;), and we are just walking in the direction of the &lt;a href="https://en.wikipedia.org/wiki/Gradient"&gt;gradient&lt;/a&gt; $-\nabla_\theta J$. The symbol $:=$ here is an assignment, not a truth assertion. It is implicit that all the $\theta$ are updated simultaneously, and that the cost function $J$ is differentiable with respect to all the $\theta$. This is what makes the mean square error a good cost function - some other possible cost functions (like the absolute error) are not differentiable.&lt;/p&gt;
&lt;p&gt;There are various flavours of gradient descent. If all the samples of the training set are used to compute the gradient at every step (as described above) the algorithm is called &lt;em&gt;batch gradient descent&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For linear regression, all the derivatives $\frac{dJ(\theta)}{d\theta_j}$ are trivial to compute (see &lt;a href="http://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/"&gt;this blog post&lt;/a&gt; for an excellent explanation for derivatives on matrices) and the gradient descent algorithm can be written as,&lt;/p&gt;
&lt;p&gt;$$\text{repeat until convergence}\left[\theta_j :=\theta_j - \alpha \frac{1}{m}\sum_i (x^i\theta-y^i)x^i_j\right]$$&lt;/p&gt;
&lt;p&gt;which, in matrix form becomes,&lt;/p&gt;
&lt;p&gt;$$\text{repeat until convergence}\left[\theta := \theta - \frac{\alpha}{m}(X\theta-y)^TX \right].$$&lt;/p&gt;
&lt;h3&gt;Practical tips for gradient descent&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Feature scaling :&lt;/strong&gt; When different features are on very different scales, the hills/valleys we would like to reach in our gradient descent optimization are shaped like long narrow canyons, and along the length, the gradient descent algorithm converges very slowly to the minimum/maximum. If we scale features so that the hills/valleys have more "circular" symmetry, gradient descent converges faster. It is better to have all features scaled into the same range of values, say $[-1,1]$ or $[0,1]$. &lt;/p&gt;
&lt;p&gt;A common way to scale a feature $j$ would be
$$
v_j = \frac{x_j-\mu_j}{\sigma_j}
$$&lt;/p&gt;
&lt;p&gt;where $\mu_j$ is the mean and $\sigma_j$ is the standard deviation of the values taken by feature $j$ in the training set, and $v_j$ are the new scaled values of the feature $j$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Learning rate $\alpha$ :&lt;/strong&gt; If gradient descent is working properly, $J$ should decrease after every iteration, and convergence is defined by $J$ decreasing by some very small value at each iteration (say $10^{-4}$ or so). If gradient descent is blowing up ($J$ is &lt;em&gt;increasing&lt;/em&gt; with each iteration) it could be because the learning rate $\alpha$ is too large and the algorithm is "overshooting" the optimum. Decreasing $\alpha$ should fix this. For a small enough $\alpha$ gradient descent should ALWAYS find a local optimum, and $J$ should decrease with every iteration. &lt;/p&gt;
&lt;p&gt;On the other hand, a very small $\alpha$ will lead to very slow convergence. &lt;/p&gt;
&lt;h3&gt;Polynomial regression&lt;/h3&gt;
&lt;p&gt;This is a generalization of linear regression which we saw in previous sections. now, instead of the simple linear from $h=\theta^Tx$, we also include higher powers of each factor. So, we list our new features as $u_0 = x_0 = 1, u_{1} = x^{f_1}&lt;em 2&gt;1,u&lt;/em&gt;} = x^{f_2&lt;em k&gt;1\cdots u&lt;/em&gt;_2\cdots \forall k, \forall x$. Thus, we include in our model all powers of each factor that we think are relevant, including fractional powers. Now, the hypothesis function (model) is defined as usual, $h=\theta^Tu$ on the new feature set ${u}$, and the same principles off linear regression discussed earlier apply.} = x^{f_k&lt;/p&gt;
&lt;p&gt;Thus, polynomial regression is an example of something that is ubiquitous in machine learning applications - &lt;strong&gt;feature engineering&lt;/strong&gt;. Machine algorithms find optima faster, and predict better when certain functions of the features available in the data are also included as inputs. Finding such functions can be a matter of intuition and experience as well as thorough data exploration. &lt;a href="www.kaggle.com"&gt;Kaggle&lt;/a&gt; contests are an example where it is clever feature engineering more than anything else that determines performance of machine learning algorithms.&lt;/p&gt;
&lt;h2&gt;The normal equation for linear regression&lt;/h2&gt;
&lt;p&gt;For linear regression, one can solve for $\theta$ analytically, without the iterative gradient descent procedure. 
The problem is to minimize $J(\theta_0,\theta_1,\cdots\theta_n) = E_i[h_{\theta}(x^i)-y^i]$ wrt the $\theta$s. This requires the solution of $(n+1)$ equations&lt;/p&gt;
&lt;p&gt;$$
\frac{d}{d\theta_0}J(\theta)=\frac{d}{d\theta_1}J(\theta)=\cdots=0
$$&lt;/p&gt;
&lt;p&gt;For the particular quadratic cost function we have used, the solution is given by&lt;/p&gt;
&lt;p&gt;$$
\theta^*=(X^TX)^{-1}X^Ty
$$
where $X$ is the matrix of all features in the training set (including $x_0=1$) and $y$ is the vector of target variables in the training set. For the derivation (express $J$ in matrix form, and differentiate wrt $\theta$ and set the derivatives to 0, there are subtleties while differentiating matrices, transposes wrt vectors) see page 45 of &lt;a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/"&gt;ESLR&lt;/a&gt;. See &lt;a href="http://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/"&gt;this blog&lt;/a&gt; for a friendly explanation of the derivation sketched out in ESLR.&lt;/p&gt;
&lt;p&gt;The normal equation is excellent when the matrix $X$ is small and the number of features is $&amp;lt;10000$. The matrix operations become slow for large data sets, and then gradient descent is the fall back option.&lt;/p&gt;
&lt;p&gt;In the normal equation, we see that we need to invert the matrix $X^TX$ and many things can make a matrix non-invertible. Generally, the solution is to delete some features that might be redundant and/or &lt;a href="https://en.wikipedia.org/wiki/Regularization_%28mathematics%29"&gt;regularize&lt;/a&gt; the matrix. Regularization prevents over-fitting when the number of variables are greater than the number of equations (in which case of course, a unique solution cannot be found).&lt;/p&gt;
&lt;h2&gt;Logistic regression, classification&lt;/h2&gt;
&lt;p&gt;When the target variable is discrete, the problem is called a classification problem. E.g.. is an email spam/not spam ? is a tumour malignant/non malignant ? Needless to say, it is not a good idea to predict discrete variables with linear regression. &lt;/p&gt;
&lt;p&gt;A simple type of classification problem is &lt;em&gt;binary classification&lt;/em&gt;, where the target variable can take one of two values. It is common to denote the two levels of a binary variable by ${0,1}$ so we need our hypothesis function $0\leq h_{\theta}(x)\leq 1$. We construct this using the sigmoid form&lt;/p&gt;
&lt;p&gt;$$
h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}} = g(\theta^Tx)
$$&lt;/p&gt;
&lt;p&gt;The function $h_{\theta}$ is interpreted as the probability that the target variable is 1 given $x$ and parametrized by $\theta$, and we denote the sigmoid function by $g$. &lt;/p&gt;
&lt;p&gt;$$
h_{\theta}(x) = \mathbf{P}(y=1|x;\theta)
$$&lt;/p&gt;
&lt;p&gt;**Tip : ** A logistic regression problem has another parameter. Given a hypothesis function $h_{\theta}(x)$, what is the threshold for the prediction to be $y=1$ rather than $y=0$ ? A sensible boundary may be $h_{\theta}=0.5$, but in practice, this parameter needs to be optimised on the training set (or a validation set) to obtain the best possible predictions.  &lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;decision boundary&lt;/strong&gt; is the hyper-surface defined by $\theta^Tx=0$ (corresponding to $h_{\theta}=0.5$) that is supposed to separate the two classes from each other. As $\theta\to\theta^*$ (the optimum), the decision boundary approaches the best possible separation between the two classes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Non-linear decision boundaries&lt;/strong&gt; are achieved through &lt;em&gt;feature engineering&lt;/em&gt;, and including as features various powers and functions of the original ${x}$, as before. Arbitrarily complex decision boundaries are possible with this kind of feature engineering. &lt;/p&gt;
&lt;h3&gt;Cost function for classification&lt;/h3&gt;
&lt;p&gt;If the hypothesis function $\frac{1}{1+e^{-\theta^Tx}}$ is substituted into the quadratic error we used for linear regression, the resulting cost function $J(\theta)$ turns out not to be convex, and with lots of local optima that will make it impossible for gradient descent to find the global minimum.&lt;/p&gt;
&lt;p&gt;The function of the error commonly used for logistic regression is &lt;/p&gt;
&lt;p&gt;$$
  \text{ErrCost}(\theta,x^i) = \begin{cases} 
      -\text{log}(h_{\theta}(x^i)) &amp;amp; y^i= 1 \
      -\text{log}(1-h_{\theta}(x^i)) &amp;amp;  y^i=0
   \end{cases}
$$
$$
J(\theta) = E_i[\text{ErrCost}(\theta,x^i)]
$$
It is clear that this function for the error imposes a very heavy penalty (which can be $\infty$) for a completely wrong prediction. This also grantees a convex $J(\theta)$ for logistic regression.&lt;/p&gt;
&lt;p&gt;Since $y\in {0,1}$, we can write,
$$
J(\theta) = E_i[-y\text{log}(h_{\theta}(x^i))-(1-y)\text{log}(1-h_{\theta}(x^i))]
$$
This cost function follows from maximum likelihood estimation. A cool derivation to look up. The optimal values $\theta^*$ are obtained by gradient descent as before.&lt;/p&gt;
&lt;h3&gt;Using other optimization algorithms&lt;/h3&gt;
&lt;p&gt;Gradient descent is not the only algorithm available to us. Given that we can compute $J(\theta)$ and $\frac{\partial}{\partial\theta_j}J(\theta)$, we can also use  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Conjugate gradient descent  &lt;/li&gt;
&lt;li&gt;BFGS  &lt;/li&gt;
&lt;li&gt;L-BFGS  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;etc. to compute the optimum. Most of them pick the learning rate $\alpha$ adaptively. They also converge faster. Looking up quickly what they do, is not a bad idea, but it is possible to use these optimization algorithms as black boxes without looking into the details. &lt;/p&gt;
&lt;p&gt;One must write an efficient function that can supply the value of $J(\theta)$ and its derivatives, and supply this function to one of Octave's in built optimization routines. Using these routines and more sophisticated optimization algorithms enables us to use logistic and linear regression on larger data sets.&lt;/p&gt;
&lt;h3&gt;Multiclass classification&lt;/h3&gt;
&lt;p&gt;Simple logistic regression works well for binary classification, but what is a good way to generalize it to a problem with multiple categories for the target variable ? We use an idea called &lt;strong&gt;one-vs-all classification&lt;/strong&gt;. So, if we have $C$ categories for the target variable $y\in { l_1\cdots l_C}$, we create $C$ new training sets each of which has a binary target variable. In the $q^{th}$ training set the target variable is 1 only if $y=l_q$ and 0 otherwise. Now, we can train a logistic regression classifier for each of these $C$ sets, $h^1_{\theta}\cdots h^q_{\theta} \cdots h^C_{\theta}$, where $h^q_{\theta}(x)=\mathbf{P}(y=l_q|x;\theta)$ and the final prediction is given by 
$$y=\max_q h^{q}_{\theta}(x)$$&lt;/p&gt;
&lt;h2&gt;Overfitting&lt;/h2&gt;
&lt;p&gt;Many learning algorithms - if not used carefully - build models that capture the behaviour of the training set as well as the &lt;em&gt;noise&lt;/em&gt; that is inherent in any data set. In general, a very rigid (&lt;strong&gt;biased&lt;/strong&gt; because it has encodes some strong assumptions that cannot be changed, e.g.. "the model is a straight line" is one such strong assumption) model will not be able to capture some important behaviour of the data. On the other hand a &lt;em&gt;very flexible&lt;/em&gt; model (a very high order polynomial, for instance) will end up capturing a lot of noise from the data, and will not predict well when presented with new data which will have a different realization of randomness. Such a model is said to have high &lt;strong&gt;variance&lt;/strong&gt; and this phenomenon is called &lt;strong&gt;overfitting&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;The link to the usual scientific idea about fitting is that given enough parameters, any data set can be fit perfectly. That does not mean that such a model will be good at predicting the result when new data is presented to it. However, with too few parameters - too simple/rigid/biased a model - the data cannot be properly explained. This is the &lt;strong&gt;bias variance tradeoff&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;Visualizing the data and the model can he helpful in diagnosis of over-fitting, but more importantly, good data visualization and exploration could help make appropriate model decisions to optimise predictive power despite the trade-off between bias and variance. &lt;/p&gt;
&lt;p&gt;If there is over-fitting, the following remedy could be tried :  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Reduce the number of features&lt;/em&gt;&lt;br&gt;
    - Manually select important features&lt;br&gt;
    - Model selection algorithms (dealt with later)&lt;br&gt;
&lt;em&gt;Regularization&lt;/em&gt;&lt;br&gt;
    - Keep all features, but modify/reduce the values of parameters of the model&lt;br&gt;
    - Works well when there are a lot of features and each has low predictive power.  &lt;/p&gt;
&lt;h3&gt;Regularization&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Regularization&lt;/strong&gt; basically consists of imposing a &lt;strong&gt;cost on complexity&lt;/strong&gt; of the model. So, a model with large number of parameters, or a model that is very flexible (models with low bias, high variance) are penalized with a term in the cost function. So, the cost function might become 
$$J_R(\theta) = J(\theta) + \underbrace{\lambda\cdot f_R(\theta)}_{\text{regularization term}}$$
where $\lambda$ is called the regularization parameter. Larger $\lambda$ imposes more of a cost on flexibility and makes the model more rigid (pushes the bias-variance trade-off toward more bias, less variance). Low $\lambda$ improves the fitting accuracy with increased model flexibility (pushes the bias variance trade-off toward less bias, more variance)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Regularized linear regression&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One possible regularised cost function for linear regression would be a sum of parameter squares,
$$J_R(\theta) = \frac{1}{2m}\left[\sum_{i=1}^m (h_{\theta}(x^i)-y^i)^2 + \underbrace{\lambda\sum_j (\theta_j)^2}&lt;em&gt;{\text{regularization term}}\right],$$
then, the gradient descent algorithm for this cost function looks like
$$\text{repeat until convergence}\left[\theta_j :=\theta_j-\alpha\underbrace{\left(\frac{\lambda}{m}\theta_j + \frac{1}{m}\sum_i (x^i\theta-y^i)x^i_j\right)}&lt;/em&gt;\right]$$}{\partial\theta_j}J_R(\theta)&lt;/p&gt;
&lt;p&gt;The normal equation (derived in the same way as before) looks like
$$\theta = (X^TX-\lambda I)^{-1}X^Ty$$
The Identity matrix is used since here, we are treating all the $\theta$s in the same way. If - as in the lectures of Prof. Ng - one wants to leave $\theta_0$ (the bias term) out of the regularization process, then one use a matrix in which the first row of the identity matrix is replaced with all 0s.&lt;/p&gt;
&lt;p&gt;For wide data $m&amp;lt;n$ (more features than examples) $X^TX$ is non invertible. But, the regularised version  $X^TX-\lambda I$ solves this issue too.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Regularized logistic regression&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The regularized cost function for logistic regression follows along the same lines as for linear regression,
$$
J_R(\theta) = E_i[-y^i\text{log}(h_{\theta}(x^i))-(1-y^i)\text{log}(1-h_{\theta}(x^i))]+\frac{\lambda}{2m}\sum_j (\theta_j)^2
$$
Gradient descent too has the same form, but of course, the hypothesis function is the logistic function.&lt;/p&gt;
&lt;h2&gt;The anatomy of supervised learning&lt;/h2&gt;
&lt;p&gt;For &lt;strong&gt;any supervised machine learning problem&lt;/strong&gt;, the following are true :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;There is a &lt;strong&gt;training set&lt;/strong&gt;. This consists of data which we will use to teach the "machine" about the system we are interested in. We organize the training set such that each row of the data is an independent example, and each column is one feature, or predictor. We call this table $X$. The target variable - which is to be predicted - we call $y$. The problem is to identify a good function $y=h(x)$. If we are successful, then our function $h$ will correctly predict the target variable for data it has not seen, from a set commonly called the &lt;strong&gt;test set&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For a given problem and data, we start with some &lt;strong&gt;hypothesis&lt;/strong&gt; about the form of the function $h$ parametrized by the parameter set $\theta$. In this scheme, our assumption is that $y=h_{\theta}(x)$. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We define a &lt;strong&gt;cost function&lt;/strong&gt; $J(\theta,X,y)$ that quantifies the error in prediction on the training set using $h_{\theta}$. A good cost function not only reflects the quality of the prediction, but is also well behaved and differentiable (since it will be fed to an optimization algorithm).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now, the machine learning problem is reduced to an &lt;strong&gt;optimization problem&lt;/strong&gt;. We need to find the set $\theta^&lt;em&gt;$ which minimises $J(\theta,X,y)$. Generally, we write a subroutine that takes in the training data points and a particular value of $\theta$ and returns the value and derivatives of $J$ with respect to the $\theta$s. This can be supplied to fast optimising routines found in any number of easily available libraries, and a good value of $\theta^&lt;/em&gt;$ is found that minimises the cost function $J$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now, we have constructed a &lt;strong&gt;model&lt;/strong&gt; for our data - $h_{\theta^&lt;em&gt;}$. When we have some new and unseen row of data $\hat{x}$, we can make a prediction by evaluating $h_{\theta^&lt;/em&gt;}(\hat{x})$. How well a machine learning algorithm does, is judged by how predictions on an unseen test set where the correct $\hat{y}$ are known. So, the performance of the machine learning model is given by $J(\theta^*,\hat{X},\hat{y})$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Neural Networks&lt;/h2&gt;
&lt;p&gt;Logistic regression with feature engineering (higher order polynomial features etc) works well when there are small number of features. When there are a very large number of features already, even adding an exhaustive list of quadratic features becomes unreasonable, let alone higher orders. This makes logistic regression unsuitable for complex problems with large number of features. &lt;/p&gt;
&lt;p&gt;Even small pictures (for instance) have thousands and thousands of pixels, each of which has some values (intensity, colour) associated with it. This is a very large list of features. It is not reasonable to include all the higher order combinations of such a large feature set for an algorithm like logistic regression.&lt;/p&gt;
&lt;p&gt;Neural Networks turn out to be a much better way to deal with this need for non linearity. One example of the power of neural networks is illustrated by the &lt;strong&gt;one learning hypothesis&lt;/strong&gt; - the brain (a super complex neural network) does not do each learning task with a unique algorithm, but uses the same learning algorithm to do ALL the learning tasks it encounters. Then, it would make sense to mimic this learning algorithm to recreate such versatility in our machine learning systems. This is where neural networks excel. There are all kinds of cool experiments where the brain can learn to &lt;a href="https://thepsychologist.bps.org.uk/volume-25/edition-12/exotic-sensory-capabilities-humans"&gt;interpret new sensory stimuli&lt;/a&gt;, or learn to see with the audio cortex, etc.&lt;/p&gt;
&lt;p&gt;A very good introduction to the fundamentals of neural networks is available &lt;a href="http://neuralnetworksanddeeplearning.com/chap1.html"&gt;from M. Nielsen here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On our scheme, each neuron as several inputs ${x}$, and one output $h_{\theta}(x)$. Each neuron will typically implement a sigmoidal function (parametrized by $\theta$) on the inputs. As before,&lt;br&gt;
$$h_{\theta}(x) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}.$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notation :&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;The inputs are &lt;strong&gt;layer 1&lt;/strong&gt;, the last layer ($L$) is the &lt;strong&gt;output layer&lt;/strong&gt; and all intermediate layers are &lt;strong&gt;hidden layers&lt;/strong&gt;. The activation (computed output) of a neuron $i$ in layer $j$ is denoted by $a^j_i$. The weight of the output $x_p$ from a neuron $p$ in layer $j-1$ coming into neuron $i$ in layer $j$, is represented by $\Theta^{j-1}&lt;em j-1&gt;{ip}$. So, for each set of connections from layer $j-1\to j$ we have the weight matrix $\Theta^{j-1}$. Now, the activation of all neurons in layer $j$ is given by the vector,
$$a^j = h(\Theta^{j-1}x),$$
where $h$ is the sigmoidal function as before.
By convention, a bias input $x_0$ present at each neuron. So, if there are $s&lt;/em&gt;+1)$ dimensional matrix. }$ neurons in layer $j-1$ and $s_j$ neurons in layer $j$, $\Theta^{j-1}$ is a $s_j\times (s_{j-1&lt;/p&gt;
&lt;p&gt;If we have one hidden layer with 3 neurons, and 3 inputs (plus a bias), and one output neuron, then our neural network is represented by,
$$\begin{bmatrix} x_0\ x_1\ x_2\ x_3 \end{bmatrix} \to
\begin{bmatrix} a^2_0\ a^2_1\ a^2_2\ a^2_3 \end{bmatrix} \to
h_{\theta}(x),
$$
where $h_{\Theta}(x)$ represents the computation of the output neuron on the inputs from the hidden layer. For this case, the output is,
$$
h_{\Theta}(x) = g\left(\Theta^2&lt;em&gt;\underbrace{\text{adBias}(\Theta^1&lt;/em&gt;\text{adBias}(x^T))}_{\text{output of layer 2 }{a^2}}\right)
$$
where the function $\text{adBias}()$ increases dimension by 1 and adds the bias input $1$ for each layer, and $\Theta^1$, $\Theta^2$ are the weights to go from layer $1 \to 2$, and $2 \to \text{output}$ layer respectively.&lt;/p&gt;
&lt;p&gt;This process of computing the output of the neural network by starting with the input layer is called &lt;strong&gt;forward computation&lt;/strong&gt;. Each neuron is just a logistic regression unit, with the features being the outputs of neurons in the last layer. This means that each layer &lt;em&gt;learns&lt;/em&gt; what the best features are, to solve the given problem. This is what eliminates the need to include huge numbers of higher order terms as we would if were just doing logistic regression. &lt;/p&gt;
&lt;p&gt;The way neurons are linked up in an artificial neural network is known as the &lt;strong&gt;architecture&lt;/strong&gt; of the neural network.&lt;/p&gt;
&lt;p&gt;The output layer of a neural network can have more than one neuron. This enables multi-class classification, and we associate each class we are interested in with a vector, each component being the output of one output neuron. &lt;/p&gt;
&lt;h2&gt;Backpropagation&lt;/h2&gt;
&lt;p&gt;The feed forward network with some parameters $\Theta$ described above can be thought of as some complicated hypothesis function that maps $x\to y$. To find the parameters that correspond to a good (predictive) mapping, we need to (as before) define a cost function and find the parameters that minimise it. &lt;/p&gt;
&lt;h3&gt;Cost function for neural network&lt;/h3&gt;
&lt;p&gt;Let the number of layers in a neural network be denoted by $L$ and the number of neurons in a layer $l$ be denoted by $s_l$. In terms of classification problems with $K$ classes, we will have $s_L=K$ neurons in the output layer. &lt;/p&gt;
&lt;p&gt;Clearly, we need a generalization of the cost function for logistic regression. Instead of one value, the network is outputting a $K$ dimensional vector. To reflect this we sum over all the outputs and the cost function can be written as,&lt;/p&gt;
&lt;p&gt;$$J(\Theta)= -\frac{1}{m}\left[\underbrace{\sum_i}&lt;em _text_o_p="\text{o/p"&gt;{\text{examples}}\underbrace{\sum_k}&lt;/em&gt;(x^i)}} y^i_k\text{log}(h_{\Theta&lt;em _Theta="\Theta"&gt;k)+(1-y^i_k)\text{log}\left(1-h&lt;/em&gt;(x^i)&lt;em l_1="l+1"&gt;k\right)\right]+\underbrace{\frac{\lambda}{2m}\underbrace{\sum^{L-1}_l\sum^{s_l}_i\sum^{s&lt;/em&gt;}&lt;em _text_layers_="\text{layers," weights&gt;j}&lt;/em&gt;$$}} (\Theta^l_{ji})^2}_{\text{regulatization term}&lt;/p&gt;
&lt;p&gt;In the cost function above, the bias parameters are not penalized in the regularization term, per convention. The parameters of the neural network are the $\Theta^l_{ji}$ each of which is the weight to go from neuron $i$ of layer $l$ to neuron $j$ of layer $l+1$. &lt;/p&gt;
&lt;h3&gt;Learning/optimizing the parameters of a neural network&lt;/h3&gt;
&lt;p&gt;As before, we need to search for the parameters $\Theta^&lt;em&gt;$ that minimise the cost function $J(\Theta)$. The algorithm used to do this for neural networks is called the &lt;/em&gt;&lt;em&gt;backpropagation&lt;/em&gt;* algorithm. As before, for particular values of ${\Theta^l_{ij}}$ we need to compute the value $J(\Theta)$ as well as all the derivatives $\frac{\partial}{\partial \Theta^l_{ij}}J(\Theta)$. &lt;/p&gt;
&lt;p&gt;To calculate the dependence of $J$ on the parameters, we use the chain rule. Consider the parameters $\Theta^{L-1}$ which feed the output layer,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\frac{\partial J(\Theta)}{\partial \Theta^{L-1}&lt;em ji&gt;{ji}} &amp;amp;= \frac{\partial J(\Theta)}{\partial h^L_j}
{\frac{\partial h^L_j}{\partial z^{L}_j}}
\underbrace{\frac{\partial z^{L}_j}{\partial \Theta^{L-1}&lt;/em&gt; \
&amp;amp;= \delta^L_j h^{L-1}_i.
\end{align}
$$
where $h^L_j$ denotes the activation of the $j^{th}$ neuron in the $L^{th}$ layer (in this case, the output layer), and the quantities below the under-braces follow from trivial differentiation. We defined $z^{L}_j = \Theta^{L-1}_j\text{adBias}(h^{L-1})$ and,
$$
\delta^L_j = \frac{\partial J(\Theta)}{\partial h^L_j}\frac{\partial h^L_j}{\partial (z^{L}_j)} = \frac{\partial J(\Theta)}{\partial h^L_j}g'(z^{L}_j)
$$}}}_{h^{L-1}_i&lt;/p&gt;
&lt;p&gt;We would like to calculate the $\delta^{L-1}$s based on our knowledge of the $\delta^{L}$s. This is the essence of back-propagation, we compute the errors for earlier layers based on the error we compute at the output. What follows, is going to be a repeated application of uni and multivariate chain rules (see &lt;a href="http://adbrebs.github.io/Backpropagation-simply-explained/"&gt;this blog for another detailed explanation&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\delta^{L-1}&lt;em kj&gt;j &amp;amp;= \frac{\partial J(\Theta)}{\partial h^{L-1}_j}\frac{\partial h^{L-1}_j}{\partial (z^{L-1}_j)} \
&amp;amp;=  \frac{\partial J(\Theta)}{\partial z^{L-1}_j}\
&amp;amp;= \sum_k\frac{\partial J(\Theta)}{\partial z^{L}_k}\frac{\partial z^{L}_k}{\partial z^{L-1}_j}\
&amp;amp;= \sum_k \delta^{L}_k \frac{\partial z^{L}_k}{\partial z^{L-1}_j}\
&amp;amp;= \sum_k \delta^{L}_k \frac{\partial z^{L}_k}{\partial h^{L-1}_j}\frac{\partial h^{L-1}_j}{\partial z^{L-1}_j}\
&amp;amp;= g'(z^{L-1}_j)\sum_k \delta^L_k \Theta^{L-1}&lt;/em&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Then, the usual gradient descent algorithm tells us,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\Delta \Theta^{L-1}&lt;em ji&gt;{ji} &amp;amp;= -\alpha \frac{\partial J(\Theta)}{\partial \Theta^{L-1}&lt;/em&gt; \
&amp;amp;= -\alpha \delta^L_j h^{L-1}_i.
\end{align}
$$}&lt;/p&gt;
&lt;p&gt;And, since we can calculate all the $\delta$s successively starting from the output layer, we can derive the corrections to all the parameters of the neural network.&lt;/p&gt;
&lt;p&gt;**In matrix form : **&lt;/p&gt;
&lt;p&gt;$$
\delta^{l-1} = g'(z^{l-1})\odot(\Theta^{l-1})^T\delta^l\
\Delta \Theta^{l-1} = -\alpha (h^{l-1})^T\delta^l\
h^l = g(\Theta^{l-1}\text{adBias}(h^{l-1}))
$$&lt;/p&gt;
&lt;p&gt;It is implicit in the above discussion that this process is conducted for one training example at a time. So,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Using forward propagation, calculate the output $h_{\Theta}(x)$ for one training example.&lt;/li&gt;
&lt;li&gt;Using the above prescription and the correct output $y$, calculate the correction to each parameter by successively calculating the $\delta$s, starting from the output layer. (be careful about the scaling of the regularization term, to update for each example, the regularization parameter $\lambda$ should be divided by $m$).&lt;/li&gt;
&lt;li&gt;Repeat for every training example.&lt;/li&gt;
&lt;li&gt;This yields $J(\Theta)$ and its derivatives. Supply these to a good optimisation algorithm, which will repeat steps 1-3 (back-propagation) to calculate the cost function and it's derivatives for each iteration.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Tip :&lt;/strong&gt; In any implementation of back-propagation, it is a good idea to check that the algorithm is computing the right derivatives by implementing a simple numerical differentiation loop to check that back-propagation is giving the right values for derivatives. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tip :&lt;/strong&gt; Network architecture.&lt;br&gt;
Number of i/p neurons - dimension of input vector,&lt;br&gt;
Number of o/p vector - number of classes, &lt;br&gt;
So, the questions remain, how many hidden layers and how many neurons should each one have ? The more hidden neurons the better, but of course larger networks are more computationally expensive. Usually, number of hidden units is or the order of dimension of input vector.&lt;/p&gt;
&lt;h3&gt;Visualizing the innards of a neural net&lt;/h3&gt;
&lt;p&gt;For a dataset like &lt;a href="http://yann.lecun.com/exdb/mnist/"&gt;MNIST&lt;/a&gt; of handwritten digit images, each 20x20 pixels in size, the first (input) layer has $s_1=400$. Thus, each neuron in the first hidden layer has $400+1$(bias) inputs going to it, one from each pixel. After the network has been trained, we can visualize the weights of these 400 inputs as $20\times 20$ images, and learn what aspect of the picture each of the neurons in the first hidden layer is looking for. Thus, in this case, each row of $\Theta^1$ can be visualized as an image representing the input to each of the neurons, similarly for subsequent layers. &lt;/p&gt;
&lt;p&gt;This sort of visualization tells us what elements of the picture each neuron is looking for (it will be activated/fire if that element is present), and thus it becomes clear how subsequent layers look for combinations of these basic elements, and so on. The power of deep learning and neural networks to learn at multiple levels of abstraction is neatly illustrated even in a dataset as seemingly simple as MNIST.&lt;/p&gt;
&lt;p&gt;The most famous visualization of neural nets, of course, is &lt;a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html"&gt;Google's inceptionism&lt;/a&gt; blog where neural networks trained to detect various objects in pictures were used to generate images.. and representations of the objects the networks were supposed to detect came through in novel and interesting ways. &lt;/p&gt;
&lt;h2&gt;Measuring, improving predictive performance&lt;/h2&gt;
&lt;p&gt;Arbitrarily low training error is possible with an arbitrarily complex model. But, over-fitting the training data reduces prediction accuracy on new data. This is where &lt;strong&gt;test sets&lt;/strong&gt; become useful. So, we randomly split our training set into two, say in a $0.7/0.3$ split. Then, we first learn on the training set by minimising $J(\theta)$, and compute the test set error $J_{test}(\theta)$. Then perhaps we can modify $\lambda$ to optimise $J_{test}(\theta)$. This mitigates the over-fitting problem.&lt;/p&gt;
&lt;h3&gt;Train/Validation/Test sets&lt;/h3&gt;
&lt;p&gt;One way of choosing a good model is to take an ensemble of models (say, represented by different values of $\lambda$), train all of them, on the training set, and calculate test set error. Form the ensemble of trained models, we have chosen a model that best fits the test set. Essentially, we have used the test set to fit the extra parameter $\lambda$. This means that the performance on the test set is not indicative of performance on unseen data ! Hence, split data into three sets - Training/&lt;strong&gt;Validation&lt;/strong&gt;/Test sets. &lt;/p&gt;
&lt;p&gt;Choose the model (trained on the training set) that optimises $J_{valid}(\theta)$ and then evaluate $J_{test}(\theta)$, which will then be an indicator of how well the chosen model will perform on new data.&lt;/p&gt;
&lt;h2&gt;Machine learning diagnostics&lt;/h2&gt;
&lt;p&gt;How can one improve the performance of a predictive algorithm on test data if it is not satisfactory ? The following approaches can help -&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;More training data&lt;/strong&gt;. in some settings, this could help. But, not always.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Check for overfitting&lt;/strong&gt;, and reduce number of features used.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Check for underfitting&lt;/strong&gt;, and see what other features can be included.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Feature engineering&lt;/strong&gt;. Check if including some functions of existing features improves performance. Sometimes, finding the right function of existing performance can radically improve performance.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modify regularization parameter $\lambda$&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But, which one should you do ? How to those a course of action ? This is where &lt;strong&gt;ML diagnostics&lt;/strong&gt; enter the picture. &lt;/p&gt;
&lt;h3&gt;Bias Vs Variance&lt;/h3&gt;
&lt;p&gt;Consider the three errors $J_{train}$ on the training set, $J_{valid}$ on the validation set, and $J_{test}$ on the test set. We always have,&lt;/p&gt;
&lt;p&gt;$$J_{train} &amp;lt; J_{valid} &amp;lt; J_{test}.$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When the regularization parameter $\lambda$ is very large, we are penalising the parameters of the model and &lt;strong&gt;underfitting&lt;/strong&gt; the data (high bias) : &lt;strong&gt;high $J_{train}$ and high $J_{valid}$&lt;/strong&gt;.  &lt;/li&gt;
&lt;li&gt;For very low values of $\lambda$ where our model might be &lt;strong&gt;overfitting&lt;/strong&gt; the data (high variance)  : &lt;strong&gt;very low $J_{train}$ and high $J_{valid}$&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Plotting these errors against $\lambda$ (or another parameter that indicates model complexity) can be instructive. $J_{valid}$ has a minima at the optimal model complexity. That is the sweet spot one wants to hit.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tip :&lt;/strong&gt; While the cost function includes the regularization term, it is clear that to evaluate model performance, the regularization term is not relevant. So, it is not included in the error terms mentioned above. &lt;/p&gt;
&lt;h3&gt;Learning curves&lt;/h3&gt;
&lt;p&gt;Take small subsets of various sizes of the training set, and train a particular model on each of them. Plot the errors $J_{train}$ and $J_{valid}$ as a function of training set size $m$. For small $m$, $J_{train}$ is small (since it is easy to fit a small number of examples) while the $J_{valid}$ is large (since the model has not had much data to learn from). As $m$ grows, $J_{train}$ increases and $J_{valid}$ decreases.&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;high bias&lt;/strong&gt; case, $J_{valid}$ does not decrease much with $m$, while $J_{train}$ will increase a lot with $m$ and end up close to $J_{valid}$ quite quickly. So, a high bias learning algorithm does not perform much better with lots more data. &lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;high variance&lt;/strong&gt; case, $J_{train}$ will increase slowly with $m$. The validation error $J_{valid}$ will decrease slowly because of over-fitting, and for moderate $m$ there will be a big gap between $J_{train}$ and $J_{valid}$. But, over-fitting is reduced (and accuracy increased) as more data is added. So, the curves $J_{train}$ and $J_{valid}$ come closer as $m$ increases. &lt;/p&gt;
&lt;p&gt;So once one has run some of the diagnostics above (error vs $\lambda$ plots, error vs $m$ plots (learning curves)) etc, one can consider the possible courses of action we had mentioned earlier :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Getting more training examples helps fix high variance (reduces over-fitting).   &lt;/li&gt;
&lt;li&gt;Reducing number of features also helps fix high variance.  &lt;/li&gt;
&lt;li&gt;Adding features and feature engineering helps fix high bias issues.  &lt;/li&gt;
&lt;li&gt;Increasing $\lambda$ fixes high variance, while decreasing $\lambda$ fixes high bias. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Needless to say, small neural networks are prone to under-fitting. Large neural networks are prone to over-fitting, so regularization is important. Worth trying neural nets with different number of hidden layers and finding out which of them performs well on the validation sets. &lt;/p&gt;
&lt;h3&gt;Tips from Ng&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Start with a &lt;strong&gt;simple algorithm&lt;/strong&gt; that is easy to implement.  &lt;/li&gt;
&lt;li&gt;Plot &lt;strong&gt;learning curves&lt;/strong&gt; to diagnose over/under fitting and decide on course of action.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Error analysis.&lt;/strong&gt; examine, plot etc the examples from the validation set that the algorithm failed on, and try to spot patterns or features that can be used to improve performance. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Skewed classes :&lt;/strong&gt; when the overwhelming number of examples fall into one class. E.g.. faulty parts. Only 0.01% of parts might be faulty, so just marking everything as fine will lead to 99.9% correct classification, and yet, not a single faulty part will have been caught. Thus, for such cases, a different error metric is needed. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Precision/recall :&lt;/strong&gt; calculate number of true positives, true negatives and false positives and false negatives. $$\text{Precision} = \frac{\text{True positives}}{\text{# Predicted positives}}=\frac{\text{True positives}}{\text{True positives+False positives}}$$
    $$\text{Recall}=\frac{\text{True positives}}{\text{# Actual positives}} = \frac{\text{True positives}}{\text{True positives+False negatives}}$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Precision/Recall tradeoffs :&lt;/strong&gt; trade-off occurs because increasing precision means reducing number of false positives, so stringent criteria for predicting a positive. This will inevitably mean that the number of false negatives increase too, leading to lower recall. And it works the other way too, increasing recall leads to lower precision. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;F$_1$ score :&lt;/strong&gt; comparing precision/recall numbers. $$F_1 = 2\frac{PR}{P+R}$$ makes for a good metric that ensures neither precision $P$ nor recall $R$ are too low, if the $F_1$ score is quite good. Choose the value of the threshold (for logistic regression, say) that maximises the $F_1$ score on the cross validation set.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;When is lots of data worth it ?&lt;/strong&gt; Learning algorithms with large number off parameters (low bias) need large data sets to prevent over-fitting. Basically, we address the bias problem with a flexible and powerful learning algorithm and we address the variance problem with the massive data set. &lt;/li&gt;
&lt;li&gt;Always worth asking and investigating if the problem is soluble at all, before investing in big data and machine learning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Feature engineering matters more than specific learning algorithm used&lt;/strong&gt;. The amount of data, the type of features created, and skill in how the learning algorithm is used affects results a lot more than using this or that algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Support Vector Machines&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;An alternative view of logistic regression&lt;/strong&gt; :
remember, the hypothesis function of logistic regression for an input vector $x$ is,&lt;/p&gt;
&lt;p&gt;$$
h_{\theta}(x)=\frac{1}{1+e^{-\theta^T x}} = g(z)
$$
where 
$$
z=\theta^T x.
$$
Intuitively, if $y=1$, $h_{\theta}\approx 1\implies z\gg 0$ and $y=0$, $h_{\theta}\approx 0\implies z\ll 0$.
Recall the cost function of logistic regression 
$$
-\left[y\cdot \text{log}(h_{\theta}(x))+(1-y)\cdot\text{log}(1-h_{\theta}(x)) \right].
$$ 
For a particular example $(x,y)$ where $x$ is the input vector and $y$ is the output, suppose $y=1$.
Then, the cost function becomes 
$$
\text{ErrCost}(z|y=1)=-\text{log}\frac{1}{1+e^{-z}}.
$$
To make a support vector machine, we basically use a new cost function $\text{cost}_1$ that approximates this cost function with 2 straight line segments, while approximating 
$$
\text{ErrCost}(z|y=0)=\text{log}\left(1-\frac{1}{1+e^{-z}}\right)
$$ 
with a different cost function $\text{cost}_0$ also consisting of 2 line segments. This yields a simpler, faster optimization problem.&lt;/p&gt;
&lt;p&gt;In particular, $\text{cost}_1$ is a straight line with negative slope with an x intercept at 1. For $x\geq 1$, $\text{cost}_1=0$. On the other hand, $\text{cost}_0$ is a straight line with positive slope with an x intercept at -1. For $x\leq -1$, $\text{cost}_0=0$. &lt;/p&gt;
&lt;p&gt;The cost function for &lt;strong&gt;logistic regression&lt;/strong&gt;
$$
J(\theta) = E_i[-y^i\text{log}(h_{\theta}(x^i))-(1-y^i)\text{log}(1-h_{\theta}(x^i))]+\frac{\lambda}{2m}\sum_j (\theta_j)^2
$$
while, for &lt;strong&gt;support vector machines&lt;/strong&gt; the cost function is written as
$$
J(\theta)=C\cdot mE_i[ y^{(i)}\text{cost}_1(\theta^T x^{(i)}) +(1-y^{(i)})\text{cost}_0(\theta^T x^{(i)})]+\frac{1}{2}\sum_j (\theta_j)^2.
$$
Apart from the more approximate cost functions, the differences in the two cost functions are a matter of convention. For SVMs, the relative weights of the errors and the regularization term is controlled by the parameter C that is multiplied to the error term, rather than $\lambda$ multiplied to the regularization term as in the cost function for logistic regression. Also, in support vector machines the error and regularization term are not divided through by the number of examples. These changes should not - of course - change anything fundamental in the optimization procedure. &lt;/p&gt;
&lt;p&gt;Unlike logistic regression which gives a probability, the hypothesis function of an SVM is,
$$
h_{\theta}(x) = \begin{cases}
                1 &amp;amp; \theta^T x\geq 0\
                0 &amp;amp; \theta^T x&amp;lt;0
                \end{cases}.
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Large margin classifier limit&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Due to the form of the functions $\text{cost}_1$ and $\text{cost}_0$, if $y=1$, we want $\theta^T x\geq 1$ (not just $\geq 0$) and if $y=0$, we want $\theta^T x\leq -1$ (not just $&amp;lt;0$). In other words, the boundaries for the two classes are separated from each other, unlike in logistic regression. In practice, SVMs choose separators between cases that have larger margins to all classes. hence the name. This happens because of the optimization problem we have defined with the cost function and the definitions of the functions $\text{cost}_1$ and $\text{cost}_0$. In the limit $C\gg 1$, SVMs are equivalent to large margin classifiers. &lt;/p&gt;
&lt;p&gt;On the other hand, large margin classifiers can be very sensitive to outliers, SVMs do not suffer from this as long as the parameter $C$ is chosen wisely. &lt;/p&gt;
&lt;p&gt;when the classes are well separated, we can set error to 0. Hence, the optimisation function becomes&lt;/p&gt;
&lt;p&gt;$$
\hat{\theta} = \arg \min_{\theta}\left(\frac{1}{2}\sum_{j=1}^n\theta_j^2 \right)\text{ such that }
\begin{cases}
\theta^Tx\geq 1 &amp;amp; \text{if } y^{(i)}=1 \
\theta^Tx\leq -1 &amp;amp; \text{if } y^{(i)}=0. 
\end{cases}
$$&lt;/p&gt;
&lt;h3&gt;Kernels - adapting SVMs for non linear decision boundaries&lt;/h3&gt;
&lt;p&gt;One way to get non linear boundaries is to include polynomial features and treat those as new predictors, as we discussed for logistic regression. But, for complex problems, higher order polynomials are not really a good choice and can be very computationally expensive to include all necessary features. &lt;/p&gt;
&lt;p&gt;A better way to pick features is using &lt;strong&gt;landmarks&lt;/strong&gt;. Certain points are identified in the space of features as being in some way specially significant to the problem at hand, and proximity (using some notion of distance or similarity) to these points is used to compute further features. For instance, given landmarks $l^{(1)},l^{(2)},l^{(3)}$ in the space of features, we can define one feature to be &lt;/p&gt;
&lt;p&gt;$$
f_1 = \exp\left(-\frac{||{x-l^{(1)}}||^2}{2\sigma^2}\right)
$$&lt;/p&gt;
&lt;p&gt;The specific similarity functions used are called &lt;strong&gt;kernels&lt;/strong&gt;. In this case, we are using a &lt;em&gt;gaussian&lt;/em&gt; kernel for $f_1$. It is clear that the Gaussian kernel falls away from the landmark at a rate determined by $\sigma$ and has a value of 1 at the landmark, and 0 infinitely far from the landmark. For classification problems, it is clear how choosing landmarks at estimated or intuitive or known centres of classes would be a good choice. &lt;/p&gt;
&lt;p&gt;In fact, given a limited number of $m$ training examples, each training example can be a landmark leading to a new feature vector $f={f_1, f_2,...f_m}$. So, for a training example $x^{(i)}$, we have the feature vector $f^{(i)} = {f^{(i)}_1,f^{(i)}_2,f^{(i)}_3....f^{(i)}_m}$ where $f^{(i)}_j$ is the similarity measure (given by the kernel) of the $i^{th}$ training example from the $j^{th}$ landmark (which, in this case, is the $j^{th}$ training example.. so $f^{(k)}_k = 1$ in this particular case). &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition of SVM with kernels&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hypothesis : given $x$, compute the features $f\in \mathbb{R}^{m+1}$. Parameters $\theta\in\mathbb{R}^{m+1}$, predict $y=1$ if $\theta^Tf\geq 0$.   &lt;/li&gt;
&lt;li&gt;Training : 
$$
\hat\theta = \arg \min_{\theta}\left[ C\cdot \sum_{i=1}^m \left( y^{(i)}\text{cost}&lt;em j="1"&gt;1(\theta^Tf^{(i)}) + (1-y^{(i)})\text{cost}_0(\theta^Tf^{(i)})\right) + \frac{1}{2}\sum&lt;/em&gt;^m \theta_j^2\right]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Intuition on over, under fitting :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Large&lt;/strong&gt; $C\implies$ low bias, high variance, while &lt;strong&gt;small&lt;/strong&gt; $C\implies$ high bias low variance.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Large&lt;/strong&gt; $\sigma\implies$ smoothly varying features, high bias low variance, while &lt;strong&gt;small&lt;/strong&gt; $\sigma\implies$ sharp features, low bias, high variance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Using SVMs in practice&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While the algorithms used to solve the optimization problem are available in many software libraries, we do have to make some choices in order to use an SVM to solve our problem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A value of the parameter C.  &lt;/li&gt;
&lt;li&gt;An appropriate kernel, and parameters involved therein.   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are many good choices of kernels depending on the problem and structure of the data, but a valid kernel needs to satisfy &lt;a href="https://www.quora.com/What-is-an-intuitive-explanation-of-Mercers-Theorem"&gt;Mercer's theorem&lt;/a&gt; in order to be compatible with the optimisation procedure for SVM implementations. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tips from Ng&lt;/strong&gt; :&lt;/p&gt;
&lt;p&gt;Let $n$ be number of features, $m$ be number of training examples.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if $n\geq m$, use logistic regression or SVM with no kernel.  &lt;/li&gt;
&lt;li&gt;if $n\leq 10^3$ and $m\leq 10^4$, use SVM with Gaussian kernel.  &lt;/li&gt;
&lt;li&gt;if $n\leq 10^3$ and $m\geq 10^4$ add features using landmarks and use logistic regression or SVM without kernels.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Neural networks will work well for most of these regimes, but will take a lot longer to train. One advantage of SVMs is that the optimisation problem is a convex problem, which means that the we are likely to end up close to a global optimum. Unlike in other algorithms, we don't have to worry about landing up in local optima. &lt;/p&gt;
&lt;h2&gt;Unsupervised learning&lt;/h2&gt;
&lt;p&gt;Unsupervised learning algorithms find structure in unlabelled datasets. for instance, clustering. Clustering problems occur in may contexts :  &lt;br&gt;
- market segmentation&lt;br&gt;
- organizing computing clusters&lt;br&gt;
- astronomical data analysis&lt;br&gt;
- social network analysis  &lt;/p&gt;
&lt;h3&gt;K-Means clustering algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Randomly initialize $K$ cluster centroids ${\mu_1, ... \mu_k...\mu_K} \in \mathbb{R}^n$(if we want to cluster the data into n clusters) in the feature space of the data set.  &lt;/li&gt;
&lt;li&gt;Assign each data point $i$ to the cluster $c^{(i)}$ with the closest cluster centroid $\mu_{c^{(i)}}$.   &lt;/li&gt;
&lt;li&gt;Compute the mean of the data points assigned to each cluster, and move the cluster centroid to that location. (update ${\mu_1, ... \mu_k...\mu_K}$)  &lt;/li&gt;
&lt;li&gt;Repeat the cluster assignment of step 2. (update cluster assignments $c^{(i)}$ for each data point $i$)  &lt;/li&gt;
&lt;li&gt;Repeat steps 2-4 until the cluster centroids don't move (much) any more.   &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;how cool will it be to visualize a K-Means run on flat, sperical, toroidal etc geometries ! does it converge on a mobius strip ? on a sphere ?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Optimization objective function&lt;/strong&gt; for the K-Means algorithm : 
we have $m$ data points, and $K$ clusters. Then, 
$$
J(c^{(1)},...c^{(m)},\mu_1,...\mu_K) = \frac{1}{m}\sum_{i=1}^{m}||x^{(i)}-\mu_{c^{(i)}}||
$$&lt;/p&gt;
&lt;p&gt;This function is also called the &lt;strong&gt;distortion&lt;/strong&gt;. The K-Means algorithm minimises this function. Clearly, the cluster assignment step is minimising the distances from data points to the cluster centroid of the clusters they are assigned to, and then, the re calculation of the cluster centroids again reduces the distance by moving the cluster centroids to the centre of mass. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Randomly initializing cluster centroids&lt;/strong&gt; : &lt;br&gt;
- clearly, $K&amp;lt;m$.&lt;br&gt;
- randomly select $K$ training examples, and set the $K$ centroids to these examples. &lt;br&gt;
- K-Means can end up at different solutions depending on initial centroid initialization and it can end up in bad local optima. &lt;br&gt;
- we can try multiple random initializations and choose the one that converges to the best (smallest cost function) solution.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to choose number of clusters ?&lt;/strong&gt; : 
Most popularly, do some data exploration and visualization and choose the number of clusters by hand. But, this may be genuinely hard, or unclear. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Elbow method &lt;/em&gt;- Plot the cost function against the number of clusters chosen. It often turns out that until a certain number of clusters chosen, the distortion decreases rapidly, and after that point goes down very slowly (forming an elbow). Then choose the number at the bend of the elbow. &lt;/p&gt;
&lt;p&gt;However, sometimes the distortion goes down smoothly with number of clusters (this is a lot more common). In this case, &lt;em&gt;optimise the number of clusters $K$ for the ultimate purpose for which the clustering is being done&lt;/em&gt;. E.g.. if we are clustering a population into sizes to manufacture t-shirts, then we can do the clustering for several values of $K$, and see how much business sense it makes to have those clusters, with the population segmented that way, in terms of cluster sizes, t-shirt fits, etc. &lt;/p&gt;
&lt;h3&gt;Dimensionality reduction&lt;/h3&gt;
&lt;p&gt;What is it good for ?&lt;br&gt;
- &lt;strong&gt;data compression&lt;/strong&gt; : basically, finding a more efficient representation of the data in a smaller number of dimensions.&lt;br&gt;
- &lt;strong&gt;visualization&lt;/strong&gt; : if dimensionality can be reduced to 3, or even better, 2 dimensions, then, structure in the data that might otherwise be difficult to see, might be easily visualized. &lt;/p&gt;
&lt;h3&gt;Principal Component Analysis&lt;/h3&gt;
&lt;p&gt;Essentially, PCA searches for a lower dimensional surface such that the sum of squares for the distance from the data points to the surface (projection error) is minimised. It is important to normalize and scale the features before PCA (so that the distances in different directions in the feature space are comparable). &lt;/p&gt;
&lt;p&gt;To reduce from $n$ dimensions to $k$ dimensions, we want to find the $k$ vectors $u^{(1)}..u^{(k)}$ onto which to project the data such that the projection error is minimized. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The algorithm :&lt;/strong&gt;&lt;br&gt;
1. always start with mean normalization and feature scaling.&lt;br&gt;
2. compute the $n\times n$ covariance matrix $\Sigma = \frac{1}{m}\sum_{i=1}^{n}(x^{(i)})(x^{(i)})^T = \frac{1}{m}X^T X$ or $\Sigma = \frac{1}{m}X^TX$.&lt;br&gt;
3. compute the eigenvectors of $\Sigma$, using the singular value decomposition function &lt;code&gt;svd&lt;/code&gt; (normally, the &lt;code&gt;eig&lt;/code&gt; function would be used, but for covariance matrices (the way they are constructed) the singular value decomposition gives the same eigenvectors) see &lt;a href="https://math.stackexchange.com/questions/320220/intuitively-what-is-the-difference-between-eigendecomposition-and-singular-valu"&gt;this page&lt;/a&gt; for some excellent intuitive explanations.&lt;br&gt;
4. in octave, &lt;code&gt;[U,S,V] = svd(Sigma)&lt;/code&gt; and &lt;code&gt;U&lt;/code&gt; has the eigen vectors. To reduce $n$ dimensions to $k$, just take the first $k$ columns of the matrix &lt;code&gt;U&lt;/code&gt;. $U_r$ is $n\times k$. &lt;br&gt;
5. the new $m\times k$ data $Z = X^TU_r$ where $X$ is the original $n$ dimensional data with $m$ examples.  &lt;/p&gt;
&lt;p&gt;If this sort of thing is to be used for data compression, clearly, we need to be able to go back to the $n$ dimensional space, with some loss of information due to the compression procedure. This is just the $m\times n$ matrix $X_{\text{approx}} = ZU_r^T$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How many principle components should I keep ?&lt;/strong&gt; :&lt;br&gt;
- average squared projection error = $\frac{1}{m}\sum_{i}^{m}||x^{(i)}-x_{\text{approx}}^{(i)}||^2$&lt;br&gt;
- total variation in the data = $\frac{1}{m}\sum_{i=1}^m ||x^{(i)}||^2$&lt;br&gt;
- choose $k$ to be the smallest value such that 99% of the variance is retained, &lt;br&gt;
$$
\frac{\frac{1}{m}\sum_{i}^{m}||x^{(i)}-x_{\text{approx}}^{(i)}||^2}{\frac{1}{m}\sum_{i=1}^m ||x^{(i)}||^2}\leq 0.01
$$
- the way to check this, is using the matrix &lt;code&gt;S&lt;/code&gt; from the &lt;code&gt;[U,S,V] = svd(Sigma)&lt;/code&gt;. &lt;code&gt;S&lt;/code&gt; is a diagonal square matrix $S_{ii}$. for a given $k$, we have,
$$
\frac{\frac{1}{m}\sum_{i}^{m}||x^{(i)}-x_{\text{approx}}^{(i)}||^2}{\frac{1}{m}\sum_{i=1}^m ||x^{(i)}||^2} = 1 - \frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}},
$$
so, with just one run of the &lt;code&gt;svd&lt;/code&gt;, we can find the value of $k$ we need. &lt;a href="https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca"&gt;Here is&lt;/a&gt; an excellent account of the relationship between singular value decomposition and PCA.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using PCA to speedup a learning algorithm :&lt;/strong&gt; essentially, learning on very high dimensional data is hard. With PCA, we can reduce the dimensionality and thus, in due to trivial computational reasons (fewer numbers to crunch !) get any algorithm to run faster. Clearly, we must apply PCA on training set, find the mapping (the learned matrix $U_r$) and apply the same mapping to cross validation and test data. &lt;/p&gt;
&lt;p&gt;Ng says PCA should be used as a part of pre-processing for ML algorithms &lt;em&gt;only if&lt;/em&gt; running with the raw data does not work. &lt;/p&gt;
&lt;h3&gt;Anomaly detection&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem definition&lt;/strong&gt; : Given a dataset of "normal" examples $X$, is a new example $x_{test}$ anomalous ?
Usually, this is approached by building a probability distribution $P_X$ over $X$ and computing $P_X(X_test)$ and then, if $P_X(X_test)&amp;lt;\epsilon$ for some sensible $\epsilon$, then we might classify $x_{test}$ is anomalous. For example,   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;fraud detection&lt;/strong&gt; : measure user characteristics (login times, typing speed etc.) on a website, and flag users that are behaving unusually.   &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;manufacturing&lt;/strong&gt; : measure features for each machine/product, and when there is a machine/product whose $P(x)$ is very small, it can be flagged for further analysis/maintenance.   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of this sounds like a job for &lt;a href="https://arxiv.org/abs/1704.03924"&gt;kernel density estimation&lt;/a&gt;, does it not :)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The algorithm :&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Training set: ${x^{(1)},....,x^{(m)}}$ each $x\in \mathbb{R^n}$.  &lt;/li&gt;
&lt;li&gt;each $p(x^{(i)}) = \prod_{j=1}^n p(x^{(i)}_j;\mu_j, \sigma^2_j)$ is a product of independent Gaussian.  &lt;/li&gt;
&lt;li&gt;Choose features ${x_j}$ that might be indicative of anomalous examples  &lt;/li&gt;
&lt;li&gt;Fit parameters ${\mu_j, \sigma^2_j}$ for each feature  &lt;/li&gt;
&lt;li&gt;given new example, compute &lt;br&gt;
$$
p(x) = \prod_{j=1}^n p(x_j;\mu_j,\sigma^2_j)&lt;br&gt;
$$&lt;/li&gt;
&lt;li&gt;anomaly if $p(x)&amp;lt;\epsilon$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Real number evaluation&lt;/strong&gt; while evaluating an algorithm, always best to have a method that returns a number.. which allows us to gauge how good the algorithms is. &lt;/p&gt;
&lt;p&gt;for anomaly detection, assume $y=1$ is anomalous, $y=0$ is non anomalous. Then, several metrics are possible, $F_1$ score, for instance. The hyper-parameter $\epsilon$ should be chosen on the cross validation set, and then the test set performance should be indicative of real performance. &lt;/p&gt;
&lt;p&gt;Why not use supervised learning ?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;very few anomalies in training set, so algorithm cannot really know all the possible anomalies, making supervised learning useless to detect new anomalies  &lt;/li&gt;
&lt;li&gt;if there are lots of anomalies, then supervised learning has a chance. but, for rare positives/anomalies.. best to go with anomaly detection, since the learning algorithm cannot learn much from the anomalous examples.   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since a lot of density estimation algorithms are based on Gaussian distributions, it is best to transform all features so that they look vaguely Gaussian. &lt;/p&gt;
&lt;p&gt;If $p(x)$ is similar for normal and anomalous examples.. then adding a feature which helps identify anomalies will help. If $p(x)$ is too small both for normal and anomalous examples, removing features with large variation might help.&lt;/p&gt;
&lt;p&gt;Clearly, the assumption of independence of features is a strong one. A true multivariate distribution will do a much better job of anomaly detection. In that case, the problem does reduce to multivariate KDE. &lt;/p&gt;
&lt;h3&gt;Recommender systems&lt;/h3&gt;
&lt;p&gt;Recommender systems are massively useful systems that directly add to the profits of many companies. Fundamentally, recommenders are market lubricants, facilitating exchange of information to improve number of exchanges made. &lt;/p&gt;
&lt;p&gt;One of the "big ideas" of machine learning is the notion of &lt;strong&gt;automatically learning features&lt;/strong&gt; instead of hand coding them in, and recommender systems are a good setting to show this. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 1:&lt;/strong&gt; Predicting movie ratings.&lt;br&gt;
Index $i$ denotes movie, $j$ denotes person. Then, for each pair $(i,j)$ we either have a movie rating $y(i,j)$ and a flag $r(i,j)=1$ (if a movie $i$ has been watched by the person $j$), or the flag $r(i,j)=0$, if person $j$ has not watched movie $i$. We want to predict the ratings $y$ for the cases when $r(i,j)=0$. let $n_m$ is number of movies, $n_u$ is number of users. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Content based recommenders :&lt;/strong&gt;  Suppose that for each movie, we have features $x_1$ which measures how romantic a movie is, and $x_2$ which measures how much of an action movie it is. In general, there could be lots of such features based on the content of the movie. For each movie $i$, we have a feature vector $x^{(i)}$. We could now treat predicting the ratings for each user as a regression problem. In the linear regression case, for user $j$, we have a parameter vector $\theta^{(j)}$. Once we learn these parameter vectors ${\theta^{(j)}}$, for a movie with feature vector $x^{(i)}$, the predicted rating is just $y(i,j) = \theta^{(j)}\cdot x^{(i)}$. The parameters $\theta^{(j)}$ is learnt on the basis of linear regression on the movies that user $j$ has rated, for each user $j$.&lt;/p&gt;
&lt;p&gt;Of course, a lot of the time, we might not have content based features for various movies. Hence, &lt;strong&gt;collaborative filtering&lt;/strong&gt;. Here, we know nothing about the content of our movies, but, we do know something about our users. Each user $j$ just tells us $\theta^{(j)}$ via some survey. Then, based on available ratings $y(i,j)$ when $r(i,j)=1$, we can infer the feature vectors $x^{(i)}$, since we have $y(i,j) = \theta^{(j)}\cdot x^{(i)}$ using linear regression, where the $x^{(i)}$ are the parameters. Once the $x^{(i)}$ are known, the $\theta$ vectors for new users can be estimated based on their ratings, as before. &lt;/p&gt;
&lt;p&gt;This suggests an iterative process :&lt;br&gt;
- guess random $\theta$s&lt;br&gt;
- infer $x$ via known ratings&lt;br&gt;
- infer $\theta$ based on $x$ and ratings&lt;br&gt;
- repeat until reasonable convergence.  &lt;/p&gt;
&lt;p&gt;But, there is a more efficient algorithm that does not need to iterate. Instead, just treat $x$s and $\theta$s as one set of parameters ${...x^{(i)}......\theta^{(j)}...}$. The modified optimization objective is 
$$
J(..x^{(i)}......\theta^{(j)}...) = \frac{1}{2}\sum_{(i,j):r(i,j)=1}\left(\left(\theta^{(j)}\right)^Tx^{(i)}-y(i,j)\right)^2 + \frac{1}{2}\sum_i\sum_k \left( x_k^{(i)} \right)^2 + \frac{1}{2}\sum_j\sum_k \left( \theta_k^{(j)} \right)^2
$$
where we must minimise over all $x$s and $\theta$s. &lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;collaborative filtering algorithm&lt;/strong&gt; is : &lt;br&gt;
1. Initialize the parameters ${...x^{(i)}......\theta^{(j)}...}$ to small random values&lt;br&gt;
2. Minimise $J(..x^{(i)}......\theta^{(j)}...)$ over all parameters using gradient descent.&lt;br&gt;
3. For a user with parameters $\theta$ and a movie with features $x$, the rating is $\theta\cdot x$.   &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vectorized collaborative filtering&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$Y_{(i,j)} = \theta^{(j)}\cdot x^{(i)}$$
$$X_{(i,k)} = x^{(i)}&lt;em _j_k_="(j,k)"&gt;k$$
$$\Theta&lt;/em&gt;_k$$
$$Y = X\Theta^T$$} = \theta^{(j)&lt;/p&gt;
&lt;p&gt;This is called &lt;strong&gt;low ranked matrix factorization&lt;/strong&gt; because $Y$ is &lt;a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)"&gt;low ranked.&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;Once we know features related to movies $x^(i)$ then finding movies related to a given movie $x^{(i_0)}$, one can just calculate the distances $||x^{(i_0)}-x^{(i)}||$ and pick a few movies with the lowest distances. &lt;/p&gt;
&lt;p&gt;In general, it's best to regularize the means for various known quantities. &lt;/p&gt;
&lt;h2&gt;Large scale machine learning&lt;/h2&gt;
&lt;p&gt;when starting with a big data set, &lt;strong&gt;always&lt;/strong&gt; first try with small subsets, and plot the learning curves ($J_{train}, J_{CV}$ vs $m$) to ensure that your learning algorithm has a large variance for small $m$.&lt;/p&gt;
&lt;h3&gt;Gradient descent with large datasets&lt;/h3&gt;
&lt;p&gt;Recall the gradient descent update rule -
$$
\theta_j :=\theta_j - \alpha \frac{1}{m}\sum_i^m (h_{\theta}(x^i)-y^i)x^i_j
$$
When $m$ is very large, each step of the gradient descent algorithm requires summing over a huge $m$, and this is computationally hugely expensive and time consuming. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stochastic gradient descent&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The usual gradient descent is called &lt;em&gt;batch gradient descent&lt;/em&gt;, when all training examples are used to update the parameters in the $\frac{1}{m}\sum_i^m (h_{\theta}(x^i)-y^i)x^i_j$ term (which reflects the derivative of the cost function $J_{train}(\theta) = \frac{1}{2m}\sum_{i=1}^m (h_{\theta}(x^i)-y^i)$).&lt;/p&gt;
&lt;p&gt;For stochastic gradient descent :&lt;br&gt;
1. define a cost function for one training example $cost(\theta, i) = \frac{1}{2}(h_{\theta}(x^i)-y^i)^2$&lt;br&gt;
2. shuffle the order of training dataset&lt;br&gt;
3. Repeat until reasonable results (between 1-10 times), 
$$
\text{for i=1..m} \
\left{
\text{for j=i..n} \
\theta_j := \theta_j-\alpha((h_{\theta}(x^i)-y^i)x^i_j)
\right}
$$
This does not really converge, but it ends up with parameters in the vicinity of the global minimum. In exchange for very significant computational savings.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mini-batch gradient descent&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Batch gradient descent - use all $m$ examples in each update. &lt;br&gt;
Stochastic gradient descent - use 1 example in each update.&lt;br&gt;
Mini-batch gradient descent - use $b$ examples in each iteration.  &lt;/p&gt;
&lt;p&gt;$$
\text{for i=1..}\frac{m}{b} \
\left{
\text{for j=i..n} \
\theta_j := \theta_j-\alpha\frac{1}{b}\sum_{k=1}^{b-1}((h_{\theta}(x^{(i-1)b+k})-y^{(i-1)b+k})x^{(i-1)b+k}_j)
\right}
$$&lt;/p&gt;
&lt;p&gt;this gives better performance than stochastic gradient if we have a very good vectorized implementation. Of course, this is same as batch gradient descent if $b=m$. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tips for ensuring stochastic gradient descent is working&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;during learning compute $cost(\theta, i) = \frac{1}{2}(h_{\theta}(x^i)-y^i)^2$ before updating $\theta$ with that training example.  &lt;/li&gt;
&lt;li&gt;every 1000 steps (say) plot $cost(\theta, i)$ averaged over the last 1000 examples.  &lt;/li&gt;
&lt;li&gt;this plots should slowly get better as more examples are processed.   &lt;/li&gt;
&lt;li&gt;this might suggest using a smaller learning rate, since the oscillations around the minimum will now be smaller  &lt;/li&gt;
&lt;li&gt;slowly decrease learning rate to get stochastic gradient descent to converge $\alpha = \frac{c_1}{\text{iter}+c_2}$, but then there are two more hyper-parameters that need to be fiddled with  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Online learning : continuous data stream&lt;/h3&gt;
&lt;p&gt;An increasingly common setting, since a lot of websites and other companies collect very large amounts of data in real time and need to use it in real time. Now, we discard the notion of a fixed training set. An example comes in, we update our model with the data, and abandon the data and just keep the updated model. If there is a small number of users, it might make sense to store all the data.. but for huge data volumes, it makes sense to learn from incoming traffic and let your model learn continuously. &lt;/p&gt;
&lt;p&gt;This has the advantage of letting your website/business adapt to changing user preferences. &lt;/p&gt;
&lt;h3&gt;Map-reduce and parallelism&lt;/h3&gt;
&lt;p&gt;Some data problems are too large to handle on one machine. Such problems are usually tackled with clusters of computers, and map-reduce is a frame work to parallelize work over several machines. Can handle problems far larger than stochastic gradient descent. &lt;/p&gt;
&lt;p&gt;If there are $m$ training examples, and there are $q$ machines to run these on, then $m/q$ are sent to each machine, and each machine computes 
$$
t^q_j = \sum_k^{(m/q)} (h_{\theta}(x^k)-y^k)x_j^k
$$
then, we can compute the update for batch gradient descent 
$$
\theta_j := \theta_j -\alpha\frac{1}{m}\sum_q t_j^q
$$&lt;/p&gt;
&lt;p&gt;and (ignoring overheads) we can get a maximum speed-up of $q$ times. This basically, parallelizes the calculation of the sum involved in gradient descent updates. &lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;key question :&lt;/strong&gt; can the learning algorithm be expressed as a sum of some functions over the training set ? if so, map-reduce can help.&lt;/p&gt;
&lt;p&gt;For instance, for many optimization algorithms, we need to provide them with cost functions (thats one sum) and gradient (another sum), so for large data sets, map-reduce can parallelize these sums and pass these values to the optimization algorithm.  &lt;/p&gt;
&lt;p&gt;On multi-core machines, map-reduce can already help by paralleling. But, in such cases, vectorized implementations along with a very good, parallelized linear algebra library will take care of this. Hadoop has this system under the hood.&lt;/p&gt;
&lt;h2&gt;General lessons from a case study (photo-OCR)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;define a &lt;strong&gt;pipeline&lt;/strong&gt; for the ML problem. The photo-OCR pipeline :   &lt;ol&gt;
&lt;li&gt;text detection  &lt;/li&gt;
&lt;li&gt;character segmentation  &lt;/li&gt;
&lt;li&gt;character classification   &lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;sliding window classification :  &lt;ol&gt;
&lt;li&gt;if we have say 50px $\times$ 50px images of an object, we obtain lots of 50 $\times$ 50 images without the object and train a supervised learning classifier.  &lt;/li&gt;
&lt;li&gt;given an image, we slide a 50$times$50 window over the image and run the classifier at each step  &lt;/li&gt;
&lt;li&gt;how much the sliding window moves, is determined by the stride parameter  &lt;/li&gt;
&lt;li&gt;then, do this for a larger windows (by taking larger bits of the image and compressing down to 50 $times$) and run the classifier over the image (to detect the object at different scales).  &lt;/li&gt;
&lt;li&gt;coalesce nearby positive responses into common rectangles using an expansion operator (classify nearby negative pixels to positive too, up to a certain distance).    &lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Artificial data synthesis :  &lt;ul&gt;
&lt;li&gt;creating data from scratch : for say, text detection, take random text,m transform it into many random fonts, and paste each piece onto a random background. this is some work, but good synthetic data creation can lead to an unlimited supply of labeled data to help solve your problem.  &lt;/li&gt;
&lt;li&gt;amplifying a small training set : for each element in the training set, add various warpings, colours, backgrounds, noise etc. with insight and thought, it can lead to a much amplified training set. for different problems, of course the distortions added will be different. For instance, for audio, we can add different background noises etc. The distortions introduced should be representative of the sorts of distortions that might come up in the test set.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Before setting out to get more data -  &lt;ol&gt;
&lt;li&gt;is our algorithm low bias ? plot learning curves  &lt;/li&gt;
&lt;li&gt;"How much work would it be to get 10x as much data ?" if one can brainstorm ones way to lots more data with a few days of work, large improvements in performance can be expected. mechanical turk is an option.    &lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Ceiling analysis : what to work on next  &lt;ul&gt;
&lt;li&gt;one of the most valuable resources is your time spent working on system.  &lt;/li&gt;
&lt;li&gt;pick a single real number evaluation metric for the over all system and measure it  &lt;/li&gt;
&lt;li&gt;now, fix the test set with labels that let one module do it's job with 100% accuracy, now measure overall system accuracy  &lt;/li&gt;
&lt;li&gt;do this for each module in turn starting with the most upstream component, and work on the module that creates the largest impact on the overall system  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="data_sci_tech"></category></entry></feed>