<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>"greta playground" - p. bhogale</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">


    <link href=".//images/gravtar.jpg" rel="icon">

<link rel="canonical" href="./greta-playground.html">

        <meta name="author" content="pras" />
        <meta name="description" content="A first foray into probabilistic programming with Greta Models and modelling Much of science - physical and social - is devoted to positing mechanisms that explain how the data we observe are generated. In a classic example, after Tycho Brahe made detailed observations of planetary motion (here is data on mars), Johannes …" />

        <meta property="og:site_name" content="p. bhogale" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="&#34;greta playground&#34;"/>
        <meta property="og:url" content="./greta-playground.html"/>
        <meta property="og:description" content="A first foray into probabilistic programming with Greta Models and modelling Much of science - physical and social - is devoted to positing mechanisms that explain how the data we observe are generated. In a classic example, after Tycho Brahe made detailed observations of planetary motion (here is data on mars), Johannes …"/>
        <meta property="article:published_time" content="2018-03-11" />
            <meta property="article:section" content="data_sci_tech" />
            <meta property="article:author" content="pras" />



    <!-- Bootstrap -->
        <link rel="stylesheet" href="./theme/css/bootstrap.min.css" type="text/css"/>
    <link href="./theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="./theme/css/pygments/monokai.css" rel="stylesheet">
    <link rel="stylesheet" href="./theme/css/style.css" type="text/css"/>

        <link href="./feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="p. bhogale ATOM Feed"/>

        <link href="./feeds/data_sci_tech.atom.xml" type="application/atom+xml" rel="alternate"
              title="p. bhogale data_sci_tech ATOM Feed"/>
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-115756026-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', UA-115756026-1);
    </script>
    <!-- End Google Analytics Code -->
    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="./" class="navbar-brand">
<img class="img-responsive pull-left gap-right" src=".//images/gravtar.jpg" width=""/> p. bhogale            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                         <li><a href="./pages/about.html">
                             About
                          </a></li>
                        <li >
                            <a href="./category/blog.html">Blog</a>
                        </li>
                        <li class="active">
                            <a href="./category/data_sci_tech.html">Data_sci_tech</a>
                        </li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<!-- Banner -->
<style>
	#banner{
	    background-image:url(".//images/banner.jpg");
	}
</style>

<div id="banner">
	<div class="container">
		<div class="copy">
			<h1>p. bhogale</h1>
				<p class="intro">Data Sci, Quant Fin, Quant Bio.</p>
		</div>
	</div>
</div><!-- End Banner -->

<!-- Content Container -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="./greta-playground.html"
                       rel="bookmark"
                       title="Permalink to "greta playground"">
                        "greta playground"
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2018-03-11T00:00:00+01:00"> So 11 März 2018</time>
    </span>





    
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>A first foray into probabilistic programming with Greta</p>
<h2>Models and modelling</h2>
<p>Much of science - physical and social - is devoted to positing mechanisms that explain how the data we observe are generated. In a classic example, after <a href="https://en.wikipedia.org/wiki/Tycho_Brahe">Tycho Brahe</a> made detailed observations of planetary motion (<a href="http://www.pafko.com/tycho/observe.html">here</a> is data on mars), <a href="https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion">Johannes Kepler posited laws</a> of planetary motion that <em>explained</em> how this data were generated. Effectively, <strong>modelling</strong> is the art of constructing data generators that help us understand and predict. </p>
<p><strong>Statistical models</strong> are one class of models that aim to construct - given some observed data - the probability distribution from which the data were drawn. That is, given a sample of data, a statistical model is a hypothesis about how this data were generated. In practice, this happens in two steps :<br>
- constructing a hypothesis, or a model <span class="math">\(H\)</span> parametrized by some parameters <span class="math">\(\theta\)</span>,<br>
- finding (<em>inferring</em>) the distribution of parameters <span class="math">\(\theta\)</span> or, the most suitable parameters <span class="math">\(\theta\)</span> given the observed data</p>
<p>What parameters are "most suitable" is indicated (in a particular sense of the word "suitable" will become clear in the following discussion) by the <a href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood function</a> that quantifies how probable the observed data set is, for a given hypothesis parametrized by some particular parameters <span class="math">\(H_{\theta}\)</span>. Understandably, we want to find parameters such that the observed data is the most likely, this is called <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation</a>.</p>
<p>Since all but the simplest models are analytically intractable (i.e., the maximum of the likelihood function needs to be evaluated numerically and parameter distributions are even harder to compute) it makes sense to construct general rules and syntax to easily define statistical models and quickly infer their parameters. This is the field of probabilistic programming. </p>
<h2>Probabilistic programming</h2>
<p>The probabilistic programming language (PPL) has two tasks :  </p>
<ol>
<li>be able to construct a useful class of statistical models  </li>
<li>be able to infer the parameters (and their distributions) of this class of models given some observed data.  </li>
</ol>
<p>As has been explained in this <a href="https://www.reddit.com/r/deeplearning/comments/846wb6/the_paper_that_introduces_the_edward_ppl_by/">excellent paper introducing the PPL Edward</a> that is based on Python and <a href="https://www.tensorflow.org/">Tensorflow</a>, some PPLs restrict the class of models they allow in order to optimize the inference algorithm, while other emphasize expressiveness and sacrifice performance of the inference algorithms. Modern PPLs like <a href="http://edwardlib.org/">Edward</a>, <a href="https://eng.uber.com/pyro/">Pyro</a>, and the R based <a href="https://greta-dev.github.io/greta/index.html">Greta</a> use the robust infrastructure (hardware and software) that was first developed in the context of deep learning and thus ensure scalability and performance while being expressive. </p>
<h3>The tensor and the computational graph</h3>
<p>The fundamental data structure of this group of languages is the <a href="https://en.wikipedia.org/wiki/Tensor">tensor</a> which is just a multidimensional array. Data, model parameters, samples from distributions are all stored in tensors. All the manipulations that go into the construction of the output tensor constitute the computational graph (see <a href="http://colah.github.io/posts/2015-08-Backprop/">this</a> for an exceptionally clear exposition of the concept) associated with that tensor.  </p>
<p>Data and parameter tensors are inputs to the computational graph. In the context of deep learning, "training" consists of the following steps :  </p>
<ol>
<li>Randomly initializing the parameter tensors  </li>
<li>Computing the output  </li>
<li>Measuring the error compared to the real/desired output  </li>
<li>Tweaking the parameter tensors to reduce the error.  </li>
</ol>
<p>The algorithm that does this is called <a href="https://en.wikipedia.org/wiki/Backpropagation">back propagation</a>.
Thus, the objective in deep learning or machine learning is to obtain the <strong>best values</strong> (in the sense of that they minimize error on the training set) of the parameters given some data.</p>
<p>The objective of probabilistic modelling is subtly different. The aim here is to obtain the <strong>distribution</strong> (called <strong>posterior distribution</strong>) of parameters, given the data. If we denote the data by <span class="math">\(D\)</span>, <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes theorem</a> relates (for a particular hypothesis about how the data were generated <span class="math">\(H\)</span>), the likelihood of the data given some parameters <span class="math">\(P(D|\theta,H)\)</span>, our prior expectations about how the parameters are distributed <span class="math">\(P(\theta)\)</span> and the posterior distribution of the parameters themselves <span class="math">\(P(\theta|D,H)\)</span> :</p>
<div class="math">$$P(\theta|D,H) = \frac{P(D|\theta,H)P(\theta)}{P(D)}.$$</div>
<p>The priors <span class="math">\(P(\theta)\)</span> do not depend on the data and encode "domain knowledge" while the probability of the data set <span class="math">\(P(D)\)</span> over the whole parameter space is (typically) a high dimensional integral given by
</p>
<div class="math">$$P(D|H) = \int P(D,\theta|H)d\theta.$$</div>
<p>Intuitively, we can see that the most likely parameters given the data, i.e. the parameters <span class="math">\(\theta\)</span> which maximize <span class="math">\(P(\theta|D,H)\)</span> ought to correspond to the sense of "best" or "most suitable" described above. From Bayes theorem, it is clear that the posterior distribution is directly proportional to the likelihood <span class="math">\(P(\theta|D,H) \propto P(D|\theta,H)\)</span>. Thus, maximizing likelihood is one way to get estimates of the "most likely parameters" (in the limit of infinite data), but computing the full distribution <span class="math">\(P(\theta|D,H)\)</span> involves dealing with the difficult integral for <span class="math">\(P(D|H)\)</span>.</p>
<h3>Bayesian prediction and MCMC</h3>
<p>Prediction in this framework is also fundamentally different from typical machine learning model. The probability of a new data point <span class="math">\(d\)</span>,
</p>
<div class="math">$$P(d|D,H) = \int P(d|\theta,H)P(\theta|D,H)d\theta,$$</div>
<p>
which consists of the expectation value of the new data point over the whole distribution of parameters given the observed data (the posterior distribution calculated obtained from the solution to the inference problem), instead of a value calculated by plugging in the "learned parameter values" into the machine learning model. </p>
<p>The integrals needed for inference (<span class="math">\(P(D|H) = \int P(D,\theta|H)d\theta\)</span> as well as prediction <span class="math">\(P(d|D,H) = \int P(d|\theta,H)P(\theta|D,H)d\theta\)</span> need to be evaluated over the entire parameter space of the model which can be very high dimensional. Markov Chain Monte Carlo methods are used to approximate these integrals. <a href="https://www.reddit.com/r/deeplearning/comments/8487xg/very_good_introduction_to_hamiltonian_monte_carlo/">This</a> is an excellent overview of modern Hamiltonian Monte Carlo methods while <a href="https://www.reddit.com/r/MachineLearning/comments/84fobk/superb_overview_and_motivation_for_monte_carlo/?ref=share&amp;ref_source=link">this</a> provides wonderful perspective from the dawn of the field. Both papers are long but eminently readable and highly recommended. </p>
<p>Clearly then, along with the computational graph to define models, a PPL needs a good MCMC algorithm (or another inference algorithm) to compute the high dimensional integrals needed to infer as well as perform a prediction on a general probabilistic model. </p>
<p>A broad overview of Bayesian machine learning is available <a href="http://mlg.eng.cam.ac.uk/zoubin/talks/mit12csail.pdf">here (PDF)</a> and <a href="http://fastml.com/bayesian-machine-learning/">here</a></p>
<p>Now, we illustrate some of these points using the simplest possible example, linear regression.</p>
<h2>Basic linear regression.</h2>
<p>We will generate artificial data with known parameters, so that we can check if Greta (the PPL we are using for this article) gets it right later. </p>
<h3>Generating fake data to fit a model to</h3>
<div class="highlight"><pre><span></span><code><span class="n">length_of_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">100</span>
<span class="n">sd_eps</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="kc">pi</span><span class="o">^</span><span class="nf">exp</span><span class="p">(</span><span class="m">1</span><span class="p">)</span>
<span class="n">intercept</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">-5.0</span>
<span class="n">slope</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="kc">pi</span>
<span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">seq</span><span class="p">(</span><span class="m">-10</span><span class="o">*</span><span class="kc">pi</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="o">*</span><span class="kc">pi</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">length_of_data</span><span class="p">)</span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">intercept</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">slope</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">length_of_data</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sd_eps</span><span class="p">)</span>
<span class="n">data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data_frame</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">)</span>
<span class="nf">ggplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;simulated dependent variable&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_point</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_smooth</span><span class="p">(</span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&#39;lm&#39;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">ggtitle</span><span class="p">(</span><span class="s">&quot;Fake experimental data&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="n">ggthemes</span><span class="o">::</span><span class="nf">theme_economist</span><span class="p">()</span>
</code></pre></div>

<p><img alt="center" src="/figures/greta_playground/unnamed-chunk-2-1.png"></p>
<p>Given this data, we want to write Greta code to infer the posterior distributions of the model parameters. </p>
<h3>Defining clueless priors for model parameters</h3>
<p>In this case, the parameters of our model are simple, but in principle, they can be arbitrary tensors. Since we really don't know anything about the prior distributions of our parameters, we look at the experimental data and take rough, uniform priors. </p>
<div class="highlight"><pre><span></span><code><span class="n">intercept_p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">uniform</span><span class="p">(</span><span class="m">-10</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">)</span>
<span class="n">sd_eps_p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">uniform</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">50</span><span class="p">)</span>
<span class="n">slope_p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">uniform</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">)</span>
</code></pre></div>

<h3>Defining the model</h3>
<div class="highlight"><pre><span></span><code><span class="n">mean_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">intercept_p</span><span class="o">+</span><span class="n">slope_p</span><span class="o">*</span><span class="n">x</span>
<span class="nf">distribution</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">normal</span><span class="p">(</span><span class="n">mean_y</span><span class="p">,</span><span class="w"> </span><span class="n">sd_eps_p</span><span class="p">)</span>
</code></pre></div>

<p>Here, we hypothesize that the target variable <span class="math">\(y\)</span> is linearly dependent on some independent variable <span class="math">\(x\)</span> with a noise term drawn from a Gaussian distribution whose standard deviation is also a parameter of the model. </p>
<p>Under the hood, Greta has constructed a computational graph that encapsulates all these operations, and defines the process of computing the target <span class="math">\(y\)</span> starting from the prior distributions of our input variables. We plot this computational graph below :</p>
<div class="highlight"><pre><span></span><code><span class="n">our_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">model</span><span class="p">(</span><span class="n">intercept_p</span><span class="p">,</span><span class="w"> </span><span class="n">slope_p</span><span class="p">,</span><span class="w"> </span><span class="n">sd_eps_p</span><span class="p">)</span>
<span class="n">our_model</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">plot</span><span class="p">()</span>
</code></pre></div>

<p><img alt="center" src="/figures/greta_playground/model.png"></p>
<h3>Inference</h3>
<p>There are two distinct types of inference possible, </p>
<ol>
<li><strong>Sampling from the full posterior distribution</strong> for the parameters given the data and the model. </li>
<li><strong>Maximizing likelihood to compute "most probable" parameters</strong> given the data and the model.</li>
</ol>
<h4>Sampling from the posterior distribution of parameters with MCMC</h4>
<div class="highlight"><pre><span></span><code><span class="n">num_samples</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1000</span>
<span class="n">param_draws</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">mcmc</span><span class="p">(</span><span class="n">our_model</span><span class="p">,</span><span class="w"> </span><span class="n">n_samples</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_samples</span><span class="p">,</span><span class="w"> </span><span class="n">warmup</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_samples</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="m">10</span><span class="p">)</span>
</code></pre></div>

<p>and plot the densities of samples drawn from the parameter posterior distributions, and the parameter fits.</p>
<div class="highlight"><pre><span></span><code><span class="nf">mcmc_dens</span><span class="p">(</span><span class="n">param_draws</span><span class="p">)</span>
</code></pre></div>

<p><img alt="center" src="/figures/greta_playground/unnamed-chunk-7-1.png"></p>
<div class="highlight"><pre><span></span><code><span class="nf">mcmc_intervals</span><span class="p">(</span><span class="n">param_draws</span><span class="p">)</span>
</code></pre></div>

<p><img alt="center" src="/figures/greta_playground/unnamed-chunk-7-2.png"></p>
<p>By inspection, it looks like the <a href="https://arxiv.org/abs/1701.02434">HMC</a> has found reasonable values for our model parameters and their posterior distributions. </p>
<h4>Most probable parameters</h4>
<p>Explicitly, the mean estimates can be computed from the <code>param_draws</code> data structure, or via the <code>greta::opt</code> function.   </p>
<div class="highlight"><pre><span></span><code><span class="n">param_draws_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as_data_frame</span><span class="p">(</span><span class="n">param_draws</span><span class="p">[[</span><span class="m">1</span><span class="p">]])</span>
<span class="n">param_estimates</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">param_draws_df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">summarise_all</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
<span class="n">param_estimates</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">print</span><span class="p">()</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="gu">##</span> # A tibble: 1 x 3
<span class="gu">##</span>   intercept_p slope_p sd_eps_p
<span class="gu">##</span>         &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
<span class="gu">##</span> 1       -6.12    3.12     22.8
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">opt_params</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">opt</span><span class="p">(</span><span class="n">our_model</span><span class="p">)</span>
<span class="n">opt_params</span><span class="o">$</span><span class="n">par</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">print</span><span class="p">()</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="gu">##</span> intercept_p     slope_p    sd_eps_p 
<span class="gu">##</span>   -6.686146    3.187089   23.300232
</code></pre></div>

<h3>Bayesian prediction</h3>
<p>Bayesian prediction is implemented via the <code>calculate()</code> function available in the latest release of <code>greta</code> on github. This generates a prediction on <span class="math">\(y\)</span> for each draw from the posterior distribution of the parameters (see previous section). Taking the expectation over this distribution of predictions gives us the mean value of the target variable <span class="math">\(y\)</span> but we have the whole distribution of <span class="math">\(y\)</span> available to us if we need to analyse it. </p>
<div class="highlight"><pre><span></span><code><span class="n">mean_y_plot</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">intercept_p</span><span class="o">+</span><span class="n">slope_p</span><span class="o">*</span><span class="n">x</span>
<span class="n">mean_y_plot_draws</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">calculate</span><span class="p">(</span><span class="n">mean_y_plot</span><span class="p">,</span><span class="w"> </span><span class="n">param_draws</span><span class="p">)</span>
<span class="n">mean_y_est</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">colMeans</span><span class="p">(</span><span class="n">mean_y_plot_draws</span><span class="p">[[</span><span class="m">1</span><span class="p">]])</span>
<span class="n">data_pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">mutate</span><span class="p">(</span><span class="n">y_fit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean_y_est</span><span class="p">)</span>
<span class="nf">ggplot</span><span class="p">(</span><span class="n">data_pred</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_point</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;simulated dependent variable&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_line</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y_fit</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;estimated expectation value&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">ggtitle</span><span class="p">(</span><span class="s">&quot;Fitted model&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="n">ggthemes</span><span class="o">::</span><span class="nf">theme_economist</span><span class="p">()</span>
</code></pre></div>

<p><img alt="center" src="/figures/greta_playground/unnamed-chunk-9-1.png"></p>
<h2>Further exploration</h2>
<ol>
<li>The most mature PPL out there (with good R bindings) is Stan. There is a lot of material available, and it might be a good place to start to pick up some intuition. See <a href="http://mc-stan.org/users/documentation/">this page</a>.  </li>
<li><a href="http://www.mit.edu/~9.520/spring10/Classes/class21_mcmc_2010.pdf">This</a> is a good intro to the role of MCMC in inference.  </li>
<li>These video lectures on <a href="https://www.youtube.com/watch?v=oy7Ks3YfbDg">statistical rethinking</a> emphasizing Bayesian statistics also seem interesting.</li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </div>
            <!-- /.entry-content -->
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://www.linkedin.com/in/pbhogale"><i class="fa fa-linkedin-square fa-lg"></i> linkedin</a></li>
    <li class="list-group-item"><a href="https://github.com/pbhogale"><i class="fa fa-github-square fa-lg"></i> github</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Recent Posts -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Recent Posts</span></h4>
  <ul class="list-group" id="recentposts">
    <li class="list-group-item"><a href="./r-torch-and-the-little-book-of-deep-learning.html">"R, Torch, and the little book of deep learning"</a></li>
    <li class="list-group-item"><a href="./becoming-a-data-scientist-an-opinionated-take-in-2020.html">"Becoming a Data Scientist : an opinionated take in 2020"</a></li>
    <li class="list-group-item"><a href="./linear-and-mixed-integer-programming.html">"Linear and mixed integer programming"</a></li>
    <li class="list-group-item"><a href="./the-invisible-hand.html">"The Invisible Hand"</a></li>
    <li class="list-group-item"><a href="./greta-playground.html">"greta playground"</a></li>
  </ul>
</li>
<!-- End Sidebar/Recent Posts -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<!-- End Content Container -->

<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2024 pras
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="./theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="./theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="./theme/js/respond.min.js"></script>


    <script src="./theme/js/bodypadding.js"></script>


</body>
</html>