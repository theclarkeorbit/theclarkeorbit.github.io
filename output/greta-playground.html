
<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet/less" type="text/css" href="./theme/stylesheet/style.less">
    <script src="//cdnjs.cloudflare.com/ajax/libs/less.js/2.5.1/less.min.js" type="text/javascript"></script>

  <link rel="stylesheet" type="text/css" href="./theme/pygments/default.min.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/font-awesome.min.css">




    <link rel="shortcut icon" href="https://theclarkeorbit.github.io/images/favicon.png" type="image/x-icon">
    <link rel="icon" href="https://theclarkeorbit.github.io/images/favicon.png" type="image/x-icon">

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />

    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#333333">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#333333">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

<meta name="author" content="pras" />
<meta name="description" content="A first foray into probabilistic programming with Greta Models and modelling Much of science - physical and social - is devoted to positing mechanisms that explain how the data we observe are generated. In a classic example, after Tycho Brahe made detailed observations of planetary motion (here is data on mars), Johannes Kepler posited laws of planetary motion that explained how this data were generated. Effectively, modelling is the art of constructing data generators that help us understand and predict. Statistical models are one class of models that aim to construct - given some observed data - the probability distribution from which the data …" />
<meta name="keywords" content="">
<meta property="og:site_name" content="p. bhogale"/>
<meta property="og:title" content="“greta playground”"/>
<meta property="og:description" content="A first foray into probabilistic programming with Greta Models and modelling Much of science - physical and social - is devoted to positing mechanisms that explain how the data we observe are generated. In a classic example, after Tycho Brahe made detailed observations of planetary motion (here is data on mars), Johannes Kepler posited laws of planetary motion that explained how this data were generated. Effectively, modelling is the art of constructing data generators that help us understand and predict. Statistical models are one class of models that aim to construct - given some observed data - the probability distribution from which the data …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="./greta-playground.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-03-14 22:56:59.848531+01:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="./author/pras.html">
<meta property="article:section" content="blogs"/>
<meta property="og:image" content="https://en.gravatar.com/userimage/9352950/78ed70e67418f76f23b494458d53ac7d.jpg?size=400">

  <title>p. bhogale &ndash; “greta playground”</title>

</head>
<body>
  <aside>
    <div>
      <a href=".">
        <img src="https://en.gravatar.com/userimage/9352950/78ed70e67418f76f23b494458d53ac7d.jpg?size=400" alt="prasanna bhogale" title="prasanna bhogale">
      </a>
      <h1><a href=".">prasanna bhogale</a></h1>

<p>Data Sci, Quant Fin, Quant Bio.</p>
      <nav>
        <ul class="list">
          <li><a href="./pages/about.html#about">about</a></li>

        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-linkedin" href="https://www.linkedin.com/in/pbhogale" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-twitter" href="https://twitter.com/thegymnosophist" target="_blank"><i class="fa fa-twitter"></i></a></li>
        <li><a class="sc-reddit" href="https://www.reddit.com/user/thegymnosophist" target="_blank"><i class="fa fa-reddit"></i></a></li>
        <li><a class="sc-github" href="https://github.com/pbhogale" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href=".">    Home
</a>



    </nav>

<article class="single">
  <header>
    <h1 id="greta-playground"><span class="dquo">&#8220;</span>greta&nbsp;playground&#8221;</h1>
    <p>
          Posted on Wed 14 March 2018 in <a href="./category/blogs.html">blogs</a>


    </p>
  </header>


  <div>
    <p>A first foray into probabilistic programming with&nbsp;Greta</p>
<h2>Models and&nbsp;modelling</h2>
<p>Much of science - physical and social - is devoted to positing mechanisms that explain how the data we observe are generated. In a classic example, after <a href="https://en.wikipedia.org/wiki/Tycho_Brahe">Tycho Brahe</a> made detailed observations of planetary motion (<a href="http://www.pafko.com/tycho/observe.html">here</a> is data on mars), <a href="https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion">Johannes Kepler posited laws</a> of planetary motion that <em>explained</em> how this data were generated. Effectively, <strong>modelling</strong> is the art of constructing data generators that help us understand and&nbsp;predict. </p>
<p><strong>Statistical models</strong> are one class of models that aim to construct - given some observed data - the probability distribution from which the data were drawn. That is, given a sample of data, a statistical model is a hypothesis about how this data were generated. In practice, this happens in two steps :<br>
- constructing a hypothesis, or a model $H$ parametrized by some parameters $\theta$,<br>
- finding (<em>inferring</em>) the distribution of parameters $\theta$ or, the most suitable parameters $\theta$ given the observed&nbsp;data</p>
<p>What parameters are &#8220;most suitable&#8221; is quantified (in a particular sense of the word &#8220;suitable&#8221; will become clear in the following discussion) by the <a href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood function</a> that quantifies how probable the observed data set is, for a given hypothesis parametrized by some particular parameters $H_{\theta}$. Understandably, we want to find parameters such that the observed data is the most likely, this is called <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation</a>.</p>
<p>Since all but the simplest models are analytically intractable (i.e., the maximum of the likelihood function needs to be evaluated numerically) it makes sense to construct general rules and syntax to easily define and quickly infer the parameters of statistical models. This is the field of probabilistic&nbsp;programming. </p>
<h2>Probabilistic&nbsp;programming</h2>
<p>The probabilistic programming language (<span class="caps">PPL</span>) has two tasks&nbsp;:  </p>
<ol>
<li>be able to construct a useful class of statistical&nbsp;models  </li>
<li>be able to infer the parameters of this class of models given some observed&nbsp;data.  </li>
</ol>
<p>As has been explained in this <a href="https://www.reddit.com/r/deeplearning/comments/846wb6/the_paper_that_introduces_the_edward_ppl_by/">excellent paper introducing the <span class="caps">PPL</span> Edward</a> that is based on Python and <a href="https://www.tensorflow.org/">Tensorflow</a>, some PPLs restrict the class of models they allow in order to optimize the inference algorithm, while other emphasize expressiveness and sacrifice performance of the inference algorithms. Modern PPLs like <a href="http://edwardlib.org/">Edward</a>, <a href="https://eng.uber.com/pyro/">Pyro</a>, and the R based <a href="https://greta-dev.github.io/greta/index.html">Greta</a> use the robust infrastructure (hardware and software) that was first developed in the context of deep learning and thus ensure scalability and performance while being&nbsp;expressive. </p>
<h3>The tensor and the computational&nbsp;graph</h3>
<p>The fundamental data structure of this group of languages is the <a href="https://en.wikipedia.org/wiki/Tensor">tensor</a> which is just a multidimensional array. Data, model parameters, samples from distributions are all stored in tensors. All the manipulations that go into the construction of the output tensor constitute the computational graph (see <a href="http://colah.github.io/posts/2015-08-Backprop/">this</a> for an exceptionally clear exposition of the concept) associated with that&nbsp;tensor.  </p>
<p>Data and parameter tensors are inputs to the computational graph. In the context of deep learning, &#8220;training&#8221; consists of the following steps :
1. Randomly initializing the parameter tensors<br>
2. Computing the output<br>
3. Measuring the error compared to the real/desired output<br>
4. Tweaking the parameter tensors to reduce the error.<br>
The algorithm that does this is called <a href="https://en.wikipedia.org/wiki/Backpropagation">back propagation</a>.
Thus, the objective in deep learning or machine learning is to obtain the <strong>best values</strong> of the parameters given some&nbsp;data.</p>
<p>The objective of probabilistic modelling is subtly different. The objective here is to obtain the <strong>distribution</strong> (called <strong>posterior distribution</strong>) of parameters given the data. If we denote the data by $D$, <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes theorem</a> connects (for a particular hypothesis about how the data were generated $H$), the likelihood of the data given some parameters $P(D|\theta,H)$, our prior expectations about how the parameters are distributed $P(\theta)$ and the posterior distribution of the parameters themselves $P(\theta|D,H)$&nbsp;:</p>
<p>$$P(\theta|D,H) =&nbsp;\frac{P(D|\theta,H)P(\theta)}{P(D)}.$$</p>
<p>The priors $P(\theta)$ do not depend on the data and encode &#8220;domain knowledge&#8221; while and the probability of the data set $P(D)$ is (typically) a high dimensional integral given by
$$P(D|H) = \int&nbsp;P(D,\theta|H)d\theta.$$</p>
<p>Intuitively, we want to compute the most likely parameters given the data, i.e. we want to maximize $P(\theta|D,H)$. While maximizing the likelihood can give us the estimates of the &#8220;most likely parameters&#8221; (in the limit of infinite data), computing the full distribution $P(\theta|D,H)$ involves the computation of the difficult integral for&nbsp;$P(D|H)$.</p>
<h3>Bayesian prediction and <span class="caps">MCMC</span></h3>
<p>Prediction in this framework is also fundamentally different from typical machine learning model. The probability of a new data point $d$,
$$P(d|D,H) = \int P(d|\theta,H)P(\theta|D,H)d\theta,$$
which consists of the expectation value of the new data point over the whole distribution of parameters given the observed data (the posterior distribution calculated obtained from the solution to the inference problem), instead of a value calculated by plugging in the &#8220;learned parameter values&#8221; into the machine learning&nbsp;model. </p>
<p>The integrals needed for inference ($P(D|H) = \int P(D,\theta|H)d\theta$ as well as prediction $P(d|D,H) = \int P(d|\theta,H)P(\theta|D,H)d\theta$ are over the parameter space which can be very high dimensional. This, Markov Chain Monte Carlo methods are used to approximate these integrals. <a href="https://www.reddit.com/r/deeplearning/comments/8487xg/very_good_introduction_to_hamiltonian_monte_carlo/">This</a> is an excellent overview of modern Hamiltonian Monte Carlo methods while <a href="https://www.reddit.com/r/MachineLearning/comments/84fobk/superb_overview_and_motivation_for_monte_carlo/?ref=share&amp;ref_source=link">this</a> provides wonderful perspective from the dawn of the field. Both papers are long but eminently readable and highly&nbsp;recommended. </p>
<p>Clearly then, along with the computational graph to define models, a <span class="caps">PPL</span> needs a good <span class="caps">MCMC</span> algorithm (or another inference algorithm) to compute the high dimensional integrals needed to infer as well as perform a prediction on a general probabilistic&nbsp;model. </p>
<p>A broad overview of Bayesian machine learning is available <a href="http://mlg.eng.cam.ac.uk/zoubin/talks/mit12csail.pdf">here (<span class="caps">PDF</span>)</a> and <a href="http://fastml.com/bayesian-machine-learning/">here</a></p>
<p>Now, we illustrate some of these points using the simplest possible example, linear&nbsp;regression.</p>
<h2>Basic linear&nbsp;regression.</h2>
<p>We will generate artificial data with known parameters, so that we can check if Greta (the <span class="caps">PPL</span> we are using for this article) gets it right&nbsp;later. </p>
<h3>Generating fake data to fit a model&nbsp;to</h3>
<div class="highlight"><pre><span></span>length_of_data <span class="o">&lt;-</span> <span class="m">100</span>
sd_eps <span class="o">&lt;-</span> <span class="kc">pi</span><span class="o">^</span><span class="kp">exp</span><span class="p">(</span><span class="m">1</span><span class="p">)</span>
intercept <span class="o">&lt;-</span> <span class="m">-5.0</span>
slope <span class="o">&lt;-</span> <span class="kc">pi</span>
x <span class="o">&lt;-</span> <span class="kp">seq</span><span class="p">(</span><span class="m">-10</span><span class="o">*</span><span class="kc">pi</span><span class="p">,</span> <span class="m">10</span><span class="o">*</span><span class="kc">pi</span><span class="p">,</span> length.out <span class="o">=</span> length_of_data<span class="p">)</span>
y <span class="o">&lt;-</span> intercept <span class="o">+</span> slope<span class="o">*</span>x <span class="o">+</span> rnorm<span class="p">(</span>n <span class="o">=</span> length_of_data<span class="p">,</span> mean <span class="o">=</span> <span class="m">0</span><span class="p">,</span> sd <span class="o">=</span> sd_eps<span class="p">)</span>
data <span class="o">&lt;-</span> <span class="kt">data_frame</span><span class="p">(</span>y <span class="o">=</span> y<span class="p">,</span> x <span class="o">=</span> x<span class="p">)</span>
ggplot<span class="p">(</span>data<span class="p">,</span> aes<span class="p">(</span>x <span class="o">=</span> x<span class="p">,</span> y <span class="o">=</span> y<span class="p">))</span> <span class="o">+</span>
  geom_point<span class="p">()</span> <span class="o">+</span>
  geom_smooth<span class="p">(</span>method <span class="o">=</span> <span class="s">&#39;lm&#39;</span><span class="p">)</span> <span class="o">+</span>
  ggtitle<span class="p">(</span><span class="s">&quot;Fake experimental data&quot;</span><span class="p">)</span>
</pre></div>


<p><img alt="center" src="/figures/greta_playground/unnamed-chunk-2-1.png"></p>
<p>Given this data, we want to write Greta code to infer the posterior distributions of the model&nbsp;parameters. </p>
<h3>Defining clueless priors for model&nbsp;parameters</h3>
<p>These are the parameters of the model. In this case, they are simple but in principle they can be arbitrary tensors. Since we really don&#8217;t know anything about the prior distributions of our parameters, we look at the experimental data and take rough, uniform&nbsp;priors. </p>
<div class="highlight"><pre><span></span>intercept_p <span class="o">&lt;-</span> uniform<span class="p">(</span><span class="m">-10</span><span class="p">,</span> <span class="m">10</span><span class="p">)</span>
sd_eps_p <span class="o">&lt;-</span> uniform<span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">50</span><span class="p">)</span>
slope_p <span class="o">&lt;-</span> uniform<span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">10</span><span class="p">)</span>
</pre></div>


<h3>Defining the&nbsp;model</h3>
<div class="highlight"><pre><span></span>mean_y <span class="o">&lt;-</span> intercept_p<span class="o">+</span>slope_p<span class="o">*</span>x
distribution<span class="p">(</span>y<span class="p">)</span> <span class="o">&lt;-</span> normal<span class="p">(</span>mean_y<span class="p">,</span> sd_eps_p<span class="p">)</span>
</pre></div>


<p>Here, we hypothesize that the target variable $y$ is linearly dependent on some independent variable $x$ with a noise term drawn from a Gaussian distribution whose standard deviation is also a parameter of the&nbsp;model. </p>
<p>Under the hood, Greta has constructed a computational graph that encapsulates all these operations, and defines the process of computing $y$ starting from drawing from the prior distributions. We plot this computational graph&nbsp;below</p>
<div class="highlight"><pre><span></span>our_model <span class="o">&lt;-</span> model<span class="p">(</span>intercept_p<span class="p">,</span> slope_p<span class="p">,</span> sd_eps_p<span class="p">)</span>
our_model <span class="o">%&gt;%</span> plot<span class="p">()</span>
</pre></div>


<p><img alt="center" src="model.png"></p>
<h3>Inference</h3>
<p>There are two distinct types of inference&nbsp;possible, </p>
<ol>
<li><strong>Sampling from the full posterior distribution</strong> for the parameters given the data and the&nbsp;model. </li>
<li>Maximizing likelihood to compute &#8220;most probable&#8221; parameters given the data and the&nbsp;model.</li>
</ol>
<h4>Sampling from the posterior distribution of parameters with <span class="caps">MCMC</span></h4>
<div class="highlight"><pre><span></span>num_samples <span class="o">&lt;-</span> <span class="m">1000</span>
param_draws <span class="o">&lt;-</span> mcmc<span class="p">(</span>our_model<span class="p">,</span> n_samples <span class="o">=</span> num_samples<span class="p">,</span> warmup <span class="o">=</span> num_samples <span class="o">/</span> <span class="m">10</span><span class="p">)</span>
</pre></div>


<p>and plot the samples, and the parameter&nbsp;fits.</p>
<div class="highlight"><pre><span></span>mcmc_dens<span class="p">(</span>param_draws<span class="p">)</span>
</pre></div>


<p><img alt="center" src="/figures/greta_playground/unnamed-chunk-7-1.png"></p>
<div class="highlight"><pre><span></span>mcmc_intervals<span class="p">(</span>param_draws<span class="p">)</span>
</pre></div>


<p><img alt="center" src="/figures/greta_playground/unnamed-chunk-7-2.png"></p>
<p>By inspection, it looks like the <a href="https://arxiv.org/abs/1701.02434"><span class="caps">HMC</span></a> has reached some reasonable values for our model&nbsp;parameters. </p>
<h4>Most probable&nbsp;parameters</h4>
<p>Explicitly, the mean estimates can be computed from the <code>param_draws</code> data structure, or via the <code>greta::opt</code> function.   </p>
<div class="highlight"><pre><span></span>param_draws_df <span class="o">&lt;-</span> as_data_frame<span class="p">(</span>param_draws<span class="p">[[</span><span class="m">1</span><span class="p">]])</span>
param_estimates <span class="o">&lt;-</span> param_draws_df <span class="o">%&gt;%</span> 
  summarise_all<span class="p">(</span><span class="kp">mean</span><span class="p">)</span>
param_estimates <span class="o">%&gt;%</span> <span class="kp">print</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>## # A tibble: 1 x 3
##   intercept_p slope_p sd_eps_p
##         &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
## 1       -4.88    3.12     22.5
</pre></div>


<div class="highlight"><pre><span></span>opt_params <span class="o">&lt;-</span> opt<span class="p">(</span>our_model<span class="p">)</span>
opt_params<span class="o">$</span>par <span class="o">%&gt;%</span> <span class="kp">print</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>## intercept_p     slope_p    sd_eps_p 
##   0.4660273   2.9427588  20.7912216
</pre></div>


<h3>Bayesian&nbsp;prediction</h3>
<p>the <code>calculate()</code> function is available on the latest release of <code>greta</code> on github. This generates a prediction on $y$ for each draw from the posterior distribution of the parameters (see section on Bayesian prediction earlier). Taking the expectation over this distribution of predictions gives us the mean value of the target variable $y$ but we have the whole distribution of $y$ available to us if we need to analyse&nbsp;it. </p>
<div class="highlight"><pre><span></span>mean_y_plot <span class="o">&lt;-</span> intercept_p<span class="o">+</span>slope_p<span class="o">*</span>x
mean_y_plot_draws <span class="o">&lt;-</span> calculate<span class="p">(</span>mean_y_plot<span class="p">,</span> param_draws<span class="p">)</span>
mean_y_est <span class="o">&lt;-</span> <span class="kp">colMeans</span><span class="p">(</span>mean_y_plot_draws<span class="p">[[</span><span class="m">1</span><span class="p">]])</span>
data_pred <span class="o">&lt;-</span> data <span class="o">%&gt;%</span> mutate<span class="p">(</span>y_fit <span class="o">=</span> mean_y_est<span class="p">)</span>
ggplot<span class="p">(</span>data_pred<span class="p">)</span> <span class="o">+</span>
    geom_point<span class="p">(</span>aes<span class="p">(</span>x<span class="p">,</span>y<span class="p">),</span> colour <span class="o">=</span> <span class="s">&quot;blue&quot;</span><span class="p">)</span> <span class="o">+</span>
    geom_line<span class="p">(</span>aes<span class="p">(</span>x<span class="p">,</span>y_fit<span class="p">),</span> colour <span class="o">=</span> <span class="s">&#39;red&#39;</span><span class="p">)</span> <span class="o">+</span>
    ggtitle<span class="p">(</span><span class="s">&quot;Fitted model&quot;</span><span class="p">)</span>
</pre></div>


<p><img alt="center" src="/figures/greta_playground/unnamed-chunk-9-1.png"></p>
<h2>Further&nbsp;exploration</h2>
<ol>
<li>The most mature <span class="caps">PPL</span> out there (with good R bindings) is Stan. There is a lot of material available, and it might be a good place to start to pick up some intuition. See <a href="http://mc-stan.org/users/documentation/">this page</a>.  </li>
<li><a href="http://www.mit.edu/~9.520/spring10/Classes/class21_mcmc_2010.pdf">This</a> is a good intro to the role of <span class="caps">MCMC</span> in&nbsp;inference.  </li>
<li>These video lectures on <a href="https://www.youtube.com/watch?v=oy7Ks3YfbDg">statistical rethinking</a> emphasizing Bayesian statistics also seem&nbsp;interesting.</li>
</ol>
  </div>
  <div class="tag-cloud">
    <p>
    </p>
  </div>




</article>

    <footer>
<p>&copy; pras </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " p. bhogale ",
  "url" : ".",
  "image": "https://en.gravatar.com/userimage/9352950/78ed70e67418f76f23b494458d53ac7d.jpg?size=400",
  "description": "pras's home on the interwebz"
}
</script>
</body>
</html>